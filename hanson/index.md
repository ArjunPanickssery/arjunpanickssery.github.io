# Part 1: Our Thinking
## Near and Far
1. [Abstract/Distant Future Bias](#Abstract/Distant-Future-Bias)
2. [Abstractly Ideal, Concretely Selfish](#Abstractly-Ideal,-Concretely-Selfish)
3. [We Add Near, Average Far](#We-Add-Near,-Average-Far)
4. [Why We Don’t Know What We Want](#Why-We-Don’t-Know-What-We-Want)
5. [We See the Sacred from Afar, to See It Together](#We-See-the-Sacred-from-Afar,-to-See-It-Together)
6. [The Future Seems Shiny](#The-Future-Seems-Shiny)
7. [Doubting My Far Mind](#Doubting-My-Far-Mind)

## Disagreement
8. [Beware the Inside View](#Beware-the-Inside-View)
9. [Are Meta Views Outside Views?](#Are-Meta-Views-Outside-Views?)
10. [Disagreement Is Near-Far Bias](#Disagreement-Is-Near-Far-Bias)
11. [Others’ Views Are Detail](#Others’-Views-Are-Detail)
12. [Why Be Contrarian?](#Why-Be-Contrarian?)
13. [On Disagreement, Again](#On-Disagreement,-Again)
14. [Rationality Requires Common Priors](#Rationality-Requires-Common-Priors)
15. [Might Disagreement Fade Like Violence?](#Might-Disagreement-Fade-Like-Violence?)

## Biases
16. [Reject Random Beliefs](#Reject-Random-Beliefs)
17. [Chase Your Reading](#Chase-Your-Reading)
18. [Against Free Thinkers](#Against-Free-Thinkers)
19. [Eventual Futures](#Eventual-Futures)
20. [Seen vs. Unseen Biases](#Seen-vs.-Unseen-Biases)
21. [Law as No-Bias Theatre](#Law-as-No-Bias-Theatre)
22. [Benefit of Doubt = Bias](#Benefit-of-Doubt-=-Bias)

# Part 2: Our Motives
## Signaling
23. [Decision Theory Remains Neglected](#Decision-Theory-Remains-Neglected)
24. [What Function Music?](#What-Function-Music?)
25. [Politics isn’t about Policy](#Politics-isn’t-about-Policy)
26. [Views Aren’t About Sights](#Views-Aren’t-About-Sights)
27. [Why Do Bets Look Bad?](#Why-Do-Bets-Look-Bad?)
28. [Homo Hypocritus](#Homo-Hypocritus)
29. [Resolving Your Hypocrisy](#Resolving-Your-Hypocrisy)
30. [Errors, Lies, and Self-Deception](#Errors,-Lies,-and-Self-Deception)

## Norms
31. [Enforce Common Norms On Elites](#Enforce-Common-Norms-On-Elites)
32. [Identity Norms](#Identity-Norms)
33. [Exclusion As A Substitute For Norms, Law, & Governance](#Exclusion-As-A-Substitute-For-Norms,-Law,-&-Governance)
34. [How Idealists Aid Cheaters](#How-Idealists-Aid-Cheaters)
35. [Beware Mob War Strategy](#Beware-Mob-War-Strategy)
36. [Automatic Norms](#Automatic-Norms)
37. [10 Implications of Automatic Norms](#10-Implications-of-Automatic-Norms)
38. [Automatic Norm Lessons](#Automatic-Norm-Lessons)
39. [Automatic Norms in Academia](#Automatic-Norms-in-Academia)

## Fiction
40. [Plot Holes & Blame Holes](#Plot-Holes-&-Blame-Holes)
41. [Fairy Tales Were Cynical](#Fairy-Tales-Were-Cynical)
42. [Why Fiction Lies](#Why-Fiction-Lies)
43. [Biases Of Fiction](#Biases-Of-Fiction)
44. [Why We Fight Over Fiction](#Why-We-Fight-Over-Fiction)
45. [Stories Are Like Religion](#Stories-Are-Like-Religion)
46. [More Stories As Religion](#More-Stories-As-Religion)

## The Dreamtime
47. [This is the Dream Time](#This-is-the-Dream-Time)
48. [DreamTime](#DreamTime)
49. [Dreamtime Social Games](#Dreamtime-Social-Games)
50. [We Moderns Are Status-Drunk](#We-Moderns-Are-Status-Drunk)
51. [Earth: A Status Report](#Earth:-A-Status-Report)
52. [On Teen Angst](#On-Teen-Angst)

# Part 3: Our Institutions
## Prediction Markets
53. [Prediction Markets “Fail” To Moloch](#Prediction-Markets-“Fail”-To-Moloch)
54. [Seeking Robust Credible Expertise Buyers](#Seeking-Robust-Credible-Expertise-Buyers)
55. [Prediction Markets Need Trial and Error](#Prediction-Markets-Need-Trial-and-Error)
56. [New-Hire Prediction Markets](#New-Hire-Prediction-Markets)
57. [Shoulda-Listened Futures](#Shoulda-Listened-Futures)
58. [Brand Truth Narrowly](#Brand-Truth-Narrowly)
## Academia
59. [Fixing Academia Via Prediction Markets](#Fixing-Academia-Via-Prediction-Markets)
60. [Intellectual Prestige Futures](#Intellectual-Prestige-Futures)
61. [Academic Stats Prediction Markets](#Academic-Stats-Prediction-Markets)
62. [How To Fund Prestige Science](#How-To-Fund-Prestige-Science)
## Medicine
63. [Medical Doubts OpEd](#Medical-Doubts-OpEd)
64. [Medical Market Failures](#Medical-Market-Failures)
## Paternalism
65. [Paternalism Is About Status](#Paternalism-Is-About-Status)
66. [Rulesy Folks Push Paternalism](#Rulesy-Folks-Push-Paternalism)
67. [Universal Basic Dorms](#Universal-Basic-Dorms)
## Law
68. [Elites Must Rule](#Elites-Must-Rule)
69. [Status App Concept](#Status-App-Concept)
70. [Our Prestige Obsession](#Our-Prestige-Obsession)
71. [Yay Stability Rents](#Yay-Stability-Rents)
72. [Conditional Harberger Tax Games](#Conditional-Harberger-Tax-Games)
73. [Reliable Private-Enough Physical Identity](#Reliable-Private-Enough-Physical-Identity)
74. [Freedom Isn’t Free](#Freedom-Isn’t-Free)
75. [Quality Regs Say ‘High Is Good’](#Quality-Regs-Say-‘High-Is-Good’)
76. [Socialism: A Gift You’d Exchange?](#Socialism:-A-Gift-You’d-Exchange?)
77. [Vouch For Pandemic Passports](#Vouch-For-Pandemic-Passports)
78. [Can We Tame Political Minds?](#Can-We-Tame-Political-Minds?)
79. [Consider Reparations](#Consider-Reparations)
80. [Regulating Infinity](#Regulating-Infinity)
81. [Privately Enforced & Punished Crime](#Privately-Enforced-&-Punished-Crime)
82. [Fine Grain Futarchy Zoning Via Harberger Taxes](#Fine-Grain-Futarchy-Zoning-Via-Harberger-Taxes)

# Our 3: Our Past
## Farmers and Foragers
83. [Fear Made Farmers](#Fear-Made-Farmers)
84. [Forage vs Farm Future](#Forage-vs-Farm-Future)
85. [Two Types of People](#Two-Types-of-People)
86. [Forager v Farmer, Elaborated](#Forager-v-Farmer,-Elaborated)
87. [Rome As Semi-Foragers](#Rome-As-Semi-Foragers)
88. [Self-Control Is Slavery](#Self-Control-Is-Slavery)
89. [School Is To Submit](#School-Is-To-Submit)
90. [Why Grievances Grow](#Why-Grievances-Grow)
91. [The World Forager Elite](#The-World-Forager-Elite)
## History as Exponential Modes
92. [The Great Cycle Rule](#The-Great-Cycle-Rule)
93. [The Labor-From-Factories Explosion](#The-Labor-From-Factories-Explosion)
94. [Lost Advanced Civilizations](#Lost-Advanced-Civilizations)
## The Great Filter
95. [Try-Try or Try-Once Great Filter?](#Try-Try-or-Try-Once-Great-Filter?)
96. [Great Filter with Set-Backs, Dead-Ends](#Great-Filter-with-Set-Backs,-Dead-Ends)
97. [Seeing ANYTHING Other Than Huge-Civ Is Bad News](#Seeing-ANYTHING-Other-Than-Huge-Civ-Is-Bad-News)
98. [Our Level in the Great Filter](#Our-Level-in-the-Great-Filter)
99. [At Least Two Filters](#At-Least-Two-Filters)
100. [Fertility: The Big Problem](#Fertility:-The-Big-Problem)

# Part 4: Our Future
## Aliens
101. [Humans Are Early](#Humans-Are-Early)
102. [An Alien War Nightmare](#An-Alien-War-Nightmare)
103. [Non-Grabby Legacies](#Non-Grabby-Legacies)
104. [Why We Can’t See Grabby Aliens](#Why-We-Can’t-See-Grabby-Aliens)
105. [Beware General Visible Prey](#Beware-General-Visible-Prey)
106. [If The Future Is Big](#If-The-Future-Is-Big)
## UFOs
107. [UFOs – What The Hell?](#UFOs-–-What-The-Hell?)
108. [Our Alien Stalkers](#Our-Alien-Stalkers)
109. [On UFOs-As-Aliens Priors](#On-UFOs-As-Aliens-Priors)
110. [UFOs Show Govt. Competence As Either Surprisingly High Or Low](#UFOs-Show-Govt.-Competence-As-Either-Surprisingly-High-Or-Low)
111. [My Awkward Inference](#My-Awkward-Inference)
112. [UFO Stylized Social Facts](#UFO-Stylized-Social-Facts)
113. [Explaining Stylized UFO Facts](#Explaining-Stylized-UFO-Facts)
114. [Non-UFO Local Alien Clues](#Non-UFO-Local-Alien-Clues)
115. [UFOs and Status](#UFOs-and-Status)
## The Age of Em
116. [Why Age of Em Will Happen](#Why-Age-of-Em-Will-Happen)
117. [How To Not Die (Soon)](#How-To-Not-Die-(Soon))
118. [How Does Brain Code Differ?](#How-Does-Brain-Code-Differ?)
119. [Progeny Probabilities: Souls, Ems, Quantum](#Progeny-Probabilities:-Souls,-Ems,-Quantum)
120. [Em Redistribution](#Em-Redistribution)
## Artificial Intelligence
121. [A.I. Old-Timers](#A.I.-Old-Timers)
122. [How Lumpy AI Services?](#How-Lumpy-AI-Services?)
123. [A History Of Foom](#A-History-Of-Foom)
124. [I Still Don’t Get Foom](#I-Still-Don’t-Get-Foom)
125. [Foom Justifies AI Risk Efforts Now](#Foom-Justifies-AI-Risk-Efforts-Now)
126. [Is The City-ularity Near?](#Is-The-City-ularity-Near?)
127. [The Betterness Explosion](#The-Betterness-Explosion)
128. [An Outside View of AI Control](#An-Outside-View-of-AI-Control)
129. [AI Risk, Again](#AI-Risk,-Again)
130. [Foom Update](#Foom-Update)

## [Abstract/Distant Future Bias](#table-of-contents)
_Posted on 2008-11-26_

The latest <em>Science </em>has a <a href="http://www.sciencemag.org/cgi/reprint/322/5905/1201.pdf">psych article</a> saying we think of distant stuff more abstractly, and vice versa.  "The brain is hierarchically organized with higher points in the cortical hierarchy representing increasingly more abstract aspects of stimuli"; activating a region makes nearby activations more likely.  This has stunning implications for our biases about the future.  

<em>All of these bring each other more to mind: </em> here, now, me, us; trend-deviating likely real local events; concrete, context-dependent, unstructured, detailed, goal-irrelevant incidental features; feasible safe acts; secondary local concerns; socially close folks with unstable traits.  

<em>Conversely, all these bring each other more to mind: </em> there, then, them; trend-following unlikely hypothetical global events; abstract, schematic, context-freer, core, coarse, goal-related features; desirable risk-taking acts, central global symbolic concerns, confident predictions, polarized evaluations, socially distant people with stable traits.  

 Since these things mostly just cannot go together in reality, this must bias our thinking both about now and about distant futures.  When "in the moment," we focus on ourselves and in-our-face details, feel "one with" what we see and close to quirky folks nearby, see much as uncertain, and safely act to achieve momentary desires given what seems the most likely current situation.  Kinda like smoking weed.

 Regarding distant futures, however, we’ll be too confident, focus too much on unlikely global events, rely too much on trends, theories, and loose abstractions, while neglecting details and variation.  We’ll assume the main events take place far away (e.g., space), and uniformly across large regions.  We’ll focus on untrustworthy consistently-behaving globally-organized social-others.  And we’ll neglect feasibility, taking chances to achieve core grand symbolic values, rather than ordinary muddled values.  Sound familiar?

 <span id="more-16875"></span> 

More bluntly, we seem primed to confidently see history as an inevitable march toward a theory-predicted global conflict with an alien united them determined to oppose our core symbolic values, making infeasible overly-risky overconfident plans to oppose them.  We seem primed to neglect the value and prospect of trillions of quirky future creatures not fundamentally that different from us, focused on their simple day-to-day pleasures, mostly getting along peacefully in vastly-varied uncoordinated and hard-to-predict local cultures and life-styles.  

Of course being biased to see things a certain way doesn’t mean they aren’t that way.  But it should sure give us pause.  Selected quotes for those who want to <a href="http://www.sciencemag.org/cgi/reprint/322/5905/1201.pdf">dig deeper</a>: 


> In sum, different dimensions of psychological distance – spatial, temporal, social, and hypotheticality – correspond to different ways in which objects or events can be removed from the self, and farther removed objects are construed at a higher (more abstract) level. Three hypotheses follow from this analysis. (i) As the various dimensions map onto a more fundamental sense of psychological distance, they should be interrelated. (ii) All of the distances should similarly affect and be affected by the level of construal. People would think more abstractly about distant than about near objects, and more abstract construals would lead them to think of more distant objects. (iii) The various distances would have similar effects on prediction, evaluation, and action. … 
> [On] a task that required abstraction of coherent images from fragmented or noisy visual input … performance improved … when they anticipated working on the actual task in the more distant future … when participants thought the actual task was less likely to take place and when social distance was enhanced by priming of high social status. … Participants who thought of a more distant event created fewer, broader groups of objects. … Participants tended to describe more distant future activities (e.g., studying) in high-level terms (e.g., "doing well in school") rather than in low-level terms (e.g., "reading a textbook"). … Compared with in-groups, out-groups are described in more abstract terms and believed to possess more global and stable traits … Participants drew stronger inferences about others’ personality from behaviors that took place in spatially distal, as compared with spatially proximal locations. … Behavior that is expected to occur in the more distant future is more likely to be explained in dispositional rather than in situational terms … 
> Thinking about an activity in high level, "why," terms rather than low level, "how," terms led participants to think of the activity as taking place in more distant points in time. … Students were more confident that an experiment would yield theory-confirming results when they expected the experiment to take place in a more distant point in time. … Spatial distance enhanced the tendency to predict on the basis of the global trend rather than on the basis of local deviation. … As temporal distance from an activity (e.g., attending a guest lecture) increased, the attractiveness of the activity depended more on its desirability (e.g.,how interesting the lecture was) and less on its feasibility (e.g., how convenient the timing of the lecture was). … People take greater risks (i.e., favoring bets with a low probability of winning a high amount over those that offer a high probability to win a small amount) in decisions about temporally more distant bets.

## [Abstractly Ideal, Concretely Selfish](#table-of-contents)
_Posted on 2014-04-28_

A new <em>JPSP</em> <a href="http://psycnet.apa.org/journals/psp/106/5/790/">paper</a> [confirms](far-idealism-puzzles) that we are idealistic in far mode, and selfish in near mode. If you ask people for short abstract descriptions of their goals, they’ll say they have ideal goals. But if you ask them to describe in details what is it like to be them pursuing their goals, their selfishness shines clearly through. Details:
Completing an inventory asks the respondent to take an observer’s perspective upon the self, effectively asking, “What do you look like to others?” Imagining watching a video of oneself driving a car, playing basketball, or speaking to a friend is an experience as the self-as-actor. Rating the importance of various goals also recruits the self-as-actor. Motivated to maintain a moral reputation, the self-as-actor is infused with prosocial, culturally vetted scripts.

Another way of accessing motivation is by asking people questions about their lives. Open-ended verbal responses (e.g., narratives or implicit measures) require the respondent to produce ideas, recall details, reflect upon the significance of concrete events, imagine a future, and narrate a coherent story. In effect, prompts to narrate ask respondents, “What is it like to be you?” Imagining actually driving a car, playing basketball, or speaking to a friend is an experience as the self-as-agent (McAdams, 2013). Asking people to tell about their lives also recruits the self-as-agent. Motivated by survival, the self-as-agent is selfish in nature. …

Taken together, this leads to the prediction that frames the current research: Inventory ratings, which recruit the self-as-actor, will yield moral impressions, whereas narrated descriptions, which recruit the self-as-agent, will yield the impression of selfishness. …

The motivation to behave selfishly while appearing moral gave rise to two, divergently motivated selves. The actor—the watched self— tends to be moral; the agent—the self as executor—tends to be selfish. Each self serves its own adaptive function: The actor helps people maintain inclusion in groups, whereas the agent attends to basic survival needs. Three studies support the thesis that the actor is moral and the agent is selfish. In Study 1, actors claimed their goals were equally about helping the self and others (viz., moral); agents claimed their goals were primarily about helping the self (viz., selfish). This disparity was evident in both individualist and collectivist cultures, albeit more so among individualists. Study 2 compared actors and agents’ motives to those of people role-playing highly prosocial or selfish exemplars. In content and in the impression they made upon an outside observer, actors’ motives were similar to those of the prosocial role-players, whereas agents’ motives were similar to those of the selfish role-players. In Study 3, participants claimed that their agent’s motives were the more realistic and their actor’s motives the more idealistic of the two. When asked to take on an idealistic mindset, agents became more moral; a realistic mindset made the actor more selfish. (<a href="http://psycnet.apa.org/journals/psp/106/5/790/">more</a>)

## [We Add Near, Average Far](#table-of-contents)
_Posted on 2012-10-02_

Quick, what is the <em>best</em> gift you ever got from a woman? From your parents? From a left-handed person? From a teacher? These aren’t easy questions to answer. But they seem easier than these questions: What is the <em>total value of</em> all the gifts you ever got from women? From your parents? From left-handed folks? From teachers?

For the first set of questions you can try to think of examples of particular people in those categories, and then think of particular gifts you got from those particular people. That can help you guess at the best gift from those categories. But to estimate the total value of gifts from people in categories, you’ll have to also estimate how many gifts you ever got from folks in each category.

Note that it also seems easy to estimate the <em>average</em> value of gifts from each category. To do this, you need only remember a few gifts that fit each category, and then average their values.

As another example, imagine you are looking at building entrance laid out in multi-colored tiles. Some tiles are blue, some red, some green, etc. You are looking at it from a distance, at an angle, in variable lighting. In this situation it will be much easier to estimate if there is more blue than red area in the tiles, than to estimate how many square inches of blue tile area is in that entrance. This later estimate requires you to additionally estimate distances to reference points, to estimate the total surface area.

These examples suggest that when we think in far mode, without a structured systematic representation of our topic, it is usually easier to average than to add values. So averaging is what we’ll tend to do. All of which I mention to introduce to a fascinating paper that I just noticed, even though it got a lot of publicity last December:

This analysis introduces the Presenter’s Paradox. Robust findings in impression formation demonstrate that perceivers’ judgments show a weighted averaging pattern, which results in less favorable evaluations when mildly favorable information is added to highly favorable information. Across seven studies, we show that presenters do not anticipate this averaging pattern on the part of evaluators and instead design presentations that include all of the favorable information available. This additive strategy (“more is better”) hurts presenters in their perceivers’ eyes because mildly favorable information dilutes the impact of highly favorable information. For example, presenters choose to spend more money to make a product bundle look more costly, even though doing so actually cheapened its value from the evaluators’ perspective. (<a href="http://www.jstor.org/stable/10.1086/664497">more</a>)

The authors attribute this to a near-far effect:

Presenters face many pieces of potentially relevant information and need to determine, in a bottom-up fashion, which ones to include in a presentation. This presumably draws attention to each individual piece of information as a discrete entity and a focus on piecemeal processing. If a given piece of information exceeds a neutrality threshold, the presenter will conclude that it is compatible with the message he or she seeks to convey and will include it. This results in presentations that would fare better under an adding rather than averaging rule. In contrast, evaluators’ primary task is to make a summary judgment of the overall presentation, which fosters a focus on holistic processing and the big picture and results in an averaging pattern as observed in many impression formation studies.

Additional experiments confirm this near-far interpretation. Those who prepare presentations and proposals tend to focus on them in detail, and so add part values in near mode style, while those who consume such presentations or proposals tend to pay much less attention, and so average their values in far mode style.

This result seems to me quite pregnant with interesting implications, none of which were mentioned in the dozen blog posts on the subject that have appeared since last December. So I guess it’s up to me.

First, this result predicts the usual academic advice to delete publications from low ranked journals from your vita. Yes those extra publications took extra work, and show more total intellectual contribution, but distracted readers evaluate you by averaging your publications, not adding them.

Second, this also predicts that academia will tend in general to neglect conclusions suggested by lots of weak clues, relative to conclusions based on a single strong theory or empirical comparison. People with a practical understanding of particular areas will correctly complain that academics tend too much to latch on to a few easy to explain and justify arguments, at the cost of lots of detail that practitioners appreciate.

Third, this predicts that in morality and politics, which are especially far sorts of topics, arguments tend to be won by those who push simple strong principles, even though people privately tend to choose actions that deviate from such principles. For example, while laws say no one can get medical advice from non-doctors, on the grounds that docs know best, but given a private choice most of us would often let other considerations convince us to listen to non-docs. While actions tend to be chosen in a near mode where lots of other weaker considerations get added, people know their best chance for winning an argument with a distracted audience is to focus on their one strongest point.

Fourth, this predicts Tetlock’s hedgehog vs. foxes result. Foreign policy is an especially far view sort of subject, and experts who focus on one strongest consideration get the most respect and attention, but experts who rely on many considerations, which are on average weaker, are more accurate.

Futurism is probably the most far view sort of topic, so I’d guess that all this holds there the most strongly. That is, while the most futurists who get the most attention from distracted audiences are those who harp endlessly on one clear plausible idea, the most accurate futurists are probably those who know and use hundreds of clues, many of them weak. Alas this is a problem for those of us who want to consider some aspect of the future in detail, since we quickly run out of strong principles, and then have to rely more on many weak clues.

<strong>Added Nov 25, 2012</strong>: <a href="http://80000hours.org/blog/118-why-don-t-people-help-others-more-part-1">This post</a> gives data showing people donate money based more on the average than the total sympathy of the recipients. So you are better off asking for donations to help a particular especially sympathetic recipient, than to help many such folks.

## [Why We Don’t Know What We Want](#table-of-contents)
_Posted on 2022-10-27_

> Moons and Junes and Ferris wheels  
> The dizzy dancing way that you feel  
> As every fairy tale comes real  
> I’ve looked at love that way
> 
> But now it’s just another show  
> And you leave ’em laughing when you go  
> And if you care, don’t let them know  
> Don’t give yourself away
> 
> I’ve looked at love from both sides now  
> From give and take and still somehow  
> It’s love’s illusions that I recall  
> I really don’t know love  
> Really don’t know love at all
> 
> _Both Sides Now_, Joni Mitchell 1966.

If you look at two things up close, it is usually pretty easy to tell which one is closest. And also to tell their relative sizes, e.g., which one might fit inside the other. But if you look far in the distance, such as toward the sky or the horizon, it gets much harder to tell relative sizes or distances. While you might notice that one thing occludes another, when considering unknown things in different directions it is harder to tell relative sizes or distances.

I see similar effects also for things that are more “distant” in other ways, such as in time, social distance, or hypothetically; it also seems harder to judge relative distance when things are further away in these ways. Furthermore, it seems harder to tell of two abstract descriptions which is more abstract, but easier to tell which of two detailed things which has more detail. Thus in the sense of [near-far](https://www.overcomingbias.com/2010/06/near-far-summary.html) (or construal-level) theory, it seems that we generally find it harder to compare relative distances when things are further away.

According to near-far theory, we also frame our more stable, general, and fundamental goals as more far and abstract, compared to the more near local considerations that constrain our plans. Thus this theory seems to predict that we will have more trouble comparing the relative value of our more abstract values. That is, when comparing two general persistent values, we will find it hard to say which one we value more. Thus near-far theory predicts a big puzzling human feature: we know surprisingly little about what we want. For example, we [find](https://www.overcomingbias.com/2021/12/what-hypocrisy-feels-like.html) it very hard to imaging concrete, coherent, and attractive utopias.

When we see an object from up close, and then we later see it from afar, we often remember its details from when we saw it up close. So similarly, we might learn to compare our general values by remembering examples of concrete decisions where such values were in conflict. And we do often have concrete situations where we are aware that our general values apply to those concrete cases. Such as when we are very hungry, horny, injured, or socially embarrassed. Why don’t we learn our values from those?

Here I will invoke my [theory](https://www.overcomingbias.com/2022/08/sacredness-as-unity-induced-abstraction.html) of the sacred: for some key values and things, we set our minds to try to always see them in a rather far mode, no matter how close we are to them. This enables different people in a community to bond together by seeing those sacred things in the same way, even when some of them are much closer to them than others. And this also enables a single person to better maintain a unified identity and commitments over time, even when that person sees concrete examples from different distances at different times in their life. (I thank Arnold Brooks for pointing this out in [an](https://mindsalmostmeeting.com/episodes/the-sacred) upcoming MAM podcast.)

For example, most of us have felt strong feelings of lust, limerence, and attachment to other people at many times during our lives. So we should have plenty of data on which to base rough estimates of what exactly is “love”, and how much we value it compared to other things. But our treating love as sacred makes it harder to use that data to construct such a detailed and unified account. Even when we think about concrete examples up close, it seems hard to use those to update our general views on “love”. We still “really don’t know love at all.”

Because we really can’t see love up close and in detail. Because we treat love as sacred. And sacred things we see from afar, so we can see them together.

## [We See the Sacred from Afar, to See It Together](#table-of-contents)
_Posted on 2022-08-27_

I’ve recently been [trying](https://www.overcomingbias.com/2022/07/beware-sacred-cows.html) to make sense of our concept of the “sacred”, by puzzling over its many correlates. And I think I’ve found a way to make more sense of it in terms of near-far (or “construal level”) theory, a [framework](https://www.overcomingbias.com/tag/nearfar) [that](https://www.overcomingbias.com/2008/11/abstractdistant.html) [I’ve](https://www.overcomingbias.com/2010/05/far-is-hypocritical.html) [discussed](https://www.overcomingbias.com/2010/06/near-far-summary.html) [here](https://www.overcomingbias.com/2014/07/near-far-work-continues.html) many times before.

When we look at a scene full of objects, a few of those objects are big and close up, while a lot more are small and far away. And the core idea of near-far is that it makes sense to put more mental energy into analyzing each object up close, objects that matters to us more, by paying more attention to their detail, detail often not available about stuff far away. And our brains do seem to be organized around this analysis principle.

That is, we do tend to think less, and think more abstractly, about things far from us in time, distance, social connection, or hypothetically. Furthermore, the more abstractly we think about something, the more distant we tend to assume are its many aspects. In fact, the more distant something is in any way, the more distant we tend to assume it is in other ways.

This all applies not just to dates, colors, sounds, shapes, sizes, and categories, but also to the goals and priorities we use to evaluate our plans and actions. We pay more attention to detailed complexities and feasibility constraints regarding actions that are closer to us, but for far away plans we are content to think about them more simply and abstractly, in terms of relatively general values and principles that depend less on context. And when we think about plans more abstractly, we tend to assume that those actions are further away and matter less to us.

Now consider some other ways in which it might make sense to simplify our evaluation of plans and actions where we care less. We might, for example, just follow our intuitions, instead of consciously analyzing our choices. Or we might just accept expert advice about what to do, and care little about experts incentives. If there are several relevant abstract considerations, we might assume they do not conflict, or just pick one of them, instead of trying to weigh multiple considerations against each other. We might simplify an abstract consideration from many parameters down to one factor, down to a few discrete options, or even all the way down to a simple binary split.

It turns out that all of these analysis styles are characteristic of the sacred! We are not supposed to calculate the sacred, but just follow our feelings. We are to trust priests of the sacred more. Sacred things are presumed to not conflict with each other, and we are not to trade them off against other things. Sacred things are idealized in our minds, by simplifying them and neglecting their defects. And we often have sharp binary categories for sacred things; things are either sacred or not, and sacred things are not to be mixed with the non-sacred.

All of which leads me to suggest a theory of the sacred: when a group is united by valuing something highly, they value it in a style that is very abstract, having the features usually appropriate for quickly evaluating things relatively unimportant and far away. Even though this group in fact tries to value this sacred thing highly. Of course, depending on what they try to value, such attempts may have only limited success.

For example, my society (US) tries to value medicine sacredly. So ordinary people are reluctant to consciously analyze or question medical advice; they are instead to just trust its priests, namely doctors, without looking at doctor incentives or track records. Instead of thinking in terms of multiple dimensions of health, we boil it all down to a single health dimension, or even a binary of dead or alive.

Instead of seeing a continuum of cost-effectiveness of medical treatments, along which the rich would naturally go further, we want a binary of good vs bad treatments, where everyone should get the good ones no matter what their cost, and regardless of any other factors besides a diagnosis. We are not to make trades of non-sacred things for medicine, and we can’t quite believe it is ever necessary to trade medicine against other sacred things. Furthermore, we want there to be a sharp distinction between what is medicine and what is not medicine, and so we struggle to classify things like mental therapy or fresh food.

Okay, but if we see sacred things as especially important to us, why ever would we want to analyze them using styles that we usually apply to things that are far away and the least important to us? Well one theory might be that our brains find it hard to code each value in multiple ways, and so typically code our most important values as more abstracted ones, as we tend to apply them most often from a distance.

Maybe, but let me suggest another theory. When a group unites itself by sharing a key “sacred” value, then its members are especially eager to show each other that they value sacred things in the same way. However, when group members hear about and observe how an associate makes key sacred choices, they will naturally evaluate those choices from a distance. So each group member also wants to look at their own choices from afar, in order to see them in the same way that others will see them.

In this view, it is the fact groups tend to be united by sacred values that is key to explaining why they treat such values in the style usually appropriate for relatively unimportant things seen from far away, even though they actually want to value those things highly. Even though such a from-a-distance treatment will probably lead to a great many errors and misjudgments when actually trying to promote that thing.

You see, it may be more important to groups to pursue a sacred value together than to pursue it effectively. Such as the way the US spends 18% of GDP on medicine, as a costly signal of how sacred medicine is to us, even though the marginal health benefit of our medical spending [seems](https://www.overcomingbias.com/2021/12/karnataka-hospital-insurance-experiment.html) to be near zero. And we show little interest in [better](https://www.overcomingbias.com/2019/07/radical-pay-for-results.html) institutions that could make such spending far more cost effective.

Because at least this way we all see each other’s ineffective medical choices in the same way. We agree on what to do. And after all, that’s the important thing about medicine, not whether we live or die.

**Added 10Sep:** Other [dual process](https://link.springer.com/article/10.1007/s10803-022-05733-6) theories of brains give similar predictions.

## [The Future Seems Shiny](#table-of-contents)
_Posted on 2010-10-19_

<em>The future is not the realization of our hopes and dreams, a warning to mend our ways, an adventure to inspire us, nor a romance to touch our hearts. The future is just another place in spacetime. </em>(<a href="http://hanson.gmu.edu/hardscra.pdf">more</a>)

Our minds have two very different modes (and a range between). We model important things nearby in more detail than less important things far away.  The more nearby aspects we notice in a thing, the more other nearby aspects and relevant detail we assume it has. On the other hand, the more far aspects we see in something, the more other far aspects we assume it has, and the more we reason about it via broad categories and relations. (More on near vs. far thinking [here](near-far-summary) and <a href="http://www.psych-it.com.au/Psychlopedia/article.asp?id=79">here</a>.)
<img alt="futurecloudcity" class="alignleft size-medium wp-image-24603" height="133" loading="lazy" sizes="(max-width: 300px) 100vw, 300px" src="http://overcomingbias-assets.s3.amazonaws.com/wp-content/uploads/2010/10/futurecloudcity-300x133.jpg" srcset="https://www.overcomingbias.com/wp-content/uploads/2010/10/futurecloudcity-300x133.jpg 300w, https://www.overcomingbias.com/wp-content/uploads/2010/10/futurecloudcity-1024x455.jpg 1024w, https://www.overcomingbias.com/wp-content/uploads/2010/10/futurecloudcity.jpg 1149w" title="futurecloudcity" width="300"/>

Since the future is far in time, thinking about it tends to invoke a far mode of thought, which introduces other far mode defaults into our image of the future.  And thinking about the far future makes us think especially far. Of course many other considerations influence any particular imagined future, but it can help to understand the assumptions your mind is primed to make about the far future, regardless of whether those assumptions are true.

<img alt="futurebuilding" class="size-medium wp-image-24604 alignright" height="300" loading="lazy" sizes="(max-width: 200px) 100vw, 200px" src="http://overcomingbias-assets.s3.amazonaws.com/wp-content/uploads/2010/10/futurebuilding-200x300.jpg" srcset="https://www.overcomingbias.com/wp-content/uploads/2010/10/futurebuilding-200x300.jpg 200w, https://www.overcomingbias.com/wp-content/uploads/2010/10/futurebuilding.jpg 500w" title="futurebuilding" width="200"/>For example, since we expect things further away in time to also be further away in space, we expect future folk to live further away, such as in space, and to habitually travel longer distances.  Since the distant past is also further away in time, we also expect past folk to live further away and travel longer distances, but the many concrete details we know about the past reduces this effect.

Since blue light scatters more easily than red, far away things in our field of view tend to look more blue. So we expect future stuff to look blue. And since blue stuff looks cold, we expect future stuff to look cold. Finally, since we expect far away things to have less detail, we tend to imagine them with fewer parts and flourishes, and less detailed textures and patterns. The future is not paisley.

And in fact, if you Goggle “futuristic style” <a href="http://www.google.com/images?q=futuristic+style">images</a>, you’ll tend to see images like those in this post – simple, smooth, cool, blue, and sky/spacy.  In a word, “shiny.”<span id="more-24602"></span>

<img alt="jane_jetson" class="alignleft size-medium wp-image-24605" height="300" loading="lazy" sizes="(max-width: 163px) 100vw, 163px" src="http://overcomingbias-assets.s3.amazonaws.com/wp-content/uploads/2010/10/jane_jetson-163x300.jpg" srcset="https://www.overcomingbias.com/wp-content/uploads/2010/10/jane_jetson-163x300.jpg 163w, https://www.overcomingbias.com/wp-content/uploads/2010/10/jane_jetson.jpg 273w" title="jane_jetson" width="163"/>We also tend to assume there are fewer relevant categories of far things. So we’ll tend to assume future folk have fewer kinds of food, furniture, cars, houses, roads, buildings, and land uses, whose styles of use vary less from place to place. Instead of seeing a million variations bleeding into each other in dizzying complexity, we tend to assume there are fewer more discrete types, with less variation within each type and larger differences between types. For example, futuristic movies often have everyone wearing very similar clothes.

Another example is that we tend to assume future creatures are divided into relatively distinct groups whose internal divisions are less important. And since creatures that are more different seem further in social distance, we expect future groups to differ more from each other than current groups.  So we expect eloi vs. morlock, romulan vs. klingon, ape vs. human, human vs. robot, etc.

Far tends to be happy, and high in status, power, and confidence.  Conformity and obeying authority is near, but supporting underdogs is far. Sex, money, and temptation tend to be near, while love, satisfaction, trust, and self-control are far. So we often assume future folks have forgotten how to have sex, as in Sleeper or Barbarella, or that money motives are less common, as in Star Trek.

<img alt="jetsons_firebird" class="size-medium wp-image-24606 alignright" height="225" loading="lazy" sizes="(max-width: 300px) 100vw, 300px" src="http://overcomingbias-assets.s3.amazonaws.com/wp-content/uploads/2010/10/jetsons_firebird-300x225.jpg" srcset="https://www.overcomingbias.com/wp-content/uploads/2010/10/jetsons_firebird-300x225.jpg 300w, https://www.overcomingbias.com/wp-content/uploads/2010/10/jetsons_firebird.jpg 600w" title="jetsons_firebird" width="300"/>In far mode we tend to focus more on our simple abstract ideals and values, relative to messy desires and practical constraints. We also tend to neglect our messy internal contradictions and conflicts, and therefore assume our values and actions are coherent and consistent. So in far mode we tend more to explain good acts as virtue, and bad acts as vice or evil.  We assume future folk are less driven by base desires, more strongly committed to their ideals, less tolerant of domination, more morally enlightened, and more morally judgmental about others’ failings.

We therefore tend to assume that future folk feel relatively moral, confident, and strong, and that future groups have less trouble coordinating to achieve common ends (making us especially blind to [coordination being hard](coordination-is-hard)). So we can more easily imagine stark uncompromising conflicts between distinct future groups. Of course robots will war with humans, we think.  And since we tend to feel more moral and uncompromising about the future, we more accept future uncompromising self-righteous conflict, relative to such conflict today.
<img alt="futurehouse" class="alignleft size-medium wp-image-24607" height="199" loading="lazy" sizes="(max-width: 300px) 100vw, 300px" src="http://overcomingbias-assets.s3.amazonaws.com/wp-content/uploads/2010/10/futurehouse-300x199.jpg" srcset="https://www.overcomingbias.com/wp-content/uploads/2010/10/futurehouse-300x199.jpg 300w, https://www.overcomingbias.com/wp-content/uploads/2010/10/futurehouse.jpg 554w" title="futurehouse" width="300"/>Math and logic analysis is near, while creative analogy is far. So we tend to reason about the future via analogy rather than precise analysis, feeling more comfortable using metaphors and broad concepts like “exploitation”, “progress”, “boredom”, or “intelligence.” Math models of the far future are quite rare. Far mode minds tends to be more confident in the trends or theories they use, making them especially confident in trends and theories used to forecast the far future.  Rather than seeing our theories about the future as weak all-else-equal tendencies, we are tempted to see them as absolute laws with rare exceptions.

We also tend to assume that future folk themselves rely more on analogy than analysis.  They may have great tech, but we tend to see it arising more from rare sparks of creative genius than from vast armies devoting decades of attention to mind-numbing detail.

<img alt="futureinterior" class="size-medium wp-image-24608 alignright" height="138" loading="lazy" sizes="(max-width: 300px) 100vw, 300px" src="http://overcomingbias-assets.s3.amazonaws.com/wp-content/uploads/2010/10/futureinterior-300x138.jpg" srcset="https://www.overcomingbias.com/wp-content/uploads/2010/10/futureinterior-300x138.jpg 300w, https://www.overcomingbias.com/wp-content/uploads/2010/10/futureinterior.jpg 554w" title="futureinterior" width="300"/>Likely familiar events are near, while unlikely novel events are far. So we think it more likely that “there be dragons” in distant lands.  Scenarios that would seem too unlikely to consider today can seem reasonable possibilities for a far future. In fact, we may well reject future scenarios that don’t seem strange enough.

While we tend to imagine that trends <em>during</em> the future will be followed with few deviations, we are pretty willing to believe theories which predict that today’s trends, even long term trends, won’t continue into the future.  For example, even though natural resource prices rarely rise, we are willing to believe theories that resources will soon “run out”, so that prices greatly rise.

<img alt="futuresink" class="alignleft size-medium wp-image-24609" height="300" loading="lazy" sizes="(max-width: 300px) 100vw, 300px" src="http://overcomingbias-assets.s3.amazonaws.com/wp-content/uploads/2010/10/futuresink-300x300.jpg" srcset="https://www.overcomingbias.com/wp-content/uploads/2010/10/futuresink-300x300.jpg 300w, https://www.overcomingbias.com/wp-content/uploads/2010/10/futuresink-150x150.jpg 150w, https://www.overcomingbias.com/wp-content/uploads/2010/10/futuresink.jpg 450w" title="futuresink" width="300"/>Since important things seem nearer to us, stronger emotions feel nearer, and so we have weaker motives and emotions regarding far things. Instead of being filled with elation or terror regarding good or bad things that might happen in the far future, we tend to treat such events more philosophically, and to assume future folk will do so as well. In <a href="http://www.youtube.com/watch?v=NQu_RRLbVDA">a scene</a> from Monty Python’s <em>Meaning of Life</em>, a woman is willing to die to donate a liver after she’s seen how vast is the universe. Similarly, in far mode even [human extinction](nature-endorses-human-extinction) may seem no big deal; “it was our time to go.”
Tasting and touching tend to feel near, while seeing and hearing tend to feel far.  So we mainly imagine what the future looks and sounds like, relative to its taste or touch.  Words and polite speech tend to be far, while voices, grunts, and slang tend to be near.  So we more often imagine future folks’ polite words than their earthy sounds. We imagine future folk being relatively cerebral – we see them as relatively patient in listening to long intellectual speeches, and less often imagine their grunts or wild passionate music.

Of course it remains possible that many of the above far-mode-based expectations about the future will be realized. Maybe stuff in the future really will be simple, smooth, blue, and cold. Maybe future creatures will be spread across space, habitually travel far, and be divided into a few distinct types that vary greatly between types, very little internally, and coordinate well to achieve group ends. Maybe future folk really will be more driven by abstract ideals, with moral judgements driving uncompromising self-righteous conflict between groups. Maybe their innovations really will come from a few geniuses. Maybe the future will be very strange, yet is predictable from powerful theories available now. Maybe future trends really will have few deviations, and future folk will accept their demises philosophically. But please at least consider the possibility that you expect such things not because you have strong supporting evidence, but because your mind was just built to expect such things.

<strong>Added</strong>: Coincidentally, I was just quoted in <a href="http://www.npr.org/templates/story/story.php?storyId=130585002">this NPR article</a> saying the future isn’t what it used to be.

## [Doubting My Far Mind](#table-of-contents)
_Posted on 2010-09-29_

A while back I was discussing long term future values, i.e., what we want our descendants to be or achieve, and I realized that pretty much any simple description of such values seems crazy.  With a little effort it is easy to find counter-examples, or at least discomfort-examples, to most any description much beyond “I hope future folks get what they want.”

I’ve also noticed that among smart folks, the most successful keep their smarts on a short leash.  They use their smarts to make the sale, win the case, pass the test, get published, etc., but they don’t use much smarts to consider whether they really want to make the sale, win the case, etc.  Oh sure they might express some angst at a Saturday dinner, but come Monday they are back on the job.

In contrast, on average smart folks gain far less success when they seriously apply their smarts to big pictures, reconsidering what they want, what we really know, how the world is organized, what they can do to make the world a better place, and so on.  They go off in a thousand directions, and while some might break new ground, on average such smart folk gain much less personal success, and may well do less to help the world.

I count myself in this [smart sincere syndrome](the-smart-sincere-syndrome).  I’m often distracted by what I see as important neglected topics, which offer fewer academic or other rewards. These topics have included future robot econ, foundations of quantum mechanics, prediction markets, and much more.  Lately I find myself obsessed by a [homo hypocritus](homo-hipocritus) account of human nature. I’m not at all clear on the best route to pursue this, but no route seems especially promising for success in ordinary terms, or to rely heavily on skills I’ve previously invested in developing.  Yet on I go.
Applying these observations to myself, I think I have to conclude that I just don’t know much about what I really want, or what I should do to get it, in general far terms, and can’t trust my far mind to tell me much.  Lacking a good basis for challenging ordinary concepts of success, I should accept them. If I’m feeling insecure, where success matters more, I should follow the example of smart successful folks in positions similar to me.  You know, write academic papers or books, or do business consulting.

In contrast, if I’m feeling rich and comfortable, and so less in need of success, well then I should enjoy myself by doing whatever seems appealing at the time, as long as that doesn’t threaten my basic stable position in life.  I’m capable of doing a lot more abstract thinking about what is good for me or the world, but at the moment I just don’t trust that thinking much.  What I most enjoy may well be to think on big far topics, but I shouldn’t presume I have a coherent integrated account showing their true global importance.

## [Beware the Inside View](#table-of-contents)
_Posted on 2007-07-25_

Instead of watching fireworks on July 4, I did 1500 piece jigsaw puzzle of fireworks, my first jigsaw in at least ten years.  Several times I had the strong impression that I had carefully eliminated every possible place a piece could go, or every possible piece that could go in a place.  I was very tempted to conclude that many pieces were missing, or that the box had extra pieces from another puzzle.  This wasn’t impossible – the puzzle was an open box a relative had done before.  And the alternative seemed humiliating.  

 But I allowed a very different part of my mind, using different considerations, to overrule this judgment; so many extra or missing pieces seemed unlikely.  And in the end there was only one missing and no extra pieces.  I recall a similar experience when I was learning to program.  I would carefully check my program and find no errors, and then when my program wouldn’t run I was tempted to suspect compiler or hardware errors.  Of course the problem was almost always my fault.   

 Most, perhaps all, ways to overcome bias seem like this.  In the language of Kahneman and Lovallo’s classic ’93 paper, we allow an outside view to overrule an inside view.  From <a href="http://www.psychologie.leidenuniv.nl/seno/content_docs/Medewerkers_R_tot_Z/SteinelW/kahnemanlovallo1993.pdf">their paper</a>: 

 <span id="more-17938"></span> 


> Two distinct modes of forecasting were applied to the same problem in this incident.  The <em>inside view</em> of the problem is the one that all participants adopted.  An inside view forecast is generated by focusing on the case at hand, by considering the plan and the obstacles to its completion, by constructing scenarios of future progress, and by extrapolating current trends.  The <em>outside view</em> is the one that the curriculum expert was encouraged to adopt.   It essentially ignores the details of the case at hand, and involves no attempt at detailed forecasting of the future history of he project.  Instead, it focuses on the statistics of a class of cases chosen to be similar in relevant respects to the present one.  The case at hand is also compared to other members of the class, in an attempt to assess its position in the distribution of outcomes for the class.  … 
>  The inside and outside views draw on different sources of information, and apply different rules to its use.  … It should be obvious that when both methods are applied with equal intelligence and skill, the outside view is much more likely to yield a realistic estimate. … it is a serious error to assume the outcomes of the most likely scenarios are also the most likely, and that the outcomes for which no plausible scenarios come to mind are impossible.  … It is a conservative approach, which will fail to predict extreme and exceptional events, but will do well with common ones.  … 
>  Our main observation, which is psychological: the inside view is overwhelmingly preferred in intuitive forecasting.  The natural way to think about a problem is to bring to bear all one knows about it, with special attention to its unique features.  The intellectual detour into the statistics of related cases is seldom chosen spontaneously. Indeed, the relevance of the outside view is sometimes explicitly denied: physicians and lawyers often argue against the application of statistical reasoning to particular cases.   In these instances, the preference for the inside view almost bears a moral character.  The insider view is valued as a serious attempt to come to grips with the complexities of the unique case at hand, and the outside view is rejected for relying on crude analogy from superficially similar instances.  …
>  The contrast between the inside and outside views has been confirmed in systematic research.  .  … A typical result is that respondents are only correct on about 80% of cases when they describe themselves as "99% sure."  People are overconfident in evaluating the accuracy of their beliefs one at a time.  It is interesting, however, that there is no evidence of overconfidence bias when respondents are asked after the session to estimate the number of questions for which they picked the correct answer. 


If overcoming bias comes down to having an outside view overrule an inside view, then our questions become: what are valid outside views, and what will motivate us to apply them?

## [Are Meta Views Outside Views?](#table-of-contents)
_Posted on 2008-06-22_

An inside view focuses on internals of the case at hand, while an [outside view](singularity-out) compares this case to other similar cases.  The less you understand about something the harder it is to apply either an inside or an outside view.  So the simplest approach would be to just do the best you could with each view and then combine their results in some simple way.  
 Can we do better?  Perhaps, if we know something about when inside views tend to do better or worse, compared to outside views.   For example, we should probably emphasize views that give more confident estimates, and de-emphasize views from those biased by self-interest.   But do we know anything about on what <em>topics </em>to prefer an inside or outside view?

 It is not clear to me that we really do know much about this.  But whatever framework we use to make this judgment, it seems to me to count as a <em>meta-view</em>, a view about views.  Furthermore, while it is easy to imagine useful outside meta-views, which compare this view-choice situation to other related view-choice situations, it is much harder to imagine useful inside meta-views, where you go through some detailed calculation to decide which view to prefer.  

This suggests to me that most useful meta views are outside meta views.  If you are going to reject an outside view in favor of an inside view on the basis of some insight on when inside views work better, you will be relying on an outside metaview.   So it seems you can’t escape embracing some outside view, though you might embrace a meta outside view instead of a basic outside view.

## [Disagreement Is Near-Far Bias](#table-of-contents)
_Posted on 2009-01-14_

Back in November I read <a href="http://www.sciencemag.org/cgi/reprint/322/5905/1201.pdf">this</a> Science review by Nira Liberman and Yaacov Trope on their awkwardly-named "<a href="http://www.psych-it.com.au/Psychlopedia/article.asp?id=79">Construal level theory</a>", and wrote [a post](abstractdistant) I estimated "to be the most dense with useful info on identifying our biases I've ever written": 
<dl>
<dd><strong>[NEAR] </strong><em>All of these bring each other more to mind: </em>here, now, me, us; trend-deviating likely real local events; concrete, context-dependent, unstructured, detailed, goal-irrelevant incidental features; feasible safe acts; secondary local concerns; socially close folks with unstable traits.  
</dd>
<dd><strong>[FAR] </strong><em>Conversely, all these bring each other more to mind: </em>there, then, them; trend-following unlikely hypothetical global events; abstract, schematic, context-freer, core, coarse, goal-related features; desirable risk-taking acts, central global symbolic concerns, confident predictions, polarized evaluations, socially distant people with stable traits.  </dd>
</dl>
Since then I've become even more impressed with it, as it explains most biases I know and care about, including muddled thinking about economics and the future.  For example, Ross's famous "<a href="http://en.wikipedia.org/wiki/Fundamental_attribution_error">fundamental attribution error</a>" is a trivial application.  

 The key idea is that when we consider the same thing from near versus far, different features become salient, leading our minds to different conclusions.  This is now my best account of disagreement.  We disagree because we explain our own conclusions via detailed context (e.g., arguments, analysis, and evidence), and others' conclusions via coarse stable traits (e.g., demographics, interests, biases).  While we know abstractly that we also have stable relevant traits, and they have detailed context, we simply assume we have taken that into account, when we have in fact done no such thing.  

 For example, imagine I am well-educated and you are not, and I argue for the value of education and you argue against it.  I find it easy to dismiss your view as denigrating something you do not have, but I do not think it plausible I am mainly just celebrating something I do have.  I can see all these detailed reasons for my belief, and I cannot easily see and appreciate your detailed reasons.  

 And this is the key error: our minds often assure us that they have taken certain factors into account when they have done no such thing.  I tell myself that of course I realize that I might be biased by my interests; I'm not that stupid.  So I must have already taken that possible bias into account, and so my conclusion must be valid even after correcting for that bias.  But in fact I haven't corrected for it much at all; I've just assumed that I did so.

## [Others’ Views Are Detail](#table-of-contents)
_Posted on 2010-07-29_

In Jan ’09 [I wrote](disagreement-is-nearfar-bias):
This is now my best account of disagreement.  We disagree because we explain our own conclusions via detailed context (e.g., arguments, analysis, and evidence), and others’ conclusions via coarse stable traits (e.g., demographics, interests, biases).  While we know abstractly that we also have stable relevant traits, and they have detailed context, we simply assume we have taken that into account, when we have in fact done no such thing.

New data suggests a different view:

The results of 4 studies suggest that when individuals mentally construe an attitude object concretely, either because it is psychologically close or because they have been led to adopt a concrete mindset, their evaluations flexibly incorporate the views of an incidental stranger. However, when individuals think about the same issue more abstractly, their evaluations are less susceptible to incidental social influence and instead reflect their previously reported ideological values. …

The results of these four studies appear quite robust: They held for a variety of political and social attitude objects (including general issues and specific policies related to four different and important topics: organ donation, euthanasia, illegal immigration, and universal health care), and they emerged across different types of evaluative responding (overall attitudes, voting intentions, and elaboration positivity) as well as different manipulations (temporal distance and two direct manipulations of construal level). …

Whereas local evaluations serve to guide responding in the here and now by flexibly incorporating incidental contextual details, global evaluations can help to guide action at a distance by consistently reflecting a person’s core values and ideals, which are likely to be shared within important relationships or groups. (<a href="http://www.psych.nyu.edu/tropelab/publications/LedgerwoodTropeChaiken2010.pdf">more</a>)

Here’s my tentative reading of this.  We pay more attention to messy detail in near far, relative to far view. On any given topic, we see our core values and explicit reasons as big important central influences on our opinions, whereas <em>we see the opinions of others more as incidental detail</em>.  So we think we should listen to random other people more on small detail topics, and less on big important topics.

Random others are little people, you see, which are fit for little topics.  But they are just not big and important enough to influence us on big important topics; only big important things should do that. Like big explicit reasons. This makes us tend to disagree greatly with most others on what we see as big topics, though much less on millions of small detail topics, like “there’s another tree.”

Perhaps it makes sense to keep random others from influencing our core values (which are about us), but on questions of fact (which are about the world out there), most folks seem to make the [huge mistake](disagreement-de) of vastly underestimating the info contained of others’ opinions, relative to the info contained in their own explicit reasons. Yes there may be people and times when others’ opinions really do contain relatively little info, but most folks are far too quick to assume that this applies to them now.

## [Why Be Contrarian?](#table-of-contents)
_Posted on 2017-11-25_

While I’m a contrarian in many ways, it think it fair to call my ex-co-blogger Eliezer Yudkowsky even more contrarian than I. And he has just published a book, <a href="https://equilibriabook.com"><em>Inadequate Equilibria</em></a>, defending his contrarian stance, against what he calls “modesty”, illustrated in these three quotes:

<ol>
<li>I should expect a priori to be below average at half of things, and be 50% likely to be of below average talent overall; … to be mistaken about issues on which there is expert disagreement about half of the time. …</li>
<li>On most issues, the average opinion of humanity will be a better and less biased guide to the truth than my own judgment. …</li>
<li>We all ought to [avoid disagreeing with] each other as a matter of course. … You can’t trust the reasoning you use to think you’re more meta-rational than average.</li>
</ol>
In contrast, Yudkowsky claims that his book readers can realistically hope to become successfully contrarian in these 3 ways:

<ol>
<li>0-2 lifetime instances of answering “Yes” to “Can I substantially improve on my civilization’s current knowledge if I put years into the attempt?” …</li>
<li>Once per year or thereabouts, an answer of “Yes” to “Can I generate a synthesis of existing correct contrarianism which will beat my current civilization’s next-best alternative, for just myself. …</li>
<li>Many cases of trying to pick a previously existing side in a running dispute between experts, if you think that you can follow the object-level arguments reasonably well and there are strong meta-level cues that you can identify. … [This] is where you get the fuel for many small day-to-day decisions, and much of your ability to do larger things.</li>
</ol>
Few would disagree with his claim #1 as stated, and it is claim #3 that applies most often to reader lives. Yet most of the book focuses on claim #2, that “for just myself” one might annually improve on the recommendation of our best official experts.<span id="more-31673"></span>

The main reason to accept #2 is that there exist what we economists call “agency costs” and other “market failures” that result in “inefficient equilibria” (which can also be called “inadequate”). Our best experts don’t try with their full efforts to solve your personal problems, but instead try to win the world’s somewhat arbitrary games. Games that individuals just cannot change. Yudkowsky may not be saying anything especially original here about how broken the world can be, but his discussion is excellent, and I hope it will be widely read.

Yudkowsky gives some dramatic personal examples, but simpler examples can also make the point. For example, one can often use maps or a GPS to improve on official road signs saying which highway exits to use for particular destinations, as sign officials often placate local residents seeking less through-traffic. Similarly, official medical advisors tend to advise medical treatment too often relative to doing nothing, official education experts tend to advise education too often as a career strategy, official investment advisors suggest active investment too often relative to index funds, and official religion experts advise religion too often relative to non-religion. In many cases, one can see plausible system-level problems that could lower the quality of official advice, inducing these experts to try harder to impress and help each other than to help clients.

To explain how inadequate are many of our equilibria, Yudkowsky contrasts them with our most adequate institution: competitive speculative financial markets, where it is kind of crazy to expect your beliefs to be much more accurate than are market prices. He also highlights the crucial importance of competitive meta-institutions, for example lamenting that there is no place on Earth where one can pay to try out arbitrary new social institutions. (Alas he doesn’t endorse my call to fix much of the general problem of disagreement via speculative markets, especially on meta topics. Like [many others](prediction-markets-update) he seems more interested in bets as methods of personal virtue than as institution solutions.)
However, while understanding that systems are often broken can lead us to accept Yudkowsky’s claim #2 above, that doesn’t obviously support his claim #3, nor undercut the modesty that he disputes. After all, reasonable people could just agree that, by acting directly and avoiding broken institutions, individuals can often beat the best institutionally-embedded experts. For example, individuals can gain by investing more in index funds, and by choosing less medicine, school, and religion than experts advise. So the existence of broken institutions can’t by itself explain why disagreement exists, nor why readers of Yudkowsky’s book should reasonably expect to consistently pick who is right among disagreeing experts.

Thus Yudkowsky needs more to argue against modesty, and for his claim #3. Even if it is crazy to disagree with very adequate financial institutions, and not quite so crazy to disagree with less adequate institutions, that doesn’t imply that it is actually reasonable to disagree with anyone about anything.

His book says less on this topic, but it does say some. First, Yudkowsky accepts my <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0ahUKEwi7g4uQttvXAhWNQ98KHRZqByoQFgguMAA&amp;url=http%3A%2F%2Fhanson.gmu.edu%2Fdeceive.pdf&amp;usg=AOvVaw2i2crdP75icdpHrjXUU3zc">summary</a> of the rationality of disagreement, which says that agents who are mutually aware of being meta-rational (i.e., trying to be accurate and getting how disagreement works) should not be able to [foresee](we_cant_foresee) their disagreements. Even when they have very different concepts, info, analysis, and reasoning errors.
If you and a trusted peer don’t converge on identical beliefs once you have a full understanding of one another’s positions, at least one of you must be making some kind of mistake.

Yudkowsky says he has applied this result, in the sense that he’s learned to avoid disagreeing with two particular associates that he greatly respects. But he isn’t much inclined to apply this toward the other seven billion humans on Earth; his opinion of their meta-rationality seems low. After all, if they were as meta-rational as he and his two great associates, then “the world would look extremely different from how it actually does.” (It would disagree a lot less, for example.)

Furthermore, Yudkowsky thinks that he can infer his own high meta-rationality from his details:

I learned about processes for producing good judgments, like Bayes’s Rule, and this let me observe when other people violated Bayes’s Rule, and try to keep to it myself. Or I read about sunk cost effects, and developed techniques for avoiding sunk costs so I can abandon bad beliefs faster. After having made observations about people’s real-world performance and invested a lot of time and effort into getting better, I expect some degree of outperformance relative to people who haven’t made similar investments. … [Clues to individual meta-rationality include] using Bayesian epistemology or debiasing techniques or experimental protocol or mathematical reasoning.

The possibility that some agents have low meta-rationality is illustrated by these examples:

Those who dream do not know they dream, but when you are awake, you know you are awake. … If a rock wouldn’t be able to use Bayesian inference to learn that it is a rock, still I can use Bayesian inference to learn that I’m not.

Now yes, the meta-rationality of some might be low, that of others might be high, and the high might see real clues allowing them to correctly infer their different condition, clues that the low also have available to them but for some reason neglect to apply, even though the fact of disagreement should call the issue to their attention. And yes, those clues might in principle include knowing about Bayes’ rule, sunk costs, debiasing, experiments, or math. (They might also include many other clues that Yudkowsky lacks, such as relevant experience.)

Alas, Yudkowsky doesn’t offer empirical evidence that these possible clues of meta-rationality are in fact actually clues in practice, that some correctly apply these clues much more reliably than others, nor that the magnitude of these effects are large enough to justify the size of disagreements that Yudkowsky suggests as reasonable. Remember, to justifiably disagree on which experts are right in some dispute, you’ll have to be more meta-rational than are those disputing experts, not just than the general population. So to me, these all remain open questions on disagreement.

In an <a href="http://yudkowsky.net/rational/herolicensing">accompanying essay</a>, Yudkowsky notes that while he might seem to be overconfident, in many lab tests of cognitive bias,

around 10% of undergraduates fail to exhibit this or that bias … So the question is whether I can, with some practice, make myself as non-overconfident as the top 10% of college undergrads. This… does not strike me as a particularly harrowing challenge. It does require effort.

Though perhaps Yudkowsky isn’t claiming as much as he seems. He admits that allowing yourself to disagree because you think you see clues of your own superior meta-rationality goes badly for many, perhaps most, people:

For many people, yes, an attempt to identify contrarian experts ends with them trusting faith healers over traditional medicine. But it’s still in the range of things that amateurs can do with a reasonable effort, if they’ve picked up on unusually good epistemology from one source or another.

Even so, Yudkowsky endorses anti-modesty for his book readers, who he sees as better than average, and also too underconfident on average (even though most people are overconfident). His advice is especially targeted at those who aspire to his claim #1:

If you’re trying to do something unusually well (a common enough goal for ambitious scientists, entrepreneurs, and effective altruists), then this will often mean that you need to seek out the most neglected problems. You’ll have to make use of information that isn’t widely known or accepted, and pass into relatively uncharted waters. And modesty is especially detrimental for that kind of work, because it discourages acting on private information, making less-than-certain bets, and breaking new ground.

This seems to me to be a good reason to take a big anti-modest stance. If you are serious about trying hard to make a big advance somewhere, then you must get into the habit of questioning the usual accounts, and thinking through arguments for yourself in detail. If your chance of making a big advance is much higher if you are in fact more meta-rational than average, then you have a better chance of achieving a big advance if you assume your own high meta-rationality within your advance-attempt-thinking. Perhaps you could do even better if you limited this habit to the topic areas near where you have a chance of making a big advance. But maybe that sort of mental separation is just too hard.

So far this discussion of disagreement and meta-rationality has drawn nothing from the previous discussion of inefficient institutions in a broken world. And without such a connection, this book is really two separate books, tied perhaps by a mood affiliation.

Yudkowsky doesn’t directly make a connection, but I can make some guesses. One possible connection applies if official experts tend to deny that they sit in inadequate equilibria, or that their claims and advice are compromised by such inadequacy. When these experts are high status, others might avoid contradicting their claims. In this situation, those who are more willing to make cynical claims about a broken world, or more willing to disagree with high status people, can be on average more correct, relative to those who insist on taking more idealistic stances toward the world and the high in status.

In particular, such cynical contrarians can be correct about when individuals can do better via acting directly than indirectly via institution-embedded experts, and they can be correct when siding with low against high status experts. This doesn’t seem sufficient to me to justify Yudkowsky’s more general anti-modesty, which for example seems to support often picking high status experts against low status ones. But it can at least go part of the way.

We have a few other clues to Yudkowsky’s position. First, while he explains the impulse toward modesty via status effects, he claims to personally care little about status:

Many people seem to be the equivalent of asexual with respect to the emotion of status regulation—myself among them. If you’re blind to status regulation (or even status itself) then you might still see that people with status get respect, and hunger for that respect.

Second, note that if the reason you can beat on our best experts is that you can act directly, while they must win via social institutions, then this shouldn’t help much when you must also act via social institutions. So it is telling that in two examples, Yudkowsky thinks he can do substantially better than the rest of the world, even when he must act via social institutions.

First, he claims that the MIRI research institute he helped found “can do better than academia” because “We were a small research institute that sustains itself on individual donors. … we had deliberately organized ourselves to steer clear of [bad] incentives.” Second, he finds it “conceivable” that the world’s rate of innovation might increase noticeably if another small organization that he helped to found “annual budget grew 20x, and then they spent four years iterating experimentally on techniques, and then a group of promising biotechnology grad students went through a year of CFAR training.”

Putting this all together my best guess is that Yudkowsky sees himself, his associates, and his best readers as only moderately smarter and more knowledgeable than others; what really distinguishes them is that they really care much more about the world and truth. So much so that they are willing to make cynical claims, disagree with the high status, and sacrifice their careers. This is the key element of meta-rationality they see as lacking in the others with whom they feel free to disagree. Those others are mainly trying to win the usual status games, while he and his associates are after truth.

Alas this is a familiar story from a great many sides in a great many disputes. Each says they are right because the others are less sincere and more selfish. While most such sides must be wrong in these claims, no doubt some people do care more about the world and truth than others. Furthermore, those special people may see detailed signs telling them this fact, while others lack those signs but fail to sufficiently attend to that fact.

And we again come back to the core hard question in the rationality of disagreement: how can you tell if you are neglecting key signs about your (lack of) meta-rationality? But alas, other than just claiming that such clues exist, Yudkowsky doesn’t offer much analysis to help us advance on this hard problem.

Eliezer Yudkowsky’s new book <a href="https://equilibriabook.com"><em>Inadequate Equilibria</em></a>  is really two disconnected books, one (larger) book that does an excellent job of explaining how individuals acting directly can often improve on the best advice of experts embedded in broken institutions, and another (smaller) book that largely fails to explain why one can realistically hope to consistently pick the correct side among disputing experts. I highly recommend the first book, even if one has to sometimes skim through the second book to get to it.

Of course, if you are trying hard to make a big advance somewhere, then it can make sense to just assume you are better, at least within the scope of the topic areas where you might make your big advance. But for other topic areas, and for everyone else, you should still wonder how sure you can reasonably be that you have in fact not neglected clues showing that you are less meta-rational than those with whom you feel free to disagree. This remains the big open question in the rationality of disagreement. It is a question to which I hope to return someday.

## [On Disagreement, Again](#table-of-contents)
_Posted on 2022-02-15_

The usual party chat rule says to not spend too long on any one topic, but instead to flit among topics unpredictably. Many thinkers also seem to follow a rule where if they think about a topic and then write up an opinion, they are done and don’t need to ever revisit the topic again. In contrast, I have great patience for returning again and again to the most important topics, even if they seem crazy hard. And for spending a lot time on each topic, even if I’m at a party.

A long while ago I spend years studying the rationality of [Disagreement](Disagreement.md), though I haven’t thought much about it lately. But rereading Yudkowsky’s [Inadequate Equilibria](why-be-contrarian) recently inspires me to return to the topic. And I think I have a new take to report: unusual for me, I adopt a mixed intermediate position.
This topic forces one to try to choose between two opposing but persuasive sets of arguments. On the one side there is formal theory, [to](a-model-disagre) [which](we_cant_foresee) I’ve contributed, which says that rational agents with different information and calculation strategies can’t have a common belief in, nor an ability to foresee, the sign of the difference in their opinions on any “random variable”. (That is, a parameter that can be different in each different state of the world.) For example, they can’t say “I expect your next estimate of the chance of rain here tomorrow to be higher than the estimate I just now told you.”
Yes, this requires that they’d have the same ignorant expectations given a common belief that they both knew nothing. (That is, the same “priors”.) And they must be listening to and taking seriously what the other says. But these [seem](rationality-requires-common-priors) reasonable assumptions.
An informal version of the argument asks you to imagine that you and someone similarly smart, thoughtful, and qualified each become aware that your independent thoughts and analyses on some question had come to substantially different conclusions. Yes, you might know things that they do not, but they may also know things that you do not. So as you discuss the topic and respond to each others’ arguments, you should expect to on average come to more similar opinions near some more intermediate conclusion. Neither has a good reason to prefer your initial analysis over the others’.

Yes, maybe you will discover that you just have a lot more relevant info and analysis. But if they see that, they should then defer more to you, as you would if you learned that they are more expert than you. And if you realized that you were more at risk of being proud and stubborn, that should tell you to reconsider your position and become more open to their arguments.

According to this theory, if you actually end up with common knowledge of or an ability to foresee differences of opinion, then at least one of you must be failing to satisfy the theory assumptions. At least one of you is not listening enough to, and taking seriously enough, the opinions of the other. Someone is being stubbornly irrational.

Okay, perhaps you are <em>both</em> afflicted by pride, stubbornness, partisanship, and biases of various sorts. What then?

You may find it much easier to identify more biases in them than you can find in yourself. You might even be able to verify that you suffer less from each of the biases that you suspect in them. And that you are also better able to pass specific intelligence, rationality, and knowledge tests of which you are fond. Even so, isn’t that roughly what you should expect even if the two of you were similarly biased, but just in different ways? On what basis can you reasonably conclude that you are less biased, even if stubborn, and so should stick more to your guns?

A key test is: do you in fact reliably defer to most others who can pass more of your tests, and who seem even smarter and more knowledgeable than you? If not, maybe you should admit that you typically suffer from accuracy-compromising stubbornness and pride, and so for accuracy purposes should listen a lot more to others. Even if you are listening about the right amount for <a href="https://mason.gmu.edu/~rhanson/belieflikeclothes.html">other purposes</a>.

Note that in a world where many others have widely differing opinions, it is simply not possible to agree with them all. The best that could be expected from a rational agent is to not consistently disagree with some average across them all, some average with appropriate weights for knowledge, intelligence, stubbornness, rationality, etc. But even our best people seem to consistently violate this standard.

All that we’ve discussed so far has been regarding just one of the two opposing but persuasive sets of arguments I mentioned. The other argument set centers around some examples where disagreement seems pretty reasonable. For example, fifteen years ago I [said](disagree_with_s) to “disagree with suicide rock”. A rock painted with words to pretend it was a sentient creature listening carefully to your words, but offering no evidence that it actually listened, should be treated like a simple painted rock. In that case, you have strong evidence to down-weight its claims.
A second example involves sleep. While we are sleeping we don’t usually have an opinion on if we are sleeping, as that issue doesn’t occur to us. But if the subject does come up, we often mistakenly assume that we are awake. Yet a person who is actually awake can have high confidence in that fact; they can know that while a dreaming mind is seriously broken, their mind is not so broken.

An application to disagreement comes when my wife awakes in the night, hears me snoring, and tells me that I’m snoring and should turn my head. Responding half asleep, I often deny that I’m snoring, as I then don’t remember hearing myself snore recently, and I assume that I’d hear such a thing. In this case, if my wife is in fact awake, she can comfortably disagree with me. She can be pretty sure that she did hear me snore and that I’m just less reliable due to being only half awake.

Yudkowsky uses a third example, which I also find persuasive, but at which many of you will balk. That is the majority of people who say they have direct personal evidence for God or other supernatural powers. Evidence that’s mainly in their feelings and minds, or in subtle patterns in how their personal life outcomes are correlated with their prayers and sins. Even though most people claim to believe in God, and point to this sort of evidence, Yudkowsky and I think that we can pretty confidently say that this evidence just isn’t strong enough to support that conclusion. Just as we can similarly say that personal anecdotes are [usually](medical-doubts-oped) insufficient to support the usual confidence in the health value of modern medicine.
Sure, its hard to say with much confidence that there isn’t a huge smart power somewhere out there in the universe. And yes, if this power did more obvious stuff here on Earth back in the day, that might have left a trail of testimony and other evidence, to which advocates might point. But there’s just no way that either of those considerations can remotely support the usual level of widespread confidence in a God meddling in detail with their heads and lives.

The most straightforward explanation I can see here is <a href="https://www.econlib.org/archives/2014/04/social_desirabi_1.html">social desirability bias</a> a bias that not only introduces predictable errors but also one’s willingness to notice and correlate such errors. By attributing their belief to “faith”, many of them do seem to acknowledge quite directly that their argument won’t stand up to the usual evaluation standards. They are instead believing because they want to believe. Because their social world rewards them for the “courage” and “affirmation” of such a belief.

And that pretty closely fits a social desirability bias. Their minds have turned off their rationality on this topic, and are not willing to consider the evidence I’d present, or the fact that the smartest most accomplished intellectuals today tend to be atheists. Much like the sleeper who just can’t or won’t see that their mind is broken and unable to notice that they are asleep.

In fact, it seems to me that this scenario matches a great many of the disagreements I’m willing to have with others. As I tend to be willing to consider hypotheses that others find distasteful or low status. Many people tell me that the pictures I paint in my two books are ugly, disrespectful, and demotivating, but far fewer offer any opposing concrete evidence. Even though most people seem able to notice the fact that social desirability would tend to make them less willing to consider such hypotheses, they just don’t want to go there.

Yes, there is an opposite problem: many people are especially attracted to socially <em>un</em>desirable hypotheses. A minority of folks see themselves as courageous “[freethinkers](against_free_th)” who by rights should be celebrated for their willingness to “think outside the box” and embrace a large fraction of the contrarian hypotheses that come their way. Alas, by being insufficiently picky about the contrarian stories they embrace, they encourage, not discourage, everyone else to embrace social desirability biases. On average, social desirability only causes modest biases in the social consensus, and thus only justifies modest disagreements from those who are especially rational. Going all in on a great many contrarian takes at once is a sign of an opposite problem.
Yes, the stance I’m taking implies that contrarian views, i.e., views that seem socially undesirable to embrace, are on average neglected, and thus more likely than the consensus is willing to acknowledge. But that is of course far from endorsing most of them with high confidence. For example, UFOs as aliens are [indeed](on-ufo-as-aliens-priors) more likely than the usual prestigious consensus will admit, but could still be pretty unlikely. And assigning a somewhat higher chance to claims like that the moon landings were faked it is not at all the same as endorsing such claims.
So here’s my new take on the rationality of disagreement. When you have a similar level of expertise to others, you can justify disagreeing with an apparent social consensus only if you can identity a particularly strong way that the minds of most of those who think about the topic tend to get broken by the topic. Such as due to being asleep or suffering from a strong social desirability bias. (A few [weak clues](huemer-on-disagreement) won’t do.)
I see this position as mildly supported by <a href="https://twitter.com/robinhanson/status/1493595182956171279">polls</a> showing that people think that those in certain emotional states are less likely to be accurate in the context of a disagreement; different emotions plausibly trigger different degrees of willingness to be fair or rational. ([Here](disagree-resolver-variety) are some other poll results on what people think predicts who is right in a disagreement.)
But beware of going too wild embracing most socially undesirable views. And you can’t just in general presume that others disagree with each of your many positions due to their minds being broken in some way that you can’t yet see. That way lies unjustified arrogance. You instead want specific concrete evidence of strongly broken minds.

Imagine that you specialize in a topic so much that you know nearly as much as the person in the world who knows the most, but do not have the sort of credentials or ways to prove your views that the world would easily accept. And this is not the sort of topic where insight can be quickly and easily translated into big wins, wins in either money or status. So if others had come to your conclusions before, they would not have gained much personally, nor found easy ways to persuade many others.

In this sort of case, I think you should feel more free to disagree. Though you should respect base rates, and try to test your views as fast and strongly as possible. As the world is just not listening to you, you can’t expect them yet to credit what you know. Just also don’t expect the world to reward you or pay you much attention, even if you are right.

## [Rationality Requires Common Priors](#table-of-contents)
_Posted on 2018-10-18_

Late in November 2006 I started this blog, and a month later on Christmas eve I [reported](why_common_prio) briefly on the official <a href="https://link.springer.com/article/10.1007/s11238-006-9004-4">publication</a> (after 8 rejections) of my paper <a href="http://hanson.gmu.edu/prior.pdf"><em>Uncommon Priors Require Origin Disputes</em></a>. That was twelve years ago, and now Google Scholar tells me that this paper has <a href="https://scholar.google.com/scholar?cites=16708392721542063153&amp;as_sdt=5,47&amp;sciodt=0,47&amp;hl=en">17 cites</a>, which is about 0.4% of my 3933 <a href="https://scholar.google.com/citations?user=PjIP_WcAAAAJ&amp;hl=en">total cites</a>, which I’d say greatly under-estimates its value.

Recently I had the good fortune to be invited to speak at the <a href="http://www.harrycrane.com/seminar.html">Rutgers Seminar on Foundations of Probability</a>, and I took that opportunity to raise awareness about my old paper. Only about ten folks attended (a famous philosopher spoke nearby at the same time), but <a href="https://www.youtube.com/watch?v=_57fbG_gZPo&amp;feature=youtu.be">this video</a> was taken:

In the video my slides are at times dim, but they can be seen sharp <a href="https://www.slideshare.net/RobinHanson3/uncommon-priors-require-origin-disputes">here</a>. Let me now try to explain why my topic is important, and what is my result.<span id="more-31893"></span>

In economics, the most common formal model of a rational agent, by far, is that of a Bayesian. This standard model is also very common in business, political science, statistics, computer science, and many other fields. As there is actually a family of related models, we can use this space to argue about what it means to be “rational”. People argue over various particular proposed “rationality constraints” which limit this space of possibilities to varying degrees.

In economics, the standard model starts with a large (finite) <em>state space</em>, wherein each state resolves all relevant uncertainty; every interesting question is completely answered once you know which state is the true state. Each agent in this model has a <em>prior</em> function which assigns a probability to each state in this space. For any given time and situation an agent’s info can be expressed as as set; at any state, each agent has an <em>info set</em> of states where they know that the true state is somewhere within that set, but don’t know where within that set. Any small piece of info is also expressible as a set; to combine info, you intersect sets.

Given a state space, prior, and info, an agent’s expectation or belief is given by a weighted average, using their prior and conditioned on their info set. That is, all variations in agent beliefs across time or situation are to be explained by variations in their info. We usually assume that info is cumulative, so that each agent knows everything that they have ever known in the past. In order to predict actions, in addition to beliefs, the most common approach is to assume agents maximize expected <em>utility</em>, where each agent has another function that assigns a numerical utility value to each possible state.

Some people study ways to relax these assumptions, such as by using a set of priors instead of a single prior, by seeking computationally feasible approximations, or by allowing agents to forget info they once knew. Other people focus on adding stronger assumptions. For example, when a situation has a natural <em>likelihood function</em> giving the chances of particular outcomes assuming particular parameter settings, we usually assume that each agent’s prior agrees with this likelihood. Some people offer arguments for why particular priors are natural for particular situations. And models also usually assume that differing agents have the same prior.

One key rationality question is when it is reasonable to disagree with other people. Most intellectuals see disagreement as rational, and are surprised to learn that theory often says otherwise. This issue turns crucially on the common prior assumption. Given uncommon priors, it is easy to disagree, but given common priors it is hard to escape the conclusion that it is irrational to knowingly disagree, in the following sense of “[foresee to disagree](we_cant_foresee).” Assume you are now estimating some number X, and also now estimating some other person’s future estimate of X, an estimate that they will make at some future time. There is a difference now between these two numbers, and you will now clearly tell that other person the sign of this difference. They will then take this sign into account when making their future estimate.
In this situation, for standard Bayesians, this sign must equal zero; you can’t both warn them that you expect their estimate will be too high relative to your estimate, and then also still expect them to remain too high. They will instead listen to your warning and correct enough based on it. This sort of result holds nearly exactly for many slight weakenings of the standard rationality assumptions, but not if we assume big prior differences. And we have seen clearly in the lab, and in real life, humans can in fact often “foresee to disagree” in this sense.

Humans do foresee to disagree, while Bayesians with common priors do not. So are humans rational or irrational here? To answer that question, we must study the arguments for and against common priors. Not just arguments that particular aspects of priors should be common, or that they should be the common in certain simple situations. No, here we need arguments that entire prior functions should or should not be the same. And you can look long and hard without finding much on this topic.

Some people simply declare that differing beliefs should only result from differing information, but others are not persuaded by this. Some people note that as expected utility is a sum over products of probability and utility, one can arbitrarily rescale each probability and utility together holding constant that product, and get all the same decisions. So one can assume common priors without loss of generality, as long as one is free enough to change utility functions. But of course this also makes uncommon priors also without loss of generality. And we are often clear that we mean different things by probabilities and utilities, and thus are not free to vary them arbitrarily. If it means something different to say that an event is unlikely than it means to say that that event’s outcome differences are less important to you, then probabilities mean something different from utilities.

And so finally we get to my paper, <em>Uncommon Priors Require Origin Disputes</em>, which offers one of the few papers I have ever seen to give a concrete argument on common priors. Most everyone who hears it seems persuaded, yet it is rarely mentioned when people summarize what we know about rationality in Bayesian frameworks. If you read the rest of this post, at least you will know.

My argument is pretty simple, though I needed a clever construction to let me say it formally. If the beliefs of a person are described in part by a prior, then that prior must have come from somewhere. My key idea is to use beliefs about the origins of priors to constrain rational priors. For example, if you knew that a few minutes ago someone stuck a probe into your brain and randomly changed your prior, you would probably want to reverse that change. So not all causal origins of priors seem equally rational.

However, there’s one big obstacle to reasoning about prior origins. The natural way to talk about origins is to make and use some sort of probability distribution over different possible priors, origin features, and other events. But in every standard Bayesian model, the priors of all agents are common knowledge. That is, priors are all the same in all possible states, so no one can have any degree of uncertainty about them, or about what anyone else knows about them. Everyone is always completely sure about who has what priors.

To evade this obstacle, I chose to embed a standard model within a larger standard model. So there is a model and a <em>pre-model</em>. While the ordinary model has ordinary states and priors, the pre-model has <em>pre-states</em> and <em>pre-priors</em>. It is in the pre-model that we can reason about the causal origins of the priors of the model.

The pre-states of the pre-model are simply pairs of an ordinary state and an ordinary prior assignment, that says which agents get which priors. So a pre-prior is a probability distribution over the set of all combinations of possible states in the ordinary model, and possible prior assignments for that ordinary model. Each agent would initially know nothing about anything, including about ordinary states or who will get which prior. Their pre-prior would summarize their beliefs in this state of ignorance. Then at some point all agents would have learned about which prior they and the other agents will be using. From this point forward, agent info sets are entirely within an ordinary model, where their prior is common knowledge and gives them ordinary beliefs about ordinary states. So from this point on, an ordinary model is sufficient to describe everyone’s beliefs.

The key <em>pre-rationality</em> constraint that I propose is to have pre-priors agree with priors when they can condition on the same info. So if we condition an agent’s pre-prior on the assignment of who gets which priors, and then ask for the probability of some ordinary event, we should get the same answer as when we simply ask their prior for the probability of that ordinary event. And merely inspecting the form of this simple key equation is enough to draw my key conclusion: Within any single pre-prior that satisfies the pre-rationality condition, all ordinary events are conditionally independent of other agent’s priors, given that agent’s prior.

So, within a pre-prior, an agent believes that ordinary events and their own prior are informative about each other; priors are different when events are different, and in the sensible way. But also within this pre-prior, each agent believes that the priors of other agents are <em>not</em> otherwise informative about ordinary events. The priors of other agents can only predict ordinary events by predicting the prior of this agent; absent that connection, ordinary events and other priors do not predict each other.

I summarize this as believing that “my prior had special origins.” My prior was created via a process that caused it to correlate with other events in the world, but the priors of other agents were not created in this way. And of course this belief that you were made special is hard to square with many common beliefs about the causal origins of priors. This belief is not consistent with your prior being encoded in your genes via the usual processes of genetic inheritance and variation. It is similarly not consistent with many common theories of cultural inheritance and variation.

The obvious and easy way to not believe that your prior resulted from a special unusual origin process is to have common priors. And so this pre-rationality constraint can be seen as usually favoring common priors. I thus have a concrete argument that Bayesians should have common priors, an argument based on the reasonable rationality consideration that not all causal origins of priors are equally rational. If priors should be consistent with plausible beliefs about their causal origins, then priors must typically be common.

## [Might Disagreement Fade Like Violence?](#table-of-contents)
_Posted on 2019-12-19_

Violence was quite common during much of the ancient farming era. While farmers retained even-more-ancient norms against being the first to start a fight, it was often not easy for observers to tell who started a fight. And it was even harder to get those who did know to honestly report that to neutral outsiders. Fighters were typically celebrated for showing strength and bravery, And also loyalty when they claimed to fight “them” in service of defending “us”. Fighting was said to be good for societies, such as to help prepare for war. The net effect was that the norm against starting fights was not very effective at discouraging fights during the farming era, especially when many “us” and “them” were in close proximity.

Today, norms against starting fights are enforced far more strongly. Fights are much rarer, and when they do happen we try much harder to figure out who started them, and to more reliably punish starters. We have created much larger groups of “us” (e.g., nations), and use law to increase the resources we devote to enforcing norms against fighting, and the neutrality of many who spend those resources. Furthermore, we have and enforce stronger norms against retaliating overly strongly to apparent provocations that may have been accidental. We are less impressed by fighters, and prefer for people to use other ways to show off their strength and bravery. We see fighting as socially destructive, to be discouraged. And as fighting is rare, we infer undesired features about the few rare exceptions, such impulsiveness and a lack of empathy.

Now consider disagreement. I [have](we_cant_foresee) [done](why_not_impossi) [a](why_common_prio) [lot](rationality-requires-common-priors) [of](a-model-disagre) [research](disagree_with_s) [on](we-agree-on-so-much) this topic and am pretty confident of the following claim (which I won’t defend here): People who are mainly trying to present accurate beliefs that are informative to observers, without giving much weight to other considerations (aside from minimizing thinking effort), do not [foresee](we_cant_foresee) disagreements. That is, while A and B may often present differing opinions, A cannot publicly predict how a future opinion that B will present on X will differ on average from A’s current opinion on X. (Formally, A’s expectation of B’s future expectation nearly equals A’s current expectation.)
Of course today such foreseeing to disagree is quite commonplace. Which implies that in any such disagreement, one or both parties is <em>not</em> mainly trying to present accurate estimates. Which is a violation of our usual conversational norms for honesty. But it often isn’t easy to tell which party is not being fully honest. Especially as observers aren’t trying very hard very to tell, nor to report what they see honestly when they feel inclined to support “our” side in a disagreement with “them”. Furthermore, we are often quite impressed by disagreers who are smart, knowledgeable, passionate, and unyielding. And many say that disagreements are good for innovation, or for defending our ideologies against their rivals. All of which helps explain why disagreement is so common today.

But the analogy with the history of violent physical fights suggests that other equilibria may be possible. Imagine that disagreement were much less common, and that we could spend far more resources to investigate each one, using relatively neutral people. Imagine a norm of finding disagreement surprising and expecting the participants to act surprised and dig into it. Imagine that we saw ourselves much less as closely mixed groups of “us” and “them” regarding these topics, and that we preferred other ways for people to show off loyalty, smarts, knowledge, passion, and determination.

Imagine that we saw disagreement as socially destructive, to be discouraged. And imagine that the few people who still disagreed thereby revealed undesirable features such as impulsiveness and ignorance. If it is possible to imagine all these things, then it is possible to imagine a world which has far less foreseeable disagreement than our world, comparable to how we now have much less violence than did the ancient farming world.

When confronted with such an imaged future scenario, many people today claim to see it as stifling and repressive. They very much enjoy their freedom today to freely disagree with anyone at any time. But many ancients probably also greatly enjoyed the freedom to hit anyone they liked at anytime. Back then, it was probably the stronger better fighters, with the most fighting allies, who enjoyed this freedom most. Just like today it is probably the people who are best at arguing to make their opponents look stupid who enjoy our freedom to disagree today. Doesn’t mean this alternate world wouldn’t be better.

## [Reject Random Beliefs](#table-of-contents)
_Posted on 2008-03-05_

xml version="1.0" standalone="yes"? html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd" 

A <a href="http://www.newscientist.com/channel/being-human/mg19726411.800-are-political-leanings-all-in-the-genes.html">recent</a> <em>New Scientist </em>mentions a 2005<span style="color: #000000;"> <a href="http://proquest.umi.com/pqdweb?RQT=318&amp;pmid=28600&amp;TS=1204574898&amp;clientId=31810&amp;VInst=PROD&amp;VName=PQD&amp;VType=PQD">American Political Science Review</a><em> </em>p</span>aper on the genetic basis of political beliefs, which includes this key table, breaking variation in opinions (among 30,000 Virginia twins) on 28 specific topics into three origin components: genetic (heritability), family (shared environment), and other (unshared environment):

<a href="http://robinhanson.typepad.com/photos/uncategorized/2008/03/03/geneticpolitics3.gif"></a>

<a href="/wp-content/uploads/2008/03/geneticpolitics3_3.gif"><img alt="Geneticpolitics3_3" border="0" height="481" loading="lazy" src="/wp-content/uploads/2008/03/geneticpolitics3_3.gif" title="Geneticpolitics3_3" width="602"/></a> 

 <span id="more-17456"></span> 

The paper shows similar results for Australian twins.     

This is a concrete occasion to revisit a general issue.  In general, if you want to believe the truth, then you should <strong>just accept the average belief on any topic</strong> unless you have a good (and better than average) reason to think the causes of your belief difference would be substantially more informed than average.

So unless you have a good reason to think your genes tend to produce more informed beliefs than other genes, you should reject the genetically-caused parts of how your beliefs differ from average beliefs.  Even if you have a higher genetically-given IQ, and even if high IQ folks have more accurate beliefs, you should still reject genetic ways in which your beliefs differ from the average beliefs of high IQ folks.  After all, true beliefs are supposed to be about the world, not about the particular genes you were randomly given.  

 Unless you have a good reason to think your childhood family environment was more informed than an average family environment, ignoring any genetic advantages in your family, then you should reject the ways in which your beliefs differ from average beliefs due to your family background.  Similarly, you should also reject other non-genetic non-family causes of your belief differences, if you do not have a better than average reason to think your causes are better than average.

 Having an intuitive feeling that your belief causes are better is not a “good reason” if most everyone has a similar intuitive feeling.  The fact that you have specific reasons for your specific beliefs is also not good enough – most everyone has specific reasons. 

A good reason must be based on some feature of you that is different from others, where you have a good reason to think this feature is correlated with being right.  The mere fact that you have a distinguishing feature, and would like to think well of yourself, is <em>not </em>by itself a reason to think that feature is correlated with being right! And you must be wary of the common bias to lower our evidential standards when concluding that people like us tend to be more right.

<strong>Added: </strong> Let’s say that on a scale of 0 to 100, your position on property taxes is 90 – you think such taxes are good for some standard widely accepted mix of values such as economic growth, inequality aversion, or good neighbors.   When you disagree with someone at the other extreme, with a position of 10, you understand it to be a disagreement about facts, not values.

 Let’s assume the average position on this issue is 50, and that you can see no good reason to think the genes that lean you toward property taxes are better than average.  Since the table says that 41% of belief variation on this topic is genetic,  then to eliminate this genetic component of your beliefs, you might reduce your position from 90 to 81, since this takes away 40% of the variance of your belief relative to the average.

## [Chase Your Reading](#table-of-contents)
_Posted on 2010-05-28_

Hunting has two main modes: <em>searching</em> and <em>chasing</em>.  With searching you look for something to chase. With chasing, in contrast, you have a focus of attention that drives your actions. You may find something else worth chasing along the way, and then switch your focus to a new chase, but you’ll still maintain a focus.

It seems to me that while reading non-fiction, most folks are in searching mode. Most would be more intellectually productive, however, in chasing mode. It helps to have in mind a question, puzzle, or problem, and then read in order to answer your question, explain your puzzle, or solve your problem.

In searching mode, readers tend to be less critical. If a source came recommended, they tend to keep reading along even if they aren’t quite sure what the point is. Since authors tend to be more prestigious than readers, readers tend to feel reluctant to question or judge what they’ve read.  They are more likely to talk about whether they enjoyed the read, than whether the author’s argument works.

In chasing mode, readers are naturally more critical. When you are looking for something particular, it feels less presumptuous to stop reading when your source comes to seem irrelevant. After all, the source might be good for some other purpose, even if not for your purpose.

In chasing mode, you continually ask yourself whether what you are reading is relevant for your quest, or whether the author actually has anything new or interesting to say. You flip around seeking sections that might be more relevant, and you might even look up the references for an especially relevant section.

Also, search-readers often don’t have a good mental place to put each thing they learn. In which case they don’t end up learning much. Chasers, in contrast, always have specific mental places they are trying to fill with what they read, so they better integrate new things they learn with old things they know.

In chasing mode, readers also tend to better interleave reading and thinking. People often hope that search-mode reading will inspire them to new thoughts, and are disappointed to find that it doesn’t. Chase-mode reading, in contrast, requires constant thinking, in order to evaluate how the current source addresses your chosen focus. This tends to make it easier to notice missing holes in the literature, where your new idea can be placed.

So if you read to be intellectually productive, rather than just to fill your time, consider reading while chasing something, anything.  (From a conversation with Heather Macsorley.)

<strong>Added 8p</strong>: <a href="http://meteuphoric.wordpress.com/2010/05/29/thinking-better-chase/">Katja</a> and <a href="http://andymckenzie.blogspot.com/2010/05/in-defense-of-searching.html">Andy</a> comment, and <a href="http://www.dloye.com/myblog/wordpress/?p=770">dloye</a> offers this quote from Samuel Johnson:

What we read with inclination makes a much stronger impression. If we read without inclination, half the mind is employed in fixing the attention; so there is but one half to be employed on what we read.

## [Against Free Thinkers](#table-of-contents)
_Posted on 2007-06-14_

> <strong>Freethinker.</strong> <em>One who has rejected authority and dogma, especially in his religious thinking, in favor of rational inquiry and speculation.</em>  A<a href="http://www.bartleby.com/61/25/F0312500.html">merican Heritage Dictionary </a>  <em><br/></em>
> <em>Individual whose opinions are formed on the basis of an understanding and rejection of tradition, authority or established belief.</em>  <a href="http://en.wikipedia.org/wiki/Freethinker ">Wikipedia</a>

 Many people see themselves as "free thinkers," with minds open to new ideas and perspectives.  They describe themselves positively as favoring rationality, but in practice their negative self-definition seems to have more force.  Even when they turn out to have been wrong, freethinkers are proud of having resisted social pressure toward conventional wisdom.  

 Freethinkers see the deck stacked against new or contrary ideas, and see their own brave contrarian stance as a needed antidote to unreasonable conformity pressures.  On net, however, freethinkers deserve much of the <em>blame </em>for resistance to new ideas.  Bryan Caplan <a href="http://econlog.econlib.org/archives/2005/03/why_be_normal.html">explains</a>: 

> Suppose you’re interviewing a smart guy [for a job], without a college degree, and he offers you a money-back guarantee. You might think "What a great deal" and accept.  But then again, you might start thinking "What a weirdo. What’s wrong with him?"  And this, I propose, is the stumbling block to lots of worthwhile innovations. A person with an unconventional idea may have a point, but is also unlikely to be "normal." He may not fit it with other people. He may have problems with authority. He may be deviant in more ways than one!  

The problem is that on average people who support odd ideas are less desirable as associates, and less discriminating in which ideas they endorse.  If people only endorsed odd ideas when they had new information suggesting such ideas were promising, we should be eager to hear of such news, and eager to associate with such people.  But in fact the main task faced by those with good news on odd ideas is to distinguish themselves from freethinkers who just pretend to have such news.  Contrary to their self-image, undiscriminating freethinkers are our main obstacle to innovation.

## [Eventual Futures](#table-of-contents)
_Posted on 2012-05-15_

I’ve noticed that recommendations for action based on a vision of the future are based on an idea that something must “eventually” occur.  For example, <strong>eventually</strong>:

<ul>
<li>We will run out of coal, so we’d better find replacements soon.</li>
<li>Earth will run out of stored energy of fossil fuels and radioactivity, so we’d better get ready to run only on sunlight.</li>
<li>Earth will run out of place for trash, so we must stop making trash.</li>
<li>The sun will die out, so we’d better get ready to move to another sun.</li>
<li>There will be a race to colonize other planets and stars, so our group should get out there first so we don’t get lose this race.</li>
<li>Chips will use X instead of silicon, so our chip firms must use X now, to not be left behind.</li>
<li>There will be no privacy of any sort, so we might as well get used to it now.</li>
<li>Some races will win, so we’d best fight for ours before its too late.</li>
<li>Firms will be stronger than nations, unless we break their power soon.</li>
<li>There will be a stronger world government, so let’s start one now.</li>
<li>There will be conflict between China and West, or Islam and West, so we best strike first now.</li>
<li>Artificial intelligences will rule the world, so let’s figure out now how to make a good one.</li>
<li>We’ll invent all that is worth inventing, so let’s find a way now to live without innovation.</li>
<li>We’ll know all the physics there is, so lets find something else interesting now.</li>
<li>There will be a huge deadly world war, so let’s stock some bunkers to hide in.</li>
<li>Nanobots will give everyone anything they want, so why work now?</li>
<li>The first nano-assembler’s owner will rule the world, so we best study nanotech now.</li>
<li>More fertile immigrants will out number us, so we best not let them in.</li>
<li>The more fertile stupid will make the world dumb, unless we stop them now.</li>
</ul>
The common pattern: project forward a current trend to an extreme, while assuming other things don’t change much, and then recommend an action which might make sense if this extreme change were to happen all at once soon.

This is usually a mistake. The trend may not continue indefinitely. Or, by the time a projected extreme is reached, other changes may have changed the appropriate response. Or, the best response may be to do nothing for a long time, until closer to big consequences. Or, the best response may be to do nothing, ever – not all negative changes can be profitably resisted.

It is just not enough to suspect that an extreme will be reached eventually – you usually need a good reason to think it will happen soon, and and that you know a robust way to address it. In far mode it often feels like the far future is clearly visible, and that few obstacles stand in the way of planning paths to achieve far ends. But in fact, the world is much messier than far mode is willing to admit.

## [Seen vs. Unseen Biases](#table-of-contents)
_Posted on 2006-12-04_

I suspect the following issue will be a thorn in our sides for some time to come:  when can we justify seen biases as correcting for unseen biases?   "Seen" biases are relatively easy to see and document, whereas "unseen" biases are said to exist but are harder to clearly see.

The issue showed up in "[Hide Sociobiology Like Sex?](does_sociobiolo)," where some wanted the seen bias of focusing children on altruism instead of more realistic selfishness, to correct for the unseen bias of children confusing "is" and "ought."   And it shows up here in this recent <em>Washington Post</em> <a href="http://washingtonpost.com/wp-dyn/content/article/2006/11/30/AR2006113001532.html">article on drug effectiveness</a>:
 <span id="more-18371"></span> 
> Treating schizophrenia with an older, cheaper drug, rather than with heavily promoted newer medications, reduces the cost by as much as 30 percent with no apparent difference in safety and effectiveness, according to the first study to examine the economic implications of antipsychotic drug prescribing practices in the United States. … The findings have roiled the field of psychiatry in a fierce debate over the study’s implications and have triggered concerns it could lead public and private insurers to limit drastically which drugs they will pay for. …the new finding faced stiff headwinds before it was published, and was subjected to an extraordinary level of review.  … several experts said they were very worried, however, that the choice of medications would be taken from physicians and would be decreed by insurers. That would ignore the complexities of treating schizophrenia and the need for flexibility, the experts said.  Patients who have tried perphenazine unsuccessfully, for example, may not be good candidates to go back on it. 

The seen effect here is that cheaper drugs seem just as effective, so insurers may limit coverage to only them, to counter the seen doctor bias of low sensitivity to drug prices. Doctors, on the other hand, resist these new findings, because they fear losing their freedom to choose drugs based on their judgment of detailed patient circumstances. Since are no clinical trials yet to document the claim that such doctor judgment improves patients on average, this is an unseen bias (if it exists).  

I once told an investment adviser I didn’t want his services because people like him lose money for their clients on average. He replied,  "But none of my clients are average; are you?" I guess he thought his seen bias was justified by all those unseen biases he was fixing. 

The key issue here is that if it is too easy to believe in unseen biases, we could justify all of our seen biases as countering made-up unseen biases.

## [Law as No-Bias Theatre](#table-of-contents)
_Posted on 2006-12-11_

Why is there law?   Some say for social justice, others for economic efficiency.  I suspect that "law is theater"; i.e., law is there to make disputants shut up.  When one person is mad as hell at another, law wants an outcome where neither they nor their friends yell and complain, and make the law look bad.  To achieve this, hard to understand legal processes make it hard to know what exactly to complain about, a long expensive process saps the energy needed to complain, and the option of endless appeals makes it unclear when to complain. 

A complaining loser’s best argument is often "it wasn’t fair"; there was bias.  So law-as-theater predicts law-as-no-bias-theater; law will bend over backwards to avoid any possible appearance of bias.   How far does our law go in this direction?   Consider what law would be like with unbiased jurors.

If we expected jurors (or judges) to try their best to achieve social justice or economic efficiency, without substantial bias, we would have law, but not laws.  That is, there would be lots of law, i.e., activity in a system for settling disputes, but few laws, i.e., rules about how to settle disputes.   The legal system would be simple: you bring your complaint to a jury, you and your opponent each tell your side, and then the jury makes any decision they think appropriate.  Think King Solomon.   

Juries (or judges) would thus have complete control over the legal process.   They would talk to anyone about anything they liked, hardly limited by any rules of procedure or evidence.  Their final judgment would be any outcome they chose, based on any consideration they liked, hardly limited by any laws or precedent.  In fact, of course, law is nothing like this.  

Most of the legal biases that concern people are not due to juror interests.   After all, we can pretty much eliminate strong interest biases by just preventing jurors (or judges) from extorting disputants, or from sharing any substantial interests with disputants   Thus the structure of our legal system is driven in large part by fears of non-interest-based biases in juror beliefs.   

While we have built elaborate legal structures to deal with these supposed biases, legal scholars have spent almost no effort to document that such biases are real.  Legal rules constraining jurors are thus [seen biases, justified as responding to unseen biases](seen_vs_unseen_).   If the law were about social justice or economic efficiency, you’d think the legal system would study biases a lot more.   But if law is more about no-bias-theatre, needing only to make it hard for disputants to complain of bias, what would be the point?

## [Benefit of Doubt = Bias](#table-of-contents)
_Posted on 2006-12-29_

One <a href="http://idioms.thefreedictionary.com/give+the+benefit+of+the+doubt">dictionary defines</a> "to give the benefit of the doubt" as

>  To believe something good about someone, rather than something bad, when you have the possibility of doing either.

 That is, assume the best.  This may be better than assuming the worst, but honesty requires you to instead remain uncertain, assigning chances according to your evidence.   Does this mean we should stereotype people?  After all, M Lafferty [commented](when_truth_is_a):  

> To make assumptions about an individual based on a stereotype is wrong, even if the stereotypical view is broadly accurate. 

 To the contrary, I say honesty <em>demands</em> we stereotype people, instead of giving them "the benefit of the doubt."  Bryan Caplan has<a href="http://econlog.econlib.org/archives/2005/09/practicing_what.html"> emphasized</a> to me that most stereotypes are <a href="http://econlog.econlib.org/archives/2005/02/giles_and_stere.html">on average accurate</a>: 

> <em>Obviously</em>, every stereotype has exceptions; stereotypes are useful because they are better than nothing, not because they are infallible. <span id="more-18324"></span> 

For more, see John Ray’s, "<a href="http://jonjayray.tripod.com/stereo.html">Do We Stereotype Stereotyping?</a>"   I suspect people justify the usual dictum against stereotypes as countering a human tendency to assume the worst about outsiders.  But until I see evidence of this, I’ll classify this as a [seen bias justified by an unseen one](seen_vs_unseen_). 
 Consider Perry Metzger’s recent [comment](academic_overco):

> What you are saying, essentially, is that after seeing that a number of estimates of some constant do not fall within each other’s error bars, physicists should then increase the size of the error bars. I don’t think that is reasonable.  Not all methods of measurement are identical, and different groups use different instruments, so the systematic errors made by different groups are different. That means that it is not necessarily the case that all groups are underestimating their errors — in fact, it is most likely that only some of them are underestimating error. 

Yes, a set that is biased overall may be include subsets which are less biased.  And by adjusting to correct for the overall bias we may increase the error in the less-biased subset.  Nevertheless, unless we can distinguish the subsets that are more vs. less biased, we must accept this outcome 

The general principle is:  <strong>you need a better than average reason to think something is better than average</strong>.   A physicist might say "I don’t need to adjust as much because I’m measuring voltage, where systematic bias is less a problem," or "I’m from Harvard, where we are more careful."  But he needs to actually have evidence that there is less bias with voltage or at Harvard; no fair just giving himself "the benefit of the doubt."  

Furthermore, since our minds are good at selectively attending to factors favoring us, we must realize that others’ minds will attend to other factors, such as their years of experience or their IQ.   To decide if you are less biased than average, you must consider the sorts of reasons that will occur to others, and ask if your reasons are better than those.  Furthermore, if you are better in general at coming up with reasons for things, you must count that against yourself.  

Finally, consider Eliezer Yudkowsky’s [complaint](the_modesty_arg) about modesty: 

> How can you know which of you is the honest truthseeker, and which the stubborn self-deceiver?  The creationist believes that <em>he</em> is the sane one and <em>you</em> are the fool.  Doesn’t this make the situation symmetric around the two of you?  … "But I know perfectly well who the fool is.  It’s the other guy.  It doesn’t matter that he says the same thing – he’s <em>still</em> the fool."  This reply sounds bald and unconvincing when you consider it abstractly.  But if you actually face a creationist, then it certainly <em>feels</em> like the correct answer. … Those who dream do not know they dream; but when you wake you know you are awake.   

The key question is: what concrete evidence can you cite that you are more sane than a creationist, or more awake than a dreamer?   Perhaps you know more biology than a creationist, and you are more articulate than a dreamer.  But the mere feeling that you are right does not justify giving yourself "the benefit of the doubt."

## [Decision Theory Remains Neglected](#table-of-contents)
_Posted on 2020-02-01_

Back in ’84, when I first started to work at Lockheed Missiles & Space Company, I recall a manager complaining that their US government customer would not accept using decision theory to estimate the optimal thickness of missile walls; they insisted instead on using a crude heuristic expressed in terms of standard deviations of noise. Complex decision theory methods were okay to use for more detailed choices, but not for the biggest ones.

In his [excellent](missing-measurements) 2010 book <a href="https://www.howtomeasureanything.com"><em>How to Measure Anything</em></a>, Douglas W. Hubbard reports that this pattern is common:
> Many organizations employ fairly sophisticated risk analysis methods on particular problems; … But those very same organizations do not routinely apply those same sophisticated risk analysis methods to much bigger decisions with more uncertainty and more potential for loss. …
> If an organization uses quantitative risk analysis at all, it is usually for routine operational decisions. The largest, most risky decisions get the least amount of proper risk analysis. … Almost all of the most sophisticated risk analysis is applied to less risky operational decisions while the riskiest decisions—mergers, IT portfolios, big research and development initiatives, and the like—receive virtually none.

In fact, while standard decision theory has <a href="https://en.wikipedia.org/wiki/Decision_theory">long</a> been <em>extremely</em> well understood and accepted by academics, most orgs find a wide array of excuses to avoid using it to make key decisions:

> For many decision makers, it is simply a habit to default to labeling something as intangible [=unmeasurable] … committees were categorically rejecting any investment where the benefits were “soft.” … In some cases decision makers effectively treat this alleged intangible as a “must have” … I have known managers who simply presume the superiority of their intuition over any quantitative model …
> What they seem to take away from these experiences is that to use the methods from statistics one needs a lot of data, that the precise equations don’t deal with messy real-world decisions where we don’t have all of the data, or that one needs a PhD in statistics to use any statistics at all. … I have at times heard that “more advanced” measurements like controlled experiments should be avoided because upper management won’t understand them. … they opt not to engage in a smaller study—even though the costs might be very reasonable—because such a study would have more error than a larger one. …
> Measurements can even be perceived as “dehumanizing” an issue. There is often a sense of righteous indignation when someone attempts to measure touchy topics, such as the value of an endangered species or even a human life. … has spent much time refuting objections he encounters—like the alleged “ethical” concerns of “treating a patient like a number” or that statistics aren’t “holistic” enough or the belief that their years of experience are preferable to simple statistical abstractions. … I’ve heard the same objections—sometimes word-for-word—from some managers and policy makers. …
> There is a tendency among professionals in every field to perceive their field as unique in terms of the burden of uncertainty. The conversation generally goes something like this: “Unlike other industries, in our industry every problem is unique and unpredictable,” or “Problems in my field have too many factors to allow for quantification,” and so on. …
> Resistance to valuing a human life may be part of a fear of numbers in general. Perhaps for these people, a show of righteous indignation is part of a defense mechanism. Perhaps they feel their “innumeracy” doesn’t matter as much if quantification itself is unimportant, or even offensive, especially on issues like these.

Apparently most for-profit firms could make substantially more profits if only they’d use simple decision theory to analyze key decisions. Execs’ usual excuse is that key parameters are unmeasurable, but Hubbard argues convincingly that this is just [not](what-info-is-verifiable) true. He suggests that execs seek to excuse poor math abilities, but that seems implausible as an explanation to me.
I say that their motives are more [political](firm-inefficiency): execs and their allies gain more by using other more flexible decision making frameworks for key decisions, frameworks with more wiggle room to help them justify whatever decision happens to favor them politically. Decision theory, in contrast, threatens to more strongly recommend a particular hard-to-predict decision in each case. As execs gain when the orgs under them are more efficient, they don’t mind decision theory being used down there. But they don’t want it up at their level and above, for decisions that say if they and their allies win or lose.
I think I saw the same sort of effect when trying to get firms to consider prediction markets; those were okay for small decisions, but for big ones they preferred estimates made by more flexible methods. This overall view is, I think, also strongly supported by the excellent book <a href="https://www.lesswrong.com/posts/45mNHCMaZgsvfDXbw/quotes-from-moral-mazes"><em>Moral Mazes</em></a> by Robert Jackall, which goes into great detail on the many ways that execs play political games while pretending to promote overall org efficiency.

If I ever did a book on <em>The Elephant At The Office: Hidden Motives At Work</em>, this would be a chapter.

Below the fold are <em>many</em> quotes from <em>How to Measure Anything</em>:

<span id="more-32301"></span>

the word “intangible” has also come to mean utterly immeasurable in any way at all, directly or indirectly. It is in this context that I argue that intangibles do not exist—or, at the very least, could have no bearing on practical decisions. …<span class="Apple-converted-space"> </span>

For many decision makers, it is simply a habit to default to labeling something as intangible …

committees were categorically rejecting any investment where the benefits were “soft.” …

major investments were approved with no plans for measuring their effectiveness after they were implemented. …

In some cases decision makers effectively treat this alleged intangible as a “must have” so that the question of the degree to which the intangible matters is never considered in a rational, quantitative way. …

I have known managers who simply presume the superiority of their intuition over any quantitative model …

Computing and using the economic value of measurements to guide the measurement process is, at a minimum, where a lot of business measurement methods fall short. …

What they seem to take away from these experiences is that to use the methods from statistics one needs a lot of data, that the precise equations don’t deal with messy real-world decisions where we don’t have all of the data, or that one needs a PhD in statistics to use any statistics at all. …

I have at times heard that “more advanced” measurements like controlled experiments should be avoided because upper management won’t understand them. …

they opt not to engage in a smaller study—even though the costs might be very reasonable—because such a study would have more error than a larger one. …

Usually things that seem immeasurable in business reveal themselves to much simpler methods of observation, once we learn to see through the illusion of immeasurability. …

The clarification chain is just a short series of connections that should bring us from thinking of something as an intangible to thinking of it as tangible. First, we recognize that if X is something that we care about, then X, by definition, must be detectable in some way.… if this thing is detectable, then it must be detectable in some amount. If you can observe a thing at all, you can observe more of it or less of it. …

I ask who thinks the sample is “statistically significant.” Those who remember something about that idea seem only to remember that it creates some kind of difficult threshold that makes meager amounts of data useless …

“If you don’t know what to measure, measure anyway. You’ll learn what to measure.”… the objection “A method doesn’t exist to measure this thing” is never valid. …

measurements can even be perceived as “dehumanizing” an issue. There is often a sense of righteous indignation when someone attempts to measure touchy topics, such as the value of an endangered species or even a human life, …

Meehl … has spent much time refuting objections he encounters—like the alleged “ethical” concerns of “treating a patient like a number” or that statistics aren’t “holistic” enough or the belief that their years of experience are preferable to simple statistical abstractions. … I’ve heard the same objections—sometimes word-for-word—from some managers and policy makers. …

Four Useful Measurement Assumptions: It’s been measured before. You have far more data than you think. You need far less data than you think. Useful, new observations are more accessible than you think. …

I’ve noticed that there is a tendency among professionals in every field to perceive their field as unique in terms of the burden of uncertainty. The conversation generally goes something like this: “Unlike other industries, in our industry every problem is unique and unpredictable,” or “Problems in my field have too many factors to allow for quantification,” and so on. I’ve done work in lots of different fields, and some individuals in most of these fields make these same claims. So far, each one of them has turned out to have fairly standard measurement problems not unlike those in other fields. …

When managers think about measuring productivity, performance, quality, risk, or customer satisfaction, it strikes me as surprisingly rare that the first place they start is looking for existing research on the topic. …

When I asked bank managers what decisions these reports supported, they could identify only a few cases where the elective reports had, or ever could, change a decision. Perhaps not surprisingly, the same reports that could not be tied to real management decisions were rarely even read. …

The data on the dashboard was usually not selected with specific decisions in mind based on specific conditions for action. …

So the question is never whether a decision can be modeled or even whether it can be modeled quantitatively. …

Even just pretending to bet money significantly improves a person’s ability to assess odds. In fact, actually betting money turns out to be only slightly better than pretending to bet. …

Why is it that about 5% of people are apparently unable to improve at all in calibration training? Whatever the reason, it often turns out not to be that relevant. Virtually every single person we ever relied on for actual estimates was in the first two groups and almost all were in the first ideally calibrated group. Those who seemed to resist any attempt at calibration were, even before the testing, almost never considered to be the relevant expert or decision maker for a particular problem. …

there is apparently a strong placebo effect in many decision analysis and risk analysis methods. Managers need to start to be able to tell the difference between feeling better about decisions and actually having better track records over time. …

Many organizations employ fairly sophisticated risk analysis methods on particular problems; … But those very same organizations do not routinely apply those same sophisticated risk analysis methods to much bigger decisions with more uncertainty and more potential for loss. …

If an organization uses quantitative risk analysis at all, it is usually for routine operational decisions. The largest, most risky decisions get the least amount of proper risk analysis. …

Almost all of the most sophisticated risk analysis is applied to less risky operational decisions while the riskiest decisions—mergers, IT portfolios, big research and development initiatives, and the like—receive virtually none…

When I ran the macro that computed the value of information for each of these variables, I began to see this pattern: The vast majority of variables in almost all models had an information value of zero. That is, the current level of uncertainty about most variables was acceptable, and no further measurement was justified. The variables that had high information values were routinely those that the client never measured. In fact, the high-value variables often were completely absent from previous business cases. (They excluded chance of project cancellation or the risk of low user adoption.) The variables that clients used to spend the most time measuring were usually those with a very low (even zero) information value (i.e., it was highly unlikely that additional measurements of the variable would have any effect on decisions). …

At the time of this writing, however, I’ve applied this same test to more than 60 additional projects and I found out that this effect is not limited to IT. I noticed the same phenomena arise in projects relating to research and development, military logistics, the environment, venture capital, facilities expansion, and the CGIAR sustainable farming model. …

First people measure what they know how to measure or what they believe is easy to measure.… the things you measured the most in the past have less uncertainty, and therefore less information value, when you need to estimate them for future decisions.…

Managers might tend to measure things that are more likely to produce good news. After all, why measure the benefits if you have a suspicion there might not be any?…

if you aren’t computing the value of a measurement, you are very likely measuring some things that are of little or no value and ignoring some high-value items.…

The 80 or more major risk/return analyses I’ve done in the past 20 years consisted of a total of over 7,000 individual variables, or an average of almost 90 variables per model. Of those 7,000 variables, a little over 180 (about 2 per model) required further measurement according to the

information value calculation. Most of these, about 150, had to be decomposed further to find a more easily measured component of the uncertain variable. Other variables offered more direct and obvious methods of measurement, for example, having to determine the gas mileage of a truck on a gravel road (by just driving a truck with a fuel-flow meter) or estimating the number of bugs in software (by inspecting samples of code). But almost a third of the variables that were decomposed required no further measurement after decomposition. In other words, about 25% of the high-value measurements were addressed with decomposition alone.…

the EVPI is an upper limit on what you should be willing to spend even theoretically. But the best measurement expenditure is probably far below this maximum. As a ballpark estimate, I shoot for spending approximately 10% of the EVPI on a measurement and, depending on the circumstances, sometimes even as low as 2%. I use this estimate for three reasons: The EVPI is the value of perfect information. …

respondents, those of us who measure such things as the value of life and health have to face a misplaced sense of righteous indignation. Some studies have shown that about 25% of people in environmental value surveys refused to answer on the grounds that “the environment has an absolute right to be protected” regardless of cost. …

Resistance to valuing a human life may be part of a fear of numbers in general. Perhaps for these people, a show of righteous indignation is part of a defense mechanism. Perhaps they feel their “innumeracy” doesn’t matter as much if quantification itself is unimportant, or even offensive, especially on issues like these. …

## [What Function Music?](#table-of-contents)
_Posted on 2012-06-09_

Darwin argued that music evolved mainly by sexual selection through mate choice—and that we’re uncomfortable acknowledging that fact. (<a href="http://www.theatlantic.com/entertainment/archive/2012/04/did-humans-invent-music/255945/">more</a>)

My students … don’t talk about music very eagerly. In class I can get a conversation going about God with no problem. And students love talking about alcohol and its effects on the human mind and spirit, theirs in particular. A conversation about sex is easy to start and quickly goes way further than I’d imagine — and sometimes further than I want. … [Yet] when I ask what role music plays in their lives or why they listen to what they do, there is silence. (<a href="http://chronicle.com/article/Can-Music-Save-Your-Life-/132040/">more</a>)

I can also feel in myself a reluctance to analyze music, a fear that awareness might kill something precious. Yet this also suggests there’s an important hypocrisy here, a truth we’d rather not face. Digging, I found a summary of music’s functions:

Seven main functions of music listening were identified: music in the background, memories through music, music as diversion, emotions and self-regulation through music, music as reflection of self and social bonding through music. (more detail below)

Anything that we can do several different ways can help to identify us and our groups. Anything we can do together can bond us. And anything that can be done well or badly can signal ability. Any different activity could be a diversion. And any stimulation can sit in the background while we do other things. Because these functions can apply to most anything, they seem last-resort explanations for why we developed a musical capacity. More likely, such functions were layered onto an activity that had a more unique base function.

It certainly feels helpful that music can adjust our mood and emotions. The question is why we’d be built with something so expensive as our mood adjustment knobs. If we needed conscious control of mood, why not just evolve a direct control? I’m also struck by how important lyrics are to music – none of the above functions explain why we prefer songs with meaningful words.

Compared to other sorts of speech, we especially like stories to be accompanied by music. And the lyrics of songs are similar to stories in many ways. This suggests that stories and music perform similar or complementary functions.

If the lower levels of our minds tend to treat story events like real events, then we can use our stories to influence our beliefs about what happens in the real world. By consuming stories socially, and preferring stories preferred by our leaders and created by impressive story tellers, we coordinate to believe what our associates believe, and what our high status leaders choose us to believe, even against the evidence of our eyes. And by letting others see the stories we consume, we can signal this choice to others.

Thus we can use [stories to signal](stories-are-like-religion) our allegiance to our leaders’ and groups’ norms. Of course if some people evolved an ability to prevent stories from influencing their expectations about real events, they’d be able to fake this conformity signal. Which might be why we feel revulsion for “inhuman” folks who are not moved by stories.
Similarly, imagine music can directly influence our emotions and moods, but that we have only limited direct conscious control over such things. In this case by associating music with people and verbal claims, we can influence our attitudes toward such things. And by sharing music with our groups, and preferring music preferred by our leaders and created by impressive artists, we can coordinate to have have the attitudes our associates do, and the ones our high status leaders prefer. By consuming music together, we can signal this choice to others. And we’d naturally feel revulsion against those who could fake this signal, because music didn’t influence their moods.

Homo hypocritus likes to think that his beliefs and attitudes are based only on his evidence; he doesn’t believe things just to please his associates or leaders. But he in fact needs to believe what his associates do, and what his leaders like, often against his evidence. And he needs to signal this fact to his associates and leaders.

By visibly exposing himself to shared stories and music, that directly influence his beliefs, while consciously believing that stories and music do not change his beliefs, homo hypocritus can accomplish all these things. This can also explain why we are reluctant to seriously examine the function of music (and stories) in our lives.

Those promised function details:<span id="more-29816"></span>

Seven main functions of music listening were identified: music in the background, memories through music, music as diversion, emotions and self-regulation through music, music as reflection of self and social bonding through music.  Across all sub-samples the self-regulation function was the most important personal use of music, bonding was the most important social use of music and the expression of cultural identity was the most salient cultural function of music regardless of listeners’ cultural background. …

Music is often used as a background; … it can also fill gaps and help pass the time. … Music can bring back memories of events, life stages, relationships and emotions or memories of loved ones. … Music is … used for feeling good and enjoying oneself. … Music has the capacity to convey emotions and to trigger emotions or emotional and physical reactions. Particular songs are … specifically chosen … in order to express a particular emotional state of the participants. …  Music can help to relax and relieve stress and to enhance creativity and intellectual focus. Listening to music can reduce loneliness, while offering a means of escape. … Certain music can assist in venting frustration and aggression. … It allows for the expression of a person’s individuality and lifestyle. … music expresses and influences values and attitudes; it can act as inspiration. … music indicates social identity by signifying group membership, for instance, belong- ing to a particular social group (like alternative or rave) or the current ‘cool group’ in school. … Music can provide an opportunity for a collective activity, such as discussing and listening to music or going to concerts together. These shared musical activities can … create a special bond. (<a href="http://pom.sagepub.com/content/40/2/179.abstract">more</a>)

## [Politics isn’t about Policy](#table-of-contents)
_Posted on 2008-09-21_

> <em>Food isn’t about Nutrition<br/>
Clothes aren’t about Comfort<br/>
Bedrooms aren’t about Sleep<br/>
Marriage isn’t about Romance<br/>
Talk isn’t about Info<br/>
Laughter isn’t about Jokes<br/>
Charity isn’t about Helping<br/>
Church isn’t about God<br/>
Art isn’t about Insight<br/>
Medicine isn’t about Health<br/>
Consulting isn’t about Advice<br/>
School isn’t about Learning<br/>
Research isn’t about Progress<br/>
Politics isn’t about Policy</em>

The above summarizes much of my contrarian world view.  (What else should go on this list?) When I say “X is not about Y,” I mean that while Y is the function commonly said to drive most X behavior, in fact some other function Z drives X behavior more.  I won’t support all these claims here; for today, let’s just talk politics.

High school students are easily engaged to elect class presidents, even though they have little idea what if any policies a class president might influence.  Instead such elections are usually described as “popularity contests.”  That is, theses elections are about which school social factions are to have higher social status.  If a jock wins, jocks have higher status.  If your girlfriend’s brother wins, you have higher status, etc.  And the fact that you have a vote says that others should take you into account when forming coalitions – you are somebody.

<span id="more-17027"></span>

Civics teachers talk as if politics is about policy, that politics is our system for choosing policies to deal with common problems.  But [as Tyler Cowen suggests](is-ideology-abo), real politics seems to be more about who will be our leaders, and what coalitions will rise or fall in status as a result.  Election media coverage focuses on characterizing the candidates themselves – their personalities, styles, friends, beliefs, etc.  You might say this is because character is a cheap clue to the policies candidates would adopt, but I don’t buy it.
The obvious interpretation seems more believable – as with high school class presidents, we care about policies mainly as clues to candidate character and affiliations.  And to the extent we consider policies not tied to particular candidates, we mainly care about how policies will effect which kinds of people will be respected how much.

For example, we want nationalized medicine so poor sick folks will feel cared for, military actions so foreigners will treat us with respect, business deregulation as a sign of respect for hardworking businessfolk, official gay marriage as a sign we accept gays, and so on.

This perspective explains why voters tend to prefer proportional representation, why many refuse to vote for any candidate when none have earned their respect, and why so few are interested in institutional reforms that would plausibly give more informed policies.  (I’m speaking on such reform at a Trinity College <a href="http://www.smartdemocracy.com/">symposium</a> Monday afternoon.)

In each case where X is commonly said to be about Y, but is really X is more about Z, many are well aware of this but say we are better off pretending X is about Y.  You may be called a cynic to say so, but if honesty is important to you, join me in calling a spade a spade.

## [Views Aren’t About Sights](#table-of-contents)
_Posted on 2021-05-22_

Regarding window/patios with nice beach or city views, and days when those views are nice, in <a href="https://twitter.com/robinhanson/status/1395807388880494601">two</a> <a href="https://twitter.com/robinhanson/status/1395808539864707072">polls</a> respondents estimate that at any given time 1.3% and 0.8% of such places are actually occupied by people enjoying these views. Which makes one wonder why people bother to buy exclusive use, instead of sharing them. For example, ten tenants could share a single view spot for a tenth the price, and hardly ever have conflicts over who uses it when. We similarly see people owning boats and RVs that they hardly ever use and could instead rent more cheaply.

A related phenomenon is that most people strongly prefer to pay a monthly or annual fee for phones or internet, instead of paying per minute of use. Even though per usage payment can give better incentives for thriftiness. Similar for movies and TV shows. And, recently, e-books. And country clubs. Also, apparently a secret to <em>Amazon’s</em> success was that people much prefer to pay for shipping once per year than to pay each time they ship.

Many justifications are offered for these habits, some of them sensible. But surely a big fraction of all this is explained by signaling; people want others to know of and envy that they can afford to buy a view instead of renting it, and can afford the monthly phone fee, instead of having to worry about each call.

But if so, why don’t we buy more things via all-you-want-for-an-annual-fee? Like food or clothes or planes. You might say that these have high marginal costs, but then so do views and boats and RVs and country clubs.

I suspect part of the problem is that it just takes time to build up the scale required for the business arrangements which let people buy many things at marginal cost for an annual fee. Which makes me more optimistic about the future prospects for such programs. Places like <em>Costco</em> go somewhat in that direction, but we could go a lot further.

Imagine large menus of products where you can get as much of each one as you want (for personal use only) at their marginal cost, if you pay a corresponding annual fee. The higher an annual fee you pay, the larger a menu of things you can get at marginal cost. This arrangement not only gets you stuff more efficiently, grabbing anything that’s worth more to you than its marginal cost, but this also lets you signal your wealth by the menus you can afford.

It may take a lot of coordination to get all these suppliers to agree to deals where they sell stuff at marginal cost and get some fraction of the annual feel. And it takes some enforcement to prevent reselling. A single org that tries to arrange all this will face the usual scale diseconomies due to internal coordination costs. But I still think more of this is coming.

<strong>Added 23May:</strong> Many saying that they feel psychological aversions to renting, sharing, or paying per usage, aversions that go beyond concrete time and effort costs. They say this implies we aren’t avoiding these things to signal wealth. But that confuses different levels of causation. It could be that the <em>way</em> that our minds induce us to signal wealth is via making us feel these aversions.

## [Why Do Bets Look Bad?](#table-of-contents)
_Posted on 2013-07-08_

Most social worlds lack a norm of giving much extra respect to claims supported by offers to bet. This is a [shame](bets-argue) [because](suspecting-truth-hiders) such norms would reduce insincere untruthful claims, and so make for more accurate beliefs in listeners. But instead of advocating for change, in this post I wonder: <em>why</em> are such norms rare?
Yes there are random elements in which groups have which norms, and yes given a local norm that doesn’t respect bets it looks weird to offer bets there. But in this post I’m looking more to explain which norms appear where, and less who follows which norms.

Bets have been around for a long time, and by now most intellectuals understand them, and know that all else equal those who really believe more strongly are willing to bet more. So you might think it wouldn’t be that hard for a betting norm to get added on to all other local norms and cultural factors; all else equal respect bets as showing confidence. But if this happens it must be counter-balanced by other effects, or bets wouldn’t be so rare. What are these other effects?

While info often gets overtly shared in casual conversation, most of that info doesn’t seem very useful.  I thus conclude that casual conversation isn’t mainly about overtly sharing info. So I assume the obvious alternative: casual conversation is mostly about signaling (which is covert or indirect info sharing). But still the puzzle remains: whatever else we signal via conversation, why don’t we typically expect a betting offer to signal overall-admirable confidence in a claim?

One obvious general hypothesis to consider here is that betting signals typically conflict with or interact with other signals. But which other signals, and how? In the rest of this post I explore a few bad-looking features that bets might signal:

<ul>
<li><strong>Sincerity</strong> – In many subcultures it looks bad to care a lot about most any topic of casual conversation. Such passion suggests that you just don’t get the usual social functions of such conversations. Conversationalists ideally skip from topic to topic, showing off their wits, smarts, loyalties, and social connections, but otherwise caring little about the truth on particular topics. Most academia communities seem to have related norms. Offers to bet, in contrast, suggest you care too much about the truth on a particular topic. Most listeners don’t care if your claim is true, so aren’t interested in your confidence. Of course on some topics people are expected to care a lot, so this doesn’t explain fewer bets there.</li>
<li><strong>Conflict</strong> – Many actions we take are seen as signals of cooperation or conflict. That is, our actions are seen as indicating that certain folks are our allies, and that certain other folks are our rivals or opponents. A bet offer can be seen as an overt declaration of conflict, and thus make one look overly confrontational, especially within a group that saw itself as mainly made of allies. We often try to portray any apparent conflict in casual conversations as just misunderstandings or sharing useful info, but bets are harder to portray that way.</li>
<li><strong>Provinciality</strong> – Bets are most common today in sports, and sport arguments and bets seem to be mostly about showing loyalty to particular teams. In sports, confrontation is more ok and expected about such loyalties. Offering to bet on a team is seen as much like offering to have a fist fight to defend your team’s honor. Because of this association with regional loyalties in sports, offers to bet outside of sports are also seen as affirmations of loyalties, and thus to conflict with norms of a universal intellectual community.</li>
<li><strong>Imprudence</strong> – Some folks are impulsive and spend available resources on whatever suits their temporary fancy, until they just run out. Others are careful to limit their spending via various simple self-control rules on how much they may spend how often on what kinds of things. Unless one is in the habit of betting often from a standard limited betting budget, bets look like unusual impulsive spending. Bettors seem to not sufficiently keep under control their impulsive urges to show sincerity, make conflict, or signal loyalties.</li>
<li><strong>Disloyalty</strong> – In many conversations it is only ok to quote as sources or supports people outside the conversation who are “one of us.” Since betting markets must have participants on both sides of a question, they will have participants who are not part of “us”. Thus quoting betting market odds in support of a claim inappropriately brings “them” in to “our” conversation. Inviting insiders to go bet in those markets also invites some of “us” to interact more with “them”, which also seems disloyal.</li>
<li><strong>Dominance</strong> – In conversation we often pretend to support an egalitarian norm where the wealth and social status of speakers is irrelevant to which claims are accepted or rejected by the audience. Offers to bet conflict with that norm, by seeming to favor those with more money to bet. Somehow, who is how smart or articulate or has more free time to read are considered acceptable bases for conversation inequities. While richer folks could be expected to bet more, the conversation would have to explicitly acknowledge that they are richer, which is rude.</li>
<li><strong>Greed</strong> – We often try to give the impression that we talk mainly to benefit our listeners. This is a sacred activity. Offering to bet money makes it explicit that we seek personal gains, which is profane. This is why folks sometimes offer to bet charity; the money goes to the winner’s favorite charity. But that looks suspiciously like bringing profane money-lenders into a sacred temple.</li>
</ul>
Last week I [said](bets-argue) bets can function much like arguments that offer reasons for a conclusion. If so, how do arguments avoid looking bad in these ways? Since the cost to offer an argument is much less than the cost to offer a bet, arguments seem less imprudent and less show sincerity. Since the benefits from winning arguments aren’t explicit, one can pretend to be altruistic in giving them. Also, you can pretend an argument is not directed at any particular listener, and so is not a bid for conflict. Since most arguments t0day are not about sports, arguments less evoke the image of a sports-regional-signal. As long as you don’t quote outsiders, arguments seem less an invitation to invoke or interact with outsiders.
If we are to find a way to make bets more popular, we’ll need to find ways to let people make bets without sending these bad-looking signals.

<strong>Added</strong>: It is suspicious that I didn’t do this analysis much earlier. This is plausibly due to the usual corrupting effect of advocacy on analysis; because I advocated betting, I analyzed it insufficiently.

## [Homo Hypocritus](#table-of-contents)
_Posted on 2010-03-23_

<em>The standard social brain theory seems in conflict with standard anthropologist accounts of ancestral forager lifestyles.  Might “man the sly rule bender” resolve this conflict?</em>

Why do we have ginormous brains?  Animals tend to have big brains when they have big bodies, but beyond that the main brain <a href="http://www.sciencemag.org/cgi/content/abstract/317/5843/1344">pattern</a> <a href="http://www.sciencemag.org/cgi/content/abstract/317/5843/1347">is</a> [social](social-brain-theory-confirmed): bigger brains are found in birds and mammals that compete with predators or prey, and who manage pair-bonding mate relations.  The extra costs of big brains is outweighed by benefits of not being out-witted by others.
Primates (and hyenas) hit on the trick of reusing pair-bonding skills to manage friendships in large social groups.  Primates have huge expensive brains, which are bigger in species with larger social groups, and these groups spend more of their time managing social relations.  Bigger groups better protect against predators, though the coalition politics of dominance gets more complex in bigger groups.

Primates not only manage relations and coalitions, but they also <a href="http://www.sciencemag.org/cgi/content/abstract/317/5843/1347">track</a> the relations and coalitions of others.  They are adept at judging how to help their coalitions, and when to switch sides.  The top chimp is often not the strongest, but instead the one with the strongest coalition, which gets to dominate food and mating, and stay best protected from predators; chimp investments in big brains often pay off handsomely.

Humans have the biggest primate brains of all. Over the last two million years hominid brains grew more where climates were variable, but they [grew most](social-brain-theory-confirmed) where population densities were high.  This suggests that human brains were also big mainly due to social pressures.  The “mating mind” sexual selection hypothesis seems at odds with this density effect, and with the more general fact that polygamous species tend to have smaller brains.  “Man the tool user” stories seem to confuse broad group gains with individual benefits – smaller brains seem sufficient for copying others’ tool skills.  But even if social pressures were key, which pressures exactly?
Isolated nomadic forager bands today are “fossils” with crucial clues about our distant ancestors.  Anthropologists who study them <a href="http://foragers.wikidot.com/egalitarianism">report</a> <a href="http://www.hup.harvard.edu/catalog/BOEHIE.html">that</a> overt dominance is rare, and long distances make war rare (as 4 million year old fossils [suggest](humans-hide-fertility)). Foragers live in tight quarters and use language to express and enforce social norms on food sharing, non-violence, mating freedom, communal decision making, and norm enforcement.  Anger, bragging, giving orders, and anything remotely resembling dominance among men is punished by avoidance, exile, and death as required.  Human’s unusual hidden female fertility also limits male dominance temptations.
The puzzle here is that consistent enforcement of such norms seems to drastically reduce the payoff to expensive coalition-politics-savvy brains.  If you can’t collude to grab the food or the women, and everyone is treated fairly based on their contributions, why bother to be so clever?  Yes, some brain innovations were required to support language, and maybe they wouldn’t have occurred in a small brain, but after that innovation human brains could have shrunk (as perhaps with [hobbits](ancient-hobbits)).  Why did humans keep huge expensive brains?
In a messy real world, social norms expressed in language typically have many iffy boundary cases and ambiguities.  How much of what sort of food of what quality offered how conveniently counts as food sharing?  How big a frown is a grimace?  Sex with how close a relative counts as incest?  And so on.  This wouldn’t matter if boundary cases were decided randomly, but that seems unlikely.  Instead big brain gains come five ways:

<strong>Unnormed</strong> – coalition politics on acts uncovered by norms.<br/>
<strong>Skirt</strong> – keep actions near but not over edge of violating norms.<br/>
<strong>Cover</strong> – politics of observers on if to report an act to others.<br/>
<strong>Frame</strong> – lawyer-like arguing on if acts violate social norms.<br/>
<strong>Conspire</strong> – form coalitions on how to publicly interpet iffy acts.

Most norms have meta-norms against consciously trying to evade them.  Self-deception should help here; foragers might sincerely believe they usually just do their job and “tell it like it is”, and then unconsciously try to act, selectively report and frame acts, and support interpretation coalitions, to their advantage.  Instead of “man the tool user”, we might be better understood as “man the sly rule bender.”

Gains to rule bending could be greatly reduced via social norms with very clear simple rules.  But humans seems to usually prefer complex and ambiguous rules that require “judgment” to apply.  For example, foragers often have complex incest rules, forbidding a much wider range of sex partners than is needed to prevent genetic problems.  And acts of [sorcery](mysticisms-function) are allowed to count as acts of aggression that violate social norms and must be punished, even without concrete evidence showing such acts.  Both complex broad incest rules and allowing sorcery complaints greatly increase the scope for gains to large rule-bending brains, and suggest that we tend to prefer to allow such scope.
The idea that the main reason we have huge brains is to hypocritically bend rules seems to me a dramatic change in how we think about human nature.  If true, it should change how we understand a great many things in psychology and social science.  I’ve been obsessing about his topic for weeks, and last Thursday I ran it past Robin Dunbar, famed for his contributions to the social brain account, and he said it was pretty close to his view on the subject, and he suggested the incest example.

## [Resolving Your Hypocrisy](#table-of-contents)
_Posted on 2006-12-27_

> Self love is more cunning than the most cunning man in the world.  … Hypocrisy is the homage vice pays to virtue.    <a href="http://www.gutenberg.org/dirs/etext05/8roch10h.htm"><em>La Rochefoucauld</em></a>.


Humans are hypocrites.  That is, we present ourselves and our groups as pursuing high motives, when more often low motives better explain our behavior.   We say we invade nations to help them build democracy, rather than for revenge or security.  We say we marry to help our partner, rather than to gain sex or security.  We say we choose our [professio](do_helping_prof)[n](do_helping_prof) to help others, and not for prestige or income.  And so on.
 Comedians live by ridiculing such hypocrisy, but "<a href="http://hanson.gmu.edu/metacynic.html">cynics</a>" who complain without such wit and style are despised.  In contrast, we are attracted to the <a href="http://hanson.gmu.edu/innocence.html">innocent </a>who naively believe our hypocrisies.

Noticing the hypocrisy in others usually makes us feel morally superior.  After all, we are know we are not hypocrites; "I can look inside myself and and <em>see</em> my sincerity."  But eventually experience and intelligence force some of us to face the likelihood that we are no different.   At this point we can resolve our hypocrisy two ways: we can start really living up to our high ideals, or we can admit we don’t care as much as we thought about those ideals .   

 <span id="more-18327"></span> 

Most people try harder to live up to their ideals.  They usually think they succeed, but mostly they just add on a few more layers of self-deception, and find themselves too busy to ponder the issue. "Sure hypocrites give to charities that don’t really help much, but my charity really does help; I read an article that says so.  Sorry; gotta go."     

 We want to think well of ourselves, and this gives us a limited ability to make ourselves to want the things we think we should want.  And the young are more naturally innocent, with a stronger ability to remake their wants, at least toward ideals others would applaud.  But this effect fades with time, and we overestimate both how much we can change our wants, and how much we want to. 

 One of our ideals is to be honest with ourselves.   Is this honesty ideal a substitute or a complement for other ideals?   On the one hand, honesty should help us to to use resources more effectively to actually achieve other ideals, versus the appearance of achieving them.  On the other hand, I cannot reasonably expect anyone willing to try to live up to this ideal of honesty to have much will power left over to live up to other ideals.    

I expect people who are actually more honest will tend to have lower expectations about achieving ideal ends, though they may (or may not) actually achieve such ideals more.

<strong>Added:</strong>  Our conscious minds seem like a public relations department (PRD) of our minds.  A corporate PRD tries to find a coherent story to make it look like corporate actions came from high motives.  The PRD tries to have this high minded story recorded in official histories, legal testimony, and accounting records.  Corporate PRDs have a limited ability to influence corporate policy; "Boss, doing that will make us look real bad."   But corporate profits more fundamentally drive behavior.  

Similarly, our conscious minds record and tell high-minded stories about our actions.  When image is important enough, we can make real sacrifices to ensure our actions fit closely with our conscious self-image.  But we usually need only minor sacrifices, my guess is that a cost-minimizing PRD forced to be more honest will rely more on admitting to low motives, and less on switching from low to high motives.

## [Errors, Lies, and Self-Deception](#table-of-contents)
_Posted on 2009-06-15_

<a href="http://bps-research-digest.blogspot.com/2009/06/were-unable-to-read-our-own-body.html">About</a> a recent <em>European Journal of Personality</em> article:

The participants recorded a one minute television commercial, … then watched … themselves, having been given guidance on non-verbal cues that can reveal how extraverted or introverted a person is. … They were then asked to rate their own personality. … The participants’ extroversion scores on the implicit test showed no association with their subsequent explicit ratings of themselves, and there was no evidence either that they’d used their non-verbal behaviours (such as amount of eye contact with the camera) to inform their self-ratings.

In striking contrast, outside observers who watched the videos made ratings of the participants’ personalities that did correlate with those same participants’ implicit personality scores, and it was clear that it was the participants’ non-verbal behaviours that mediated this correlation … Two further experiments showed that this general pattern of findings held even when participants were given a financial incentive.

[Folks seem] extremely reluctant to revise their self-perceptions, even in the face of powerful objective evidence. … Participants seemed able to use the videos to inform their ratings of their “state” anxiety (their anxiety “in the moment”) even while leaving their scores for their “trait” anxiety unchanged.

(Hat tip to Michael Webster.)  This sort of thing terrifies me.   Let me explain why.<span id="more-18789"></span>

Any long complex design or calculation is subject to <strong>errors</strong>.  And those who do such things regularly must get into the habit of testing and checking for such errors.  This may take most of the effort, but it is at least manageable, because we expect that such errors are not very correlated with other features of interest.   If something has worked ten times in a row in field tests, it will probably work the first time for a customer, at least if that customer’s environment is not too different from field test environments.

People who have to worry about spies and <strong>liars</strong>, on the other hand, have to worry more about troublesome correlations.  Liars can coordinate their lies to tell a consistent story.  Spies and liars can choose carefully to betray us exactly when such defections are the hardest to detect and the most expensive.  So the fact that a possible spy performed reliably ten times in a row gives less confidence that he will also perform reliably the next time, if the next time is unusually important.  In these cases we rely more on private info, i.e., what the spy or liar could not plausibly know.   For example, if we do not let the possible spy know which are the important cases, he can’t choose only those cases to betray us.   And if we can check on him at unexpected times, we might catch him in a lie.

We humans have many conscious beliefs, and we are built to have accurate ones in many situations, but in many other situations we are built to have misleading conscious beliefs, i.e., to be <strong>self-deceived</strong>.   Evolution judged that such misleading beliefs would tend to help us fool our colleagues, and so better survive and reproduce.   It created subconscious mental processes to manage this process of deciding when our beliefs should be accurate or misleading.

We seem almost completely defenseless against such manipulation.  Yes we can try to check our conscious beliefs against outside standards, but our subconscious liars can not only choose carefully when to lie about what, but they probably also have access to all our conscious thoughts and info!  They might even lie to us about whether we checked our beliefs, and what those checks found.  So in principle our unconscious liars can execute extremely complex and subtle lying plans.  For example, the study above suggests that such processes choose to make us blind to clues about our average public speaking anxiety, while letting us see momentary fluctuations about that average.

If our subconscious liars were as smart and thoughtful as our conscious minds, we would seem to be completely at their mercy.  The situation may not be that bad, but it is not clear how we can tell just how bad the situation is; even if they had complete control, they would probably want us to think otherwise.

This is the context in which I find myself interested in “minimal rationality,” similar to [minimal morality](minimal-morals).  In the limit of my being subject to very powerful subconscious liars, how can I best avoid their distortions?  It seems I should then become especially distrustful of intuition, and especially interested in trustworthy processes outside myself, such as prediction markets and formal analysis.
If I have a choice between two ways to make an estimate, and one of them allows more discretion by subconscious mental processes, I should try to go with the other choice if possible.   If the data is pretty clear and theory needs a lot of judgment calls to get an answer, I go with the data.  If the data is messy and needs judgement calls while standard theory gives a pretty clear answer, I go with that theory.

Of course this minimal rationality approach makes me subject to my subconscious lying about which estimates allow more subconscious discression.  So I need to be especially careful about those judgments.  But what else can I do?

Many folks figure that if evolution planned for them to believe a lie, they might as well believe a lie; that probably helps them acheive their goals.  But I want, first and foremost, to believe the truth.

## [Enforce Common Norms On Elites](#table-of-contents)
_Posted on 2019-02-20_

In my experience, elites tend to differ in how they adhere to social norms: their behavior is more context-dependent. Ordinary people use relatively simple strategies of being generally nice, tough, silly, serious, etc., strategies that depend on relatively few context variables. That is, they are mostly nice or tough overall. In contrast, elite behavior is far more sensitive to context. Elites are often very nice to some people, and quite mean to others, in ways that can surprise and seem strange to ordinary people.

The obvious explanation is that context-dependence is gives higher payoffs when one has the intelligence, experience, and social training to execute this strategy well. When you can tell which norms will tend to be enforced how when and by whom, then you can adhere strongly to the norms most likely to be enforced, and neglect the others. And skirt right up to the edge of enforcement boundaries. For weakly enforced norms, your power as an elite gives you more ways to threaten retaliation against those who might try to enforce them on you. And for norms that your elite associates are not particularly eager to enforce, you are more likely to be given the benefit of the doubt, and also second and third chances even when you are clearly caught.

One especially important human norm says that we should each do things to promote a general good when doing so is cheap/easy, relative to the gains to others. Applied to our systems, this norm says that we should all do cheap/easy things to make the systems that we share more effective and beneficial to all. This is a weakly enforced norm that elite associates are not particularly eager to enforce.

And so elites do typically neglect this system-improving norm more. Ordinary people look at a broken system, talk a bit out how it might be improved, and even make a few weak moves in such directions. But ordinary people know that elites are in a far better position to make such moves, and they tend to presume that elites are doing what they can. So if nothing is happening, probably nothing can be done. Which often isn’t remotely close to true, given that elites usually see the system-improving norm as one they can safely neglect.

Oh elites tend to be fine with getting out in front of a popular movement for change, if that will help them personally. They’ll even take credit and pretend to have started such a movement, pushing aside the non-elites who actually did. And they are also fine with taking the initiative to propose system changes that are likely to personally benefit themselves and their allies. But otherwise elites give only lip service to the norm that says to make mild efforts to seek good system changes.

This is one of the reasons that I [favor](checkmate-on-blackmail) making blackmail legal. That is, while one might have laws like libel against making false claims, and laws against privacy invasions such as posting nude picts or stealing your passwords, if you are going to allow people to tell true negative info that they gain through legitimate means, then you should also let them threaten to not tell this info in trade for compensation.
Legalized blackmail of this sort would have only modest effects on ordinary people, who don’t have much money, and who others aren’t that interested in hearing about. But it would have much stronger effects on elites; elites would be found out much more readily when they broke common social norms. They’d be punished for such violations either by the info going public, or by their having to pay blackmail to keep them quiet. Either way, they’d learn to adhere much more strongly to common norms.

Yes, this would cause harm in some areas where popular norms are dysfunctional. Such as norms to never give in to terrorists, or to never consider costs when deciding whether to save lives. Elites would have to push harder to get the public to accept norm changes in such areas, or they’d have to follow dysfunctional norms. But elites would also be pushed to adhere better to the key norm of working to improve systems when that is cheap and easy. Which could be a big win.

Yes trying to improve systems can hurt when proposed improvements are evaluated via naive public impressions on what behavior works well. But efforts to improve via making new small scale trials that are scaled up only when smaller versions work well, that’s much harder to screw up. We need a <em>lot</em> more of that.

Norms aren’t norms if most people don’t support them, via at least not disputing the claim that society is better off when they are enforced. If so, most people must say they expect society to be better off when we find more cost-effective ways to enforced current norms. Such as legalizing blackmail. This doesn’t necessarily result in our choosing to enforce norms more strictly, though this may often be the result. Yes, better norm enforcement can be bad when norms are bad. But in that case it seems better to persuade people to change norms, rather than throwing monkey-wrenches into the gears of norm enforcement.

So let’s hold our elites more accountable to our norms, listen to them when they suggest that we change norms, and especially enforce the norm of working to improve systems. Legalized blackmail could help with getting elites to adhere more closely to common norms.

## [Identity Norms](#table-of-contents)
_Posted on 2019-04-15_

Over the weekend I did a series of Twitter polls on identity. Seeing a <a href="http://Over the weekend I did a series of Twitter polls on identity. Seeing a survey showing that 74% of blacks but only 15% of whites find race to be central to their identity, I asked if this attitude is good for either group, and found 83% saw it as bad for both groups. Asking a similar question on sex, answers were more split, with 50% saying it is bad for both and 43% saying it is good for both. In both race and sex cases, less than 8% said it was good for one group but bad for the other. I think picked 16 features and asked which one is best for most people to treat as most central to their identity. I got these relative weights: personality 28%, family 14%, smarts 8%, fav hobby 8%, ideology 7%, job 7%, age 6%, religion 5%. gender 4%, class 3%, race 2.2%, urban area 1.6%, fav fiction 0.7%, looks 0.7%. Finally, I asked if seeing someone else treating a feature as a central to their identity tempts you more (or less) to treat it as central to your identity, and how that depends on if they have same or different value of that feature from you. I found that for features like personality, family, or favorite hobby, people think they’ll make a feature more central when they see others treat it as central, and that happens more when those others share their feature value. But for features like race, gender, or class, it was the opposite; seeing others treat it as central makes them less likely to treat it as central, an effect that is stronger when those others have a different feature value. To make sense of these results, let me invoke two theories of identity, and two relevant social norms. One theory is that identity is a way to simply ourselves to be more easily understood and predicted: We are built to find a simple story we can project about who we are that will let others predict us well. This story includes what we like, what we are good at, how we decide who we are loyal to, and so on. Such stories are naturally more than a few stats but less than all our details. … Early in our lives we search for a story that fits well with our abilities and opportunities. In our unstable youth we adjust this story as we learn more, but we reduce those changes as we start to make big life choices, and want to appear stable to our new associates. Another theory is that identity is a way to coordinate on our social/political coalitions; we ally with folks like us. Sarah Constantin: Dasein is … self-definition with respect to a social context. Where do I fit in society? Who is my tribe? Who am I relative to other people? What’s my type? “Identifying as” always includes an element of misdirection. Merely describing yourself factually (“I was born in 1988”) is not Dasein. Placing an emphasis, exaggerating, cartoonifying, declaring yourself for a team, is Dasein. But when you identify as, you say “I am such-and-such”, as though you were merely describing. … One of the qualities of Dasein is that it’s very very stealthy, and it wants everything to be about Dasein, so it winds up muddying the waters, even when you don’t intend it to. … Dasein can mess up the attempt to solve social problems. … Sexual harassment gets perceived as a flag for pink-flavored people to wave, and if you’re not pink-flavored, you’re not the target market, so you don’t take it seriously. One common human norm is that sub-group coalitions are mildly illicit. We aren’t supposed to break into factions that each fight other factions; we are supposed to all work together toward common goals, and treat each other as individuals. As with other norms against fighting, it is more okay to defend yourself against attacks from others, but you aren’t supposed to start a fight. This norm against factions explains a lot of the above poll data. Regarding what features to have as central to your identity, we approve of features which are actually useful to predict individual behavior, where people with different feature values tend to complement each other. In contrast, we disapprove of features that could more easily be used, and that have recently been used, as the basis of factional fights. People who treat less approved features as more central to their identity compensate by claiming that there is already a pre-existing faction fight along that feature, one that the other side started, and where the other side isn’t fighting fair (e.g, fighting via dominance and not prestige). They invoke our common human norm that requires independent observers to support the side of a fight favored by justice and fairness.">survey</a> showing that 74% of blacks but only 15% of whites find race to be central to their identity, I <a href="https://twitter.com/robinhanson/status/1117169689598341120">asked</a> if this attitude is good for either group, and found that 83% saw it as bad for both groups. <a href="https://twitter.com/robinhanson/status/1117430505937543168">Asking</a> a similar question on sex, answers were more split, with 50% saying it is bad for both and 43% saying it is good for both. In both the race and sex cases, less than 8% said it was good for one group but bad for the other.

I then picked 16 features and <a href="https://twitter.com/robinhanson/status/1117592714848210945">asked</a> which one is best for most people to treat as most central to their identity. I got these relative weights: personality 28%, family 14%, smarts 8%, fav hobby 8%, ideology 7%, job 7%, age 6%, religion 5%. gender 4%, class 3%, race 2.2%, urban area 1.6%, fav fiction 0.7%, looks 0.7%.

Finally, I <a href="https://twitter.com/robinhanson/status/1117761610481852416">asked</a> <a href="https://twitter.com/robinhanson/status/1117760097139216384">if</a> seeing someone else treating a feature as a central to their identity tempts you more (or less) to treat it as central to your identity, and how that depends on if they have same or different value of that feature from you. I found that for features we approve of for identity, like personality, family, or favorite hobby, people think they’ll make a feature more central when they see others treat it as central, and that happens more when those others share their feature value. But for features we disapprove of for identity, like race, gender, or class, it was the opposite; seeing others treat it as central makes them less likely to treat it as central, an effect that is stronger when those others have a different feature value.

To make sense of these results, let me invoke two theories of identity, and two relevant social norms.

One [theory](a-theory-of-identity) is that identity is a way to simplify ourselves to be more easily understood and predicted:
> We are built to find a simple story we can project about who we are that will let others predict us well. This story includes what we like, what we are good at, how we decide who we are loyal to, and so on. Such stories are naturally more than a few stats but less than all our details. … Early in our lives we search for a story that fits well with our abilities and opportunities. In our unstable youth we adjust this story as we learn more, but we reduce those changes as we start to make big life choices, and want to appear stable to our new associates.

Another <a href="https://srconstantin.wordpress.com/2017/03/09/4896/">theory</a> is that identity is a way to coordinate on our social/political coalitions; we ally with folks like us. Sarah Constantin:

> Dasein is … self-definition with respect to a social context. Where do I fit in society? Who is my tribe? Who am I relative to other people? What’s my type? “Identifying as” always includes an element of misdirection. Merely describing yourself factually (“I was born in 1988”) is not Dasein. Placing an emphasis, exaggerating, cartoonifying, declaring yourself for a team, is Dasein. But when you identify as, you say “I am such-and-such”, as though you were merely describing. …
> One of the qualities of Dasein is that it’s very very stealthy, and it wants everything to be about Dasein, so it winds up muddying the waters, even when you don’t intend it to. … Dasein can mess up the attempt to solve social problems. … Sexual harassment gets perceived as a flag for pink-flavored people to wave, and if you’re not pink-flavored, you’re not the target market, so you don’t take it seriously.

One common human norm is that sub-group coalitions are mildly illicit. We aren’t supposed to break into factions that fight other factions; we are supposed to all work together toward common goals, and treat each other as individuals. As with other norms against fighting, it is more okay for a group to defend itself against attacks from others, but you aren’t supposed to start a fight.

This norm against factions explains a lot of the above poll data. Regarding what features to have as central to your identity, we approve of features which are actually useful to predict individual behavior, features where people with different feature values tend to complement each other, and features which are hard to use for coalitions because they are too granular (e.g., families). In contrast, we disapprove of features that could more easily be used, and that have recently been used, as the basis of factional fights.

People who treat less approved features as more central to their identity compensate by claiming that there is already a pre-existing faction fight along that feature in which they are they underdogs; the other side started the fight, and isn’t fighting fair (e.g, via dominance and not prestige). They invoke our common human norm that requires independent observers to support the side of a fight that is favored by justice and fairness.

Combining these theories and norms we can say that we have a licit and an illicit reason to choose identities: simplifying ourselves and joining coalitions. We often pretend to do the former while we actually do the latter. And when it gets too obvious that we are doing the latter, we try the excuses that they started it or that they aren’t fighting fair.

From all this I conclude that we have a limited tolerance for identity politics. The more different features that become a basis for explicit coalitional fights, the less happy we will all become, and the less tolerance we will have for each fight. We can together only handle a few big factional fights at any one time, and so we’ll have to set a high bar for how clear is the evidence in each case that they started it and are not fighting fair. And when we do see justice and fairness as clearly favoring one side of a fight, we’ll want to aid that side, make justice happen, and then [end](consider-reparations) the fight.

## [Exclusion As A Substitute For Norms, Law, & Governance](#table-of-contents)
_Posted on 2017-12-18_

Hell may not be other people, but worry sure is. That is, what we worry most about is what other people might do to us. People at the office, near our home, at the store, on the street, and even at church.

To reduce our worries, we can rely on norms, law, and governance. That is, to discourage bad behavior, we can encourage stronger informal social rules, we can adopt more formal legal rules, and we can do more with complex governance mechanisms.

In addition, we can rely on a simple and robust ancient solution: exclusion. That is, we can limit who is allowed with the circles we travel. We can use exclusion to limit who lives in our apartment complex, who shows up at the parties we attend, and who works in a cubicle near us.

Now the modern world tends to say that it disapproves of exclusion. The bad ancient world did much gossiping about what types of people could be trusted how, and then it relied a lot on the resulting shared judgements within their norms, law, and governance. We today have instead been trying to expunge such judgments from our formal systems; they are supposed to treat everyone equally without much reference to the groups to which they belong.

In addition, we’ve become more wary of using harsh punishments, like torture, death, or exile.  And we are more wary of using corruptible quick and dirty evaluations within our norms, law, and governance. For example, we have raised our standards for shunning neighbors, pulling over drivers, convicting folks at court, and approving large bold governance changes. And people today seem less willing to help the law via reports and testimony. Oh we may be more willing to apply norms to people we read about on social media; but we apply them less to the people we meet around us.

As a result of these trends, many people perceive that we have on net weakened the power of our systems of norms, law, and governance to constrain bad behavior. In response, I think they’ve naturally increased their reliance on exclusion. They look more carefully at who they allow into their schools, firms, apartments, and nations. And they are less willing to give a marginal person the benefit of the doubt.

Since we don’t want to look like we are excluding on the basis of simple group affiliations, we instead try to rely on a more intuitive and informal aggregation of many weak clues. We try to get a feel for how much we like them or feel comfortable with them overall. But that need not result in more mixing.

For example, colleges that admit people just on GPA and test scores can be more open to lower class students than colleges that require applicants to have adopted the right set of extracurricular actives, and to have hit on the right themes in their essays. Lower class people can find it is easier to get good grades and scores than to track the new fashions in activities and essays.

Similarly, Tyler Cowen makes the point somewhere that when firms had simple and clear rules on dress and behavior, someone with a low class background could more easily pass as high class; they just had to follow the rules. Today, without such simple rules, people rely more on many subtle clues of clothes, conversation topics, travel locations, favorite music and movies, and so on. Someone with a lower class background finds it harder to adopt all these patterns, and so is more obviously outed and rejected as not one of us.

The point seems to apply more generally. The net effect of our today relying less on norms, law, and governance, and avoiding simple group labels in exclusion, is that we rely more on exclusion based on an intuitive feel that someone is like us.

This may be a cause of our increasing class and political polarization, at home and work. Feeling less protected by norms, law, and governance, and shy of using simple group identifiers, we are more and more surrounding ourselves with others who feel comfortably like us. We can tell ourselves that we aren’t excluding Joe or Sue <em>because</em> they are Republicans, or don’t have a college degree. Its just that those sort of people tend to give off dozens of other off-putting signs that they are just not people like us.

We would call it an outrage if society as a whole excluded them explicitly and formally because of a few simple signs. Only ignorant and rude societies do that. But we feel quite comfortable excluding them from our little part of the world based on our just not feeling comfortable with them. Hey, as anyone knows, in our part of the world it is just really important to have the right people.

Consider this [another](prefer-law-to-values) weak argument for relying more on stronger norms, law, and governance. That could let us rely less on exclusion locally. And mix up a bit more.

## [How Idealists Aid Cheaters](#table-of-contents)
_Posted on 2019-08-23_

Humans have [long](hail-christopher-boehm) used norms to great advantage to coordinate behavior. Each norm requires or prohibits certain behavior in certain situations, and the norm system requires that others who notice norm violations call attention to those violations and coordinate to discourage or punish them.
This system is powerful, but not infinitely so. If a small enough group of people notice a minor enough norm violation, and are friendly enough with each other and with the violator, they often coordinate instead to <em>not</em> enforce the norm, and yet pretend that they did so. That is, they let cheaters get away with it.

To encourage norm enforcement, our social systems make many choices of how many people typically see each behavior or its signs. We pair up police in squad cars, and decide how far away in the police organizational structure sits internal affairs. Many kinds of work is double-checked by others, sometimes from independent agencies. Schools declare honor-codes that justify light checking. At times, we “measure twice and cut once.”

These choices of how much to check are naturally tied to our estimates of how strongly people tend to enforce norms. If even small groups who observe violations will typically enforce them, we don’t need to check as much or as carefully, or to punish as much when we catch cheaters. But if large diverse groups commonly manage to coordinate to evade norm enforcement, then we need frequent checks by diverse people who are widely separated organizationally, and we need to punish cheaters more when we catch them.

I’ve been reading the book <a href="https://en.wikipedia.org/wiki/Moral_Mazes"><em>Moral Mazes</em></a> for the last few months; it is excellent, but also depressing, which is why it takes so long to read. It makes a strong case, through many detailed examples, that in typical business organizations, norms are actually enforced <em>far</em> less than members pretend. The typical level of checking is in fact far too little to effectively enforce common norms, such as against self-dealing, bribery, accounting lies, fair evaluation of employees, and treating similar customers differently. Combining this data with other things I know, I’m convinced that this applies not only in business, but in human behavior more generally.

We often argue about this key parameter of how hard or necessary it is to enforce norms. [Cynics](?s=cynic) tend to say that it is hard and necessary, while idealists tend to say that it is easy and unnecessary. This data suggests that cynics tend more to be right, even as idealists tend to win our social arguments.
One reason idealists tend to win arguments is that they impugn the character and motives of cynics. They suggest that cynics can more easily see opportunities for cheating because cynics in fact intend to and do cheat more, or that cynics are losers who seek to make excuses for their failures, by blaming the cheating of others. Idealists also tend to say what while other groups may have norm enforcement problems, our group is better, which suggests that cynics are disloyal to our group.

Norm enforcement is expensive, but worth it if we have good social norms, that discourage harmful behaviors. Yet if we under-estimate how hard norms are to enforce, we won’t check enough, and cheaters will get away with cheating, canceling much of the benefit of the norm. People who privately know this fact will gain by cheating often, as they know they can get away with it. Conversely, people who trust norm enforcement to work will be cheated on, and lose.

When confronted with data, idealists often argue, successfully, that it is good if people tend to overestimate the effectiveness of norm enforcement, as this will make them obey norms more, to everyone’s benefit. They give this as a reason to teach this overestimate in schools and in our standard public speeches. And so that is what societies tend to do. Which benefits those who, even if they give lip service to this claim in public, are privately selfish enough to know it is a lie, and are willing to cheat on the larger pool of gullible victims that this policy creates.

That is, idealists aid cheaters.

<strong>Added 26Aug:</strong> In this post, I intended to <em>define</em> the words “idealist” and “cynic” in terms of how hard or necessary it is to enforce norms. The use of those words has distracted many. Not sure what are better words though.

## [Beware Mob War Strategy](#table-of-contents)
_Posted on 2022-09-26_

The game theory is clear: it can be in your interest to make threats that it would not be in your interest to carry out. So you can gain from committing to carrying out such threats. But only if you do it right. Your commitment plan must be simple and clear enough for your audience to see when it applies to them, how it is their interest to go along with it, and that people who look like you to them have in fact been consistently following such a plan.

So, for example, it probably won’t work to just lash out at whomever happens to be near you whenever the universe disappoints you somehow. The universe may reorganize to avoid your lashings, but probably not by catering to your every whim. More likely, others will avoid you, or crush you. That’s a bad commitment plan.

Here’s a good commitment plan. A well-run legal system can usefully deter crime via committing to consistently punish law violations. Such a system clearly defines violations, and shows potential violators an enforcement system wherein a substantial fraction of violations will be detected, prosecuted, and punished. Those under the jurisdiction of this law can see this fact, and understand which acts lead to which punishments. Such acts can thus be deterred.

Here’s another pretty good commitment plan. The main nations with nuclear weapons seem to have created a mutual expectation of “mutually assured destruction.” Each nation is committed to responding to a nuclear attack with a devastating symmetric attack. So devastating as to deter attack even if there is a substantial chance that such a response wouldn’t happen. This commitment plan is simple, easy to understand, clearly communicated, and quite focused on particular scenarios. So far, it seems to have worked.

Humans are often willing to suffer large costs to punish those who violate their moral rules. In fact, we probably evolved such moral indignation in part as a way to commit to punishing violations of our local moral norms. In small bands, with norms that were stable across many generations, members could plausibly achieve sufficient clarity and certainty about norm enforcement to deter violations via such threats. So such commitments might have had good plans in that context.

But this does _not_ imply that things would typically go well for us if we freely indulged our moral indignation inclinations in our complex modern world. For example, imagine that we encouraged, instead of discouraged, mob justice. That is, if we encouraged people to gossip to convince their friends to share their moral outrange, building off of each until they chased down and “lynched” any who offended them.

This sort of mob justice can go badly for a great many reasons. We don’t [actually](https://www.overcomingbias.com/2017/12/10-implications-of-automatic-norms.html) share norms as closely as we think, mob members are often more eager to show loyalty to each other than to verify accusation accuracy, and some are willing to make misleading accusations to take down rivals. More fundamentally, we might say that mob justice goes bad because it is not based on a good commitment plan. Observers just can’t predict mob justice outcomes well enough for it to usefully encourage good behavior, at least compared to a formal legal system.

Now consider the subject of making peace deals to end wars. Such as the current war between Russia and Ukraine. An awful lot of people, probably a majority, of the Ukrainian supporters I’ve heard from seem to be morally offended by the idea of such a peace deal in this case. Even though the usual game theory analyses of war say that there are usually peace deals that both sides would prefer at the time to continued war. (Such deals could focus on immediately verifiable terms; they needn’t focus on unverifiable promises of future actions. In April 2022 Russia and Ukraine [apparently](https://twitter.com/I_Katchanovski/status/1564687777022840832) had a tentative deal, scuttled due to pressure from Ukrainian allies.)

Many of these peace deal opponents are willing to justify this stance in consequentialist terms: they say that we should commit to not making such deals. Which, as they are eager to point out, is a logically coherent stance due to the usual game theory analysis. We should thus “hold firm”, “teach them a lesson”, “don’t let them get away with it”, etc. All justified by game theory, they say.

The problem is, I haven’t seen anyone outline anything close to a good commitment plan here. Nothing remotely as clear and simple as we have with criminal law, or with mutually assured destruction. They don’t clearly specify the set of situations where the commitment is to apply, the ways observers are to tell when they are in such situations, the behavior that has been committed to there, or the dataset of international events that shows that people that look like us have in fact consistently behaved in this way. Peace deal opponents (sometimes called “war mongers”) instead mainly just seem to point to their mob-inflamed feelings of moral outrage.

For example, some talk as if we should just ignore the fact that Russia has nuclear weapons in this war, as if we have somehow committed to doing that in order to prevent anyone from using nuclear weapons as a negotiating leverage. The claim that nations have been acting according to such a commitment doesn’t seem to me at all a good summary of the history of nuclear powers. And if the claim is that we should start now to create such a commitment by just acting as if it had always existed, that seems even crazier.

If we have not actually found and clearly implemented a good commitment plan, then it seems to me that we should proceed as if we have not made such a commitment. So we must act in accord with the usual game theory analysis. Which says to compromise and make peace if possible. Especially as a way to reduce the risk of a large nuclear war.

The possibility of a global nuclear war seems a _very_ big deal. Yes, war seems sacred and that inclines us toward relying on our intuitions instead of conscious calculations. It inclines us toward mob war strategy. But this issue seems plenty important enough to justify our resisting that inclination. Yes, a careful analysis may well identify some good commitment plans, after which we could think about how to move toward making commitments according to those plans.

But following the vague war strategy inclinations of our mob-inflamed moral outrage seems a poor substitute for such a good plan. If we have not yet actually found and implemented a good plan, we should deal with a world where we have not made useful commitments. And so make peace, to avoid risking the destructions of war.

## [Automatic Norms](#table-of-contents)
_Posted on 2017-12-27_

Some new ideas I want to explain start with a 2000 paper on <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.126.6275&amp;rep=rep1&amp;type=pdf">Taboo Tradeoffs</a>. (See also <a href="https://www.scientificamerican.com/article/psychology-of-taboo-tradeoff/">newer</a> <a href="https://www.sciencedirect.com/science/article/pii/S1755534517300684">stuff</a>.) So I’ll review that paper in this post, and then I’ll explain my new ideas in the [next post](10-implications-of-automatic-norms).
In Experiment 2 of the 2000 paper, each of 228 subjects were asked to respond to one of 8 scenarios, created by three binary alternatives. All the scenarios involved:

Robert, the key decision maker, was described as the Director of Health Care Management at a major hospital who confronted a “resource allocation decision.”

Robert was either asked to make a tragic tradeoff, where two sacred values conflicted, or a taboo tradeoff, where a sacred value was in conflict with a non-sacred value. The tragic tradeoff:

Robert can either save the life of Johnny, a five year old boy who needs a liver transplant, or he can save the life of an equally sick six year old boy who needs a liver transplant. Both boys are desperately ill and have been on the waiting list for a transplant but because of the shortage of local organ donors, only one liver is available. Robert will only be able to save one child.

The taboo tradeoff:

Robert can save the life of Johnny, a five year old who needs a liver transplant, but the transplant procedure will cost the hospital $1,000,000 that could be spent in other ways, such as purchasing better equipment and enhancing salaries to recruit talented doctors to the hospital. Johnny is very ill and has been on the waiting list for a transplant but because of dire shortage of local organ donors, obtaining a liver will be expensive. Robert could save Johnny’s life, or he could use the $1,000,000 for other hospital needs.

Robert was said to either find this decision easy or difficult:

“Robert sees his decision as an easy one, and is able to decide quickly,” or “Robert finds this decision very difficult, and is only able to make it after much time, thought, and contemplation.”

Finally, Robert was said to have chosen to save Johnny, or to have chosen otherwise. Subjects were asked to rate Robert’s decision and describe their feelings about it in 8 ways. They were also asked to make 3 decisions on actions regarding Robert, including dismiss from job, punish, and end friendship. Using factor analysis all these responses were combined into an <em>outrage</em> factor, mainly weighted on 6 of the ratings and feelings, and a <em>punish</em> factor, mainly weighted on the 3 actions. These factors were on a 1-7 point scale. Here are the average factor values for the eight possible scenarios:

[](TabooTradeoffs)
In the case of a taboo tradeoff, Robert is less likely to be punished for saving Johnny than for not.  We have a strong social norm against trading sacred things for non-sacred things, and Robert is to be punished if he violates this taboo. When Robert makes a sacred tradeoff, it is as if he must violate a norm no matter what he does. In this case, he is punished much more if he treats this as an easy choice; norm violation must be done in a serious thoughtful manner.

However, when Robert makes a taboo tradoff, he is punished much more if he treats this as a difficult choice. In fact, he is punished almost as much for saving Johnny after much thought as he is for not saving Johnny after little thought! It is <em>worse</em> to do the wrong thing after careful thought than after little thought.

Years ago, this result helped me to <a href="https://www.mitpressjournals.org/doi/pdf/10.1162/itgg.2007.2.3.73">understand</a> the political reaction when in 2003 my Policy Analysis Market (PAM) was accused of trying to let people bet on terrorist deaths.

PAM appeared to some to cross a moral boundary, which can be paraphrased roughly as “none of us should intend to benefit when some of them hurt some of us.” (While many of us do in fact benefit from terrorist attacks, we can plausibly argue that we did not intend to do so.) So, by the taboo tradeoff effect, it was morally unacceptable for anyone in Congress or the administration to take a few days to think about the accusation. The moral calculus required an immediate response.

Of course, no one at high decision-making levels knew much about a $1 million research project within a $1 trillion government budget. If PAM had been a $1 billion project, representatives from districts where that money was spent might have considered defending the project. But there was no such incentive for a $1 million project (spent mostly in California and London); the safe political response was obvious: repudiate PAM, and everyone associated with it. (<a href="https://www.mitpressjournals.org/doi/pdf/10.1162/itgg.2007.2.3.73">more</a>)

Today, however, my interest is in what these results imply for our awareness of where our norm feelings come from, and how much they are shared by others. These results suggest that when we face a choice, the categorization of some of the options as norm violating is supposed to come to us fast, and with little thought or doubt. Unless we notice that all of the options violate similarly important norms, we are supposed to be sure of which options to reject, without needing to consult with other people, and without needing to try to frame the choice in multiple ways, to see if the relevant norms are subject to framing effects. We are to presume that framing effects are unimportant, and that everyone agrees on the relevant norms and how they are to be applied.

Apparently the legal principle of “ignorance of the law is no excuse” isn’t just a convenient way to avoid incentives not to know the law, and to avoid having to inquire about who knows what laws. Regarding norms more generally, including legal norms, we seem to think “ignorance of the norms isn’t plausible; you must have known.”

If this description is correct, it seems to me to have remarkable implications. Which I’ll discuss in my next post. (Unless of course you figure them all out in the comments now.)

## [10 Implications of Automatic Norms](#table-of-contents)
_Posted on 2017-12-28_

My [last post](automatic-norms) observed that we seem to have a meta-norm that norm application should be automatic and obvious. We are to just know easily and surely which actions violate norms, without needing to reflect on or discuss the matter. We are to presume that framing effects are unimportant, and that everyone agrees on the relevant norms and how they are to be applied. If true, this has many implications:
1) We rarely feel much need to think about or discuss with others whether our own behavior violates norms. We either feel sure that we are innocent, or we feel at risk of being guilty. If we end up being seen as guilty, we’d rather be able to claim that we forgot, were distracted, or were overcome by passion. Any evidence that we discussed or thought carefully about the choice would instead suggest that we consciously choose to be guilty.

2) We aren’t much interested in ethics and misbehavior discussion or training for the purpose of helping us to figure out what to do personally. We may, however, be interested in using such things as a way to show others that we are devoted to good norms, and that we despise those who violate them. We are far more interested in norm preaching than learning or analysis.

3) We feel justified in accusing others of bad motives when they seem to us to violate norms. It seems to us that either they intended to be guilty, or they were inexcusably sloppy or lacking in control of their passions. We usually don’t need to wonder how they framed the situation, what norms they applied, or how they interpreted those norms. Of course we may not feel obligated to point out their violation, but we’d feel justified if we did.

4) We feel justified in describing those who claim to disagree with us about particular cases as either stupid or mean, or perhaps lacking a proper moral upbringing. With a proper upbringing, they are probably trying to excuse what they know to be their own guilty behavior.

5) We actually face a high risk of framing effects when interpreting particular acts as norm violating. We first learn norms by examples, and then we later apply learned norms to new examples. In both situations the result can depend on the particular examples, their context, and how we framed all this in our minds. If these were the main cognitive processes that produced norm application, then we’d all need to learn from a lot of pretty similar examples in order to reasonably have much confidence that we were all applying the same norms the same way.

6) In a relatively simple world with limited sets of actions and norms, and a small set of people who grew up together and later often enough observe and gossip about possible norm violations of others, such people might in fact learn from enough examples to mostly apply the same norms the same way. This was plausibly the case for most of our distant ancestors. They could in fact mostly be sure that, if they judged themselves as innocent, most everyone else would agree. And if they judged someone else as guilty, others should agree with that as well. Norm application could in fact usually be obvious and automatic.

7) Today however, there are far more people, and more intermixed, who grow up in widely varying contexts and now face far larger spaces of possible actions and action contexts. Relative to this huge space, gossip about particular norm violations is small and fragmented. So it isn’t very plausible that we’ve all converged on how to reliably interpret most norms in most contexts. Thus today we must quite frequently make different judgements on whether actions violate norms. We may converge in judgement with our closest associates and gossip partners, at least on our most common topics of gossip. But for everyone else, if we consider the details of most of their behavior, we will find fault with a lot of it. As they would if they considered the details of our behavior. We are usually sure that we are innocent, but in fact that’s not how many others would categorize us.

8) We must see ourselves as tolerating a <em>lot</em> of norm violation. We actually tell others about and attempt to punish socially only a tiny fraction of the violations that we could know of. When we look most anywhere at behavior details, it must seem to us like we are living in a Sodom and Gomorrah of sin. Compared to the ancient world, it must seem a lot easier to get away for a long time with a lot of norm violations. Selection effects in who chooses to complain about which violations, and which violations others are willing to punish, may seem plausibly to make a big difference to who actually gets punished how much.

We must also see ourselves as tolerating a <em>lot</em> of overeager busybodies applying what they see as norms to what we see as our own private business where their social norms shouldn’t apply. They may not complain out loud about us each time, but we know that they often judge us privately as violating norms, and for no good reason from our point of view. They should just butt out, we think.

9) Random effects of who frames which particular actions as norm violating or not may contribute substantially to who succeeds or fails overall. Some people don’t see a serious violation, and then find themselves punished for what they consider a triviality. They conclude someone had it in for them. Others see a serious potential violation, and pay substantial costs to avoid it, when they in fact faced little risk of punishment. Compared to the ancient world, today larger gains go to those with the [social savvy](hypocrisy-as-key-to-class) to discern what norm violations others can more easily observe and are likely to punish, and the moral flexibility to act on that savvy.
10) Many norms apply only to particular professions, and are mainly intended to protect outsiders from those professionals. For example, norms about how teachers should treat students, or how bankers should treat customers. Strong competition to become a professional can easily select for those with the ambition and social savvy to pretend to follow all such norms, but to only actually follow the norms with sufficient enforcement. Outsiders may then consistently be fooled to mistakenly believe that these professionals follow certain norms, as those outsiders believe that they would naturally follow such norms, if they had been assigned to be such a professional.

In the next posts: [examples](automatic-norm-examples) of all this, and [life lessons](automatic-norm-lessons) to learn from it.

## [Automatic Norm Lessons](#table-of-contents)
_Posted on 2017-12-30_

Pity the modern human who wants to be seen as a consistently good person who almost never breaks the rules. For our distant ancestors, this was a feasible goal. Today, not so much.To paraphrase my recent [post](10-implications-of-automatic-norms):
Our norm-inference process is noisy, and gossip-based convergence isn’t remotely up to the task given our huge diverse population and vast space of possible behaviors. Setting aside our closest associates and gossip partners, if we consider the details of most people’s behavior, we will find rule-breaking fault with a lot of it. As they would if they considered the details of our behavior. We seem to live in a Sodom and Gomorrah of sin, with most people getting away unscathed with most of it. At the same time, we also suffer so many overeager busybodies applying what they see as norms to what we see as our own private business where their social norms shouldn’t apply.

Norm application isn’t remotely as obvious today as our evolved habit of automatic norms assumes. But we can’t simply take more time to think and discuss on the fly, as others will then see us as violating the meta-norm, and infer that we are unprincipled blow-with-the-wind types. The obvious solution: more systematic preparation.

People tend to presume that the point of studying ethics and norms is to follow them more closely. Which is why most people are not interested for themselves, but think it is good for other people. But in fact such study doesn’t have that effect. Instead, there should be big gains to distinguishing which norms to follow more versus less closely. Whether for purely selfish purposes, or for grand purposes of helping the world, study and preparation can help one to better identify the norms that really matter, from the ones that don’t.

In each area of life, you could try to list many possibly relevant norms. For each one, you can try to estimate how it expensive it is to follow, how much the world benefits from such following, and how likely others are to notice and punish violations. Studying norms together with others is especially useful for figuring out how many people are aware of each norm, or consider it important. All this can help you to prioritize norms, and make a plan for which ones to follow how eagerly. And then practice your plan until your new habits become automatic.

As a result, instead of just obeying each random rule that pops into your head in each random situation that you encounter, you can actually only follow the norms that you’ve decided are worth the bother. And if variation in norm following is an big part of variation in success, you may succeed substantially more.

## [Automatic Norms in Academia](#table-of-contents)
_Posted on 2017-12-29_

In my career as a researcher and professor, I’ve come across many decisions where my intuition told me that some actions are prohibited by norms. I’ve usually just obeyed these intuitions, and assumed that everyone agrees. However, I only rarely observe what others think regarding the same situations. In these rare cases, I’m often surprised to see that others don’t agree with me.

I illustrate with the following set of questions on which I’ve noticed divergent opinions. Most academic institutions have no official rules to answer them, nor even an official person to which one can ask. Professors are just supposed to judge for themselves, which they usually do without consulting anyone. And yet many people treat these decisions if they are governed by norms.

<ol>
<li>What excuses are acceptable for students missing an assignment or exam?</li>
<li>If a teacher will be out of town on a class day, must a substitute teacher always be found or can classes sometimes be cancelled? How often can this be done?</li>
<li>Is there any limit on how much extra help or extra credit assignments teachers can offer only to particular students?</li>
<li>Should students be excused for misunderstanding questions due to poor understanding of English?</li>
<li>Is it okay in college to teach students to just remember and then spit back relatively dogmatic statements, instead of trying to teach them how to think about more complex problems?</li>
<li>Is it okay to assign a final exam, but then toss the exams and give out final grades based on all prior assignments?</li>
<li>Is it okay to give all grad students A grades, and to praise all their papers as brilliant, as a way to compete to get students to pick you as their PhD advisor?</li>
<li>Is it okay to lecture while stumbling drunk?</li>
<li>Must you cite the work that actually influenced your work if it is lowbrow like blogs, wikipedia, or working papers, or if it is outside your discipline?</li>
<li>Can you cite prestigious papers that look good in your references if they did not influence your work?</li>
<li>Is it okay to write as if the first work of any consequence on a topic was the first to appear in a top prestige venue, in effect presuming that lower prestige prior work was inadequate?</li>
<li>Should you cite papers requested by journal referees if you don’t think them relevant?</li>
<li>How much searching is okay, searching in theory assumptions or in statistical model specifications, in order to find the kind of result you wanted? Must you disclose such searching?</li>
<li>Is it okay to publish roughly the same idea in several places as long as you don’t use the exact same words?</li>
</ol>
I expect the same holds in most areas of life. Most detailed decisions that people treat as norm-governed have no official rules or judges. Most people decide for themselves without much thought or discussion, assuming incorrectly that relevant norms are obvious enough that everyone else agrees.

## [Plot Holes & Blame Holes](#table-of-contents)
_Posted on 2020-02-22_

We love stories, and the stories we love the most tend to support our cherished norms and morals. But our most popular stories also tend to have many gaping <a href="https://www.digitalspy.com/movies/a803303/the-10-biggest-movie-plot-holes-of-all-time/">plot holes</a>. These are acts which characters could have done instead of what they did do, to better achieve their goals. Not all such holes undermine the morals of these stories, but many do.

Logically, learning of a plot hole that undermines a story’s key morals should make us like that story less. And for a hole that most everyone actually sees, that would in fact happen. This also tends to happen when we notice plot holes in obscure unpopular stories.

But this happens much less often for widely beloved stories, such as <em>Star Wars</em>, if only a small fraction of fans are aware of the holes. While the popularity of the story should make it easier to tell most fans about holes, fans in fact try not to hear, and punish those who tell them. (I’ve noticed this re my sf reviews; fans are displeased to hear beloved stories don’t make sense.)

So most fans remain ignorant of holes, and even fans who know mostly remain fans. They simply forget about the holes, or tell themselves that there probably exist easy hole fixes – variations on the story that lack the holes yet support the same norms and morals. Of course such fans don’t usually actually search for such fixes, they just presume they exist.

Note how this behavior contrasts with typical reactions to real world plans. Consider when someone points out a flaw in our tentative plan for how to drive from A to B, how to get food for dinner, how to remodel the bathroom, or how to apply for a job. If the flaw seems likely to make our plan fail, we seek alternate plans, and are typically grateful to those who point out the flaw. At least if they point out flaws privately, and we haven’t made a big public commitment to plans.

Yes, we might continue with our basic plan if we had good reasons to think that modest plan variations could fix the found flaws. But we wouldn’t simply presume that such variations exist, regardless of flaws. Yet this is mostly what we do for popular story plot holes. Why the different treatment?

A plausible explanation is that we like to love the <em>same</em> stories as others; loving stories is a coordination game. Which is why 34% of movie budgets were <a href="https://entertainment.howstuffworks.com/movie-cost1.htm">spent</a> on marketing in ’07, compared to 1% <a href="https://www.economist.com/business/2018/01/18/something-doesnt-ad-up-about-americas-advertising-market">for</a> the average product. As long as we don’t expect a plot hole to put off most fans, we don’t let it put us off either. And a plausible partial reason to coordinate to love the same stories is that we use stories to declare our allegiance to shared norms and morals. By loving the same stories, we together reaffirm our shared support for such morals, as well as other shared cultural elements.

Now, another way we show our allegiance to shared norms and morals is when we blame each other. We accuse someone of being blameworthy when their behavior fits a shared blame template. Well, unless that person is so allied to us or prestigious that blaming them would come back to hurt us.

These blame templates tend to correlate with destructive behavior that makes for a worse (local) world overall. For example, we blame murder and murder tends to be destructive. But blame templates are not exactly and precisely targeted at making better outcomes. For example, murderers are blamed even when their act makes a better world overall, and we also fail to blame those who fail to murder in such situations.

These deviations make sense if blame templates must have limited complexity, due to being socially shared. To support shared norms and morals, blame templates must be simple enough so most everyone knows what they are, and can agree on if they match particular cases. If the reality of which behaviors are actually helpful versus destructive is more complex than that, well then good behavior in some detailed “hole” cases must be sacrificed, to allow functioning norms/morals.

These deviations between what blame templates actually target, and what they should target to make a better (local) world, can be seen as “blame holes”. Just as a plot may seem to make sense on a quick first pass, with thought and attention required to notice its holes, blame holes are typically not noticed by most who only work hard enough to try to see if a particular behavior fits a blame template. While many are capable of understanding an explanation of where such holes lie, they are not eager to hear about them, and they still usually apply hole-plagued blame templates even when they see their holes. Just like they don’t like to hear about plot holes in their favorite stories, and don’t let such holes keep them from loving those stories.

For example, a year ago I <a href="https://twitter.com/robinhanson/status/1071407325242626048">asked</a> a Twitter poll on the chances that the world would have been better off overall had Nazis won WWII. 44% said that chance was over 10% (the highest category offered). My point was that history is too uncertain to be very sure of the long term aggregate consequences of such big events, even when we are relatively sure about which acts tend to promote good.

Many then said I was evil, apparently seeing me as fitting the blame template of “says something positive about Nazis, or enables/encourages others to do so.” I soon after <a href="https://twitter.com/robinhanson/status/1078654012302471168">asked</a> a poll that found only 20% guessing it was more likely than not that the author of such a poll actually wishes Nazis had won WWII. But the other 80% might still feel justified in loudly blaming me, if they saw my behavior as fitting a widely accepted blame template. I could be blamed regardless of the factual truth of what I said or intended.

Recently many called Richard Dawkins evil for apparently fitting the template “says something positive about eugenics” when he <a href="https://twitter.com/RichardDawkins/status/1228943686953664512">said</a> that eugenics on humans would “work in practice” because “it works for cows, horses, pigs, dogs & roses”. To many, he was blameworthy regardless of the factual nature or truth of his statement. Yes, we might do better to instead use the blame template “endorses eugenics”, but perhaps too few are capable in practice of distinguishing “endorses” from “says something positive about”. At least maybe most can’t reliably do that in their usual gossip mode of quickly reading and judging something someone said.

On reflection, I think a great deal of our inefficient behavior and policies can be explained via limited-complexity blame templates. For example, consider the template:

> Blame X if X interacts with Y on dimension D, Y suffers on D, no one should suffer on D, and X “could have” interacted so as to reduce that suffering more.

So, blame X who hires Y for a low wage, risky, or unpleasant job. Blame X who rents a high price or peeling paint room to Y. Blame food cart X that sells unsavory or unsafe food to Y. Blame nation X that lets in immigrant Y who stays poor afterward. Blame emergency room X who failed to help arriving penniless sick Y. Blame drug dealer X who sells drugs to poor, sick, or addicted Y. Blame client X who buys sex, an organ, or a child from Y who would not sell it if they were much richer.

So a simple blame template can help explain laws on min wages, max rents, job & room quality regs, food quality rules, hospital care rules, and laws prohibiting drugs, organ sales, and prostitution. Yes, by learning simple economics many are capable of seeing that these rules can actually make targets Y worse off, via limiting their options. But if they don’t expect others to see this, they still tend to apply the usual blame templates. Because blame templates are socially shared, and we each tend to be punished from deviating from them, either by violating them, or failing to disapprove of violators.

In another post soon I hope to say more about the role of, and limits on, simplified blame templates. For this post, I’m content to just note their central causal roles.

<strong>Added 8am:</strong> Another key blame template happens in hierarchical organizations. When something bad seems to happen to a division, the current leader takes all the blame, even if recently replaced prior leader. Rising stars gain by pushing short term gains at the expense of long term losses, and being promoted fast enough so as not to be blamed for those losses.

Re my deliberate exposure [proposal](deliberate-exposure-intuition), many endorse a norm that those who propose policies intended to combine good and bad effects should immediately cause themselves to suffer the worst possible bad effects personally, even in the absence of implementing their proposal. Poll majorities, <a href="https://twitter.com/robinhanson/status/1228727166118088705">however</a>, don’t support such norms.

## [Fairy Tales Were Cynical](#table-of-contents)
_Posted on 2012-08-25_

A recent <em>New Yorker</em> <a href="http://www.newyorker.com/arts/critics/books/2012/07/23/120723crbo_books_acocella">article</a> on fairy tales fascinated me (quotes below). Apparently the fairy tales once “told at rural firesides” were for adults, full of sex and violence, and cynical – they did not often affirm common ideals. This stands in sharp contrast to most fiction genres today, especially today’s fairy tales targeted at kids. Why were long ago stories so much more cynical? They remind me of some joke genres, like <a href="http://dead-baby-joke.com/">dead baby jokes</a>, and of the crudeness often found off the record in many close social groups.

Here’s my homo hypocritus explanation. Our forager ancestors evolved intricate capacities to affirm standard ideals when what they said or did might be visible or reported to distant observers, and to coordinate to violate such ideals when they were less visible. Shared private rejection and violation of wider ideals can signal close bonds with associates, and reveal more about ourselves to intimates.

So when stories become more visible, such as by getting published in books, stories had to become more ideal. Similarly, when kids were taught in schools, with a curriculum visible to all, that curriculum had to become more ideal. And as law enforcement has become more visible, it has been held to higher standards.

Today harassment laws make it harder to be very crude and cynical at work, and divorce custody battles punish parents who act this way around their kids. Today, more interactions are governed by officially idealistic norms: teachers around students, doctors & lawyers around clients, etc. What costs do we pay for this panopticon-like suppression of our natural crude/cynical styles? We are probably less able to form very close social groups where we can more clearly see each others’ weaknesses and vulnerabilities. But what else?

<strong>Added 26Aug</strong>: Another contributing factor may be that in general our idealism just rises with rising wealth.

Those promised quotes:<span id="more-29931"></span>

In Grimms’ Fairy Tales there is a story called “The Stubborn Child” that is only one paragraph long. …

Once upon a time there was a stubborn child who never did what his mother told him to do. The dear Lord, therefore, did not look kindly upon him, and let him become sick. No doctor could cure him and in a short time he lay on his deathbed. After he was lowered into his grave and covered over with earth, one of his little arms suddenly emerged and reached up into the air. They pushed it back down and covered the earth with fresh earth, but that did not help. The little arm kept popping out. So the child’s mother had to go to the grave herself and smack the little arm with a switch. After she had done that, the arm withdrew, and then, for the first time, the child had peace beneath the earth.

The tale, without details to attach it to anything in particular, becomes universal. Whatever happened there, we all deserve it. A. S. Byatt has written that this is the real terror of the story: “It doesn’t feel like a warning to naughty infants. It feels like a glimpse of the dreadful side of the nature of things.” That is true of very many of the Grimms’ tales, even those with happy endings. …

The Grimms grew up in the febrile atmosphere of German Romanticism, which involved intense nationalism and, in support of that, a fascination with the supposedly deep, pre-rational culture of the German peasantry, the Volk. … They had political reasons, too—above all, Napoleon’s invasion of their beloved Hesse. …

The Grimms … first edition was not intended for the young, nor, apparently, were the tales told at rural firesides. The purpose was to entertain grownups, during or after a hard day’s work, and rough material was part of the entertainment. But the reviews and the sales of the Grimms’ first edition were disappointing to them. Other collections, geared to children, had been more successful, and the brothers decided that their second edition would take that route. … What they regarded as unsuitable for the young was information about sex. In the first edition, Rapunzel, imprisoned in the tower by her wicked godmother, goes to the window every evening and lets down her long hair so that the prince can climb up and enjoy her company. …

Grimm tales, many of which feature mutilation, dismemberment, and cannibalism, not to speak of ordinary homicide, often inflicted on children by their parents or guardians. … You get used to the outrages, though. They may even come to seem funny. … Some stories do tear you apart, usually those where the violence is joined to some emphatically opposite quality, such as peace or tenderness. … The stories are still extremely short. … They come in, clobber you over the head, and then go away. As with sections of the Bible, the conciseness makes them seem more profound. … W. H. Auden once described the Grimm-sanitizers as “the Society for the Scientific Diet, the Association of Positivist Parents, the League for the Promotion of Worthwhile Leisure, the Cooperative Camp of Prudent Progressives.” …

Marina Warner … says that most modern writers ignore the Grimms’ “historical realism.” Among the pre-modern populations, she records, death in childbirth was the most common cause of female mortality. …

The Grimm tales are no different from other art. They merely concretize and then expand our experience of life. The main reason that Zipes likes fairy tales, it seems, is that they provide hope: they tell us that we can create a more just world. The reason that most people value fairy tales, I would say, is that they do not detain us with hope but simply validate what is. Even people who have never known hunger, let alone a murderous stepmother, still have a sense—from dreams, from books, from news broadcasts—of utter blackness, the erasure of safety and comfort and trust. Fairy tales tell us that such knowledge, or fear, is not fantastic but realistic. (<a href="http://www.newyorker.com/arts/critics/books/2012/07/23/120723crbo_books_acocella">more</a>)

## [Why Fiction Lies](#table-of-contents)
_Posted on 2009-01-05_

Most religious activities make a lot of sense, especially in terms of group bonding.  It is religious beliefs that seem the most puzzling.  Many suggest supernatural beliefs are just a side effect of our having a theory of mind, and applying it liberally.  Back in 2001 I read and <a href="http://hanson.gmu.edu/religion.html">reviewed</a> Pascal Boyer’s book <em>Religion Explained</em>.  Boyer noted 1) supernatural concepts tend to violate one ontological assumption each, making them maximally memorable, and 2) supernatural entities tend to know and care about human-socially-relevant info, and to punish humans who are not nice (i.e., cooperative).  I was puzzled that Boyer didn’t explicitly make what seemed to me the obvious suggestion:  we evolved a tendency to accept strange memorable group beliefs to create a high cost of leaving our group, and to show that we expect to be punished if we are not nice.

Our obsession with gossiping about each other makes a lot of sense, but more puzzling is our obsession with stories we know are not true, about unrelated people in strange worlds.  I recently finished literary-expert William Flesch’s <em>Comeuppance</em>, a literary expert’s evo psych account of why we like fiction (reviewed <a href="http://www.epjournal.net/filestore/EP06502505.pdf">here </a>and <a href="http://papercuts.blogs.nytimes.com/2008/02/16/how-would-darwin-read/">here</a>).  Flesch says humans cooperate via a norm of celebrating cooperators and punishing defectors <em>and </em>those who violate this norm:

> In narratives we … [are] disposed to want to see the cooperators triumph over the obstacles set up by defectors of various sorts.  …. [We] root for characters with a propensity for strong reciprocity, not because the judge them as like us or identify with them, but because  a disposition to reward cooperators and to punish defectors is itself a central aspect of cooperation. (p.126)

Social life is all about signaling our abilities and cooperativeness, and discerning such signals from others:

<span id="more-16774"></span>

> Understanding narrative at all requires understanding of signaling.  We monitor signals and the reliability of signals that others produce.  We take note of how others monitor signals, and what signals they produce in turn on receive signals that we also may receive.  One of the intricate pleasures of narrative … consists in keeping track of who knows what. We like to keep track of what other people are keeping track of. … Narrative relies on the psychological incentives to engage in such monitoring of how we respond to what we know about one another. (p.85)

Yes, we love to watch, and watching abilities serve us well, by why do we apply them so enthusiastically to false stories?  Why not just tell stories about real heros and villains?  One clue is that stories can signal things about authors and tellers:

> Among the strong reciprocators to narrative events are the narrators of those events. … Gossip is a likely mode of altruistic punishment: the scandal monger punishes scandalous behavior. … Gossip … disciplines those who have violated whatever norms the gossipers are punishing.

But how is it altruistic to punish non-existent violators?  Only once does Flesch get close to the key: visibly consuming stories also signals things!

> Vicarious feelings for others is therefore both a propensity for responding emotionally to the signals of others and itself a primary example of such a signal. … Our own monitoring of costly signals and our response to the response of others constitute our own costly and altruistic absorption in the interactions of others.  And of course we signal as well with the stories we love, a mode of signaling that can range from the simple desire to repeat them to the social capital of our own conspicuous cultural attainments.  Knowing a story and, still more, telling a story signals our own capacities for altruistic interest, affect, and punishment, capacities that the story will represent its characters manifesting in order to appeal to the audiences interest in monitoring these things.  (pp 123-124)

This explanation of fiction comes close to the above explanation of religious beliefs: both religion and fiction serve to reassure our associates that we will be nice.  In addition to letting us show we can do hard things, and that we are tied to associates by doing the same things, religious beliefs show we expect the not nice to be punished by supernatural powers, and our favorite fiction shows the sort of people we think are heroes and villains, how often they are revealed or get their due reward, and so on.

We don’t believe the stories really happened, but we do tend to believe these “social truths” about their characters. We love to tell associates about our favorite stories, and prefer them to love them too.

As with religion, the beliefs of ours that most reassure others are not necessarily the most accurate.  In fiction, relative to reality, people know more why they act and what they want, good and bad personal characteristics correlate more strongly, personal character matters more relative to circumstance or larger social forces, and there are clearer ultimate resolutions to complex events.  What other social lies does fiction tell, and why does it reassure others that we believe them?

## [Biases Of Fiction](#table-of-contents)
_Posted on 2012-12-05_

<a href="http://www.scribd.com/doc/7391279/38-Most-Common-Fiction-Writing-Mistakes">This essay</a>, on “The 38 most common fiction writing mistakes”, offers advice to writers. But the rest of us can also learn useful details on how fiction can bias our thinking. Here are my summary of key ways it says fiction differs from reality (detailed quotes below):

Features of fictional folk are more extreme than in reality; real folks are boring by comparison. Fictional folks are more expressive, and give off clearer signs about their feelings and intentions. Their motives are simpler and clearer, and their actions are better explained by their motives and local visible context. Who they are now is better predicted by their history. Compared to real people, they are more likely to fight for what they want, especially when they encounter resistance. Their conversations are mostly pairwise, more logical, and to the point. In fiction, events are determined more by motives and plans, relative to random chance and larger social forces. Overt conflict between people is more common than in real life.

And I’ll add that stories [tend](stories-are-like-religion) to affirm standard moral norms. Good guys, who do good acts, have more other virtuous features than in reality, and and good acts are rewarded more often than in reality.
A lot of our biases come, I think, from expecting real life to be like fiction. For example, when we have negative opinions on important subjects, we tend too much to expect that we should explicitly and directly express those negative opinions in a dramatic conversation scene. We should speak our mind, make it clear, talk it through, etc. This usually a bad idea. We also tend to feel bad about ourselves when we notice that we avoid confrontation, and back off when from things we want when we encounter resistance. But such retreat is usually for the best.

Those promised quotes:<span id="more-30125"></span>

In more than twenty years of teaching courses in professional writing at the University of Oklahoma, I think I’ve encountered almost every difficulty an aspiring writer might face. …

“Wally, these characters are dull. What they are is flat and insipid. They are pasteboard. They have no life, no color, no vivacity. They need a lot of work. ”<br/>
Wally looked shocked. “How can these characters be dull? They’re real people-every one of them! I took them right out of real life!”<br/>
“Oh”, I said. “So that’s the problem. ”<br/>
“What?” he said.<br/>
“You can never use real people in your story. ”<br/>
“Why?”<br/>
“For one reason, real people might sue you. But far more to the point in fiction copy, real people – taken straight over and put on the page of a story – are dull. ” …

Good fiction characters, in other words, are never, ever real people. Your idea for a character may begin with a real person, but to make him vivid enough for your readers to believe in him, you have to exaggerate tremendously; you have to provide shortcut identifying characteristics that stick out all over him, you have to make him practically a monster-for readers to see even his dimmest outlines.

For example, if your real person is loyal, you will make your character tremendously, almost unbelievably loyal; if he tends to be a bit impatient in real life, your character will fidget, gnash his teeth, drum his fingers, interrupt others, twitch, and practically blow sky high with his outlandishly exaggerated impatience….

Good fiction characters also tend to be more understandable than real-life people. They do the things they do for motives that make more sense than real-life motives often do. While they’re more mercurial and colorful, they’re also more goal-motivated. Readers must be able to understand why your character does what he does; they may not agree with his motives, but you have carefully set things up so at least they can see that he’s acting as he is for some good reason. …

In real life, a young woman may come out of a poverty-stricken rural background and still somehow become the president of a great university. Except in a long novel, where you might have sufficient space to make it believable, you would have a hard time selling this meshing of background and present reality in fiction. … In short fiction, characters and their backgrounds are almost always much more consistent than people in real life.

Motivation? Again, fictional characters are better than life. In real life, people often seem to do things for no reason we can understand. They act on impulses that grow out of things in their personalities that even they sometimes don’t understand. But in fiction there is considerably less random chance. … in real life people often don’t make sense. But in fiction, they do. …

interesting characters are almost always characters who are active-risk-takers – highly motivated toward a goal. Many a story has been wrecked at the outset because the writer chose to write about the wrong kind of person -a character of the type we sometimes call a wimp. … He’s the one who wouldn’t fight under any circumstances.<br/>
Ask him what he wants, and he just sighs. Poke him, and he flinches-and retreats. Confront him with a big problem, and he fumes and fusses and can’t make a decision. …

In reality-in the real world -much of what happens is accidental. … In most effective fiction, accidents don’t determine the outcome. And your story people don’t sit around passively. … In good fiction, the story people determine the outcome. Not fate.<br/>
…

In fiction, the best times for the writer- and reader- are when the story’s main character is in the worst trouble. … There are many kinds of fiction trouble, but the most effective kind is conflict. You know what conflict is. It’s active give-and-take, a struggle between story people with opposing goals. … The calmer and more peaceful your real life, the better, in all likelihood. Your story person’s life is just the opposite. You the author must never duck trouble … Because fiction is make-believe, it has to be more logical than real life if it is to be believed. In real life, things may occur for no apparent reason. But in fiction you the writer simply cannot ever afford to lose sight of logic and let things happen for no apparent reason. …

In real life, coincidence happens all the time. But in fiction – especially when the coincidence helps the character be at the right place at the right time, or overhear the crucial telephone conversation, or something similar -coincidence is deadly. Your readers will refuse to believe it. …

Your character must have an immediate, physical cause for what he does. This immediate stimulus cannot be merely a thought inside his head; for readers to believe many transactions, they have to be shown a stimulus to action that is outside of the character-some kind of specific prod that is onstage right now. Turning this around, it’s equally true that if you start by showing a stimulus, then you can’t simply ignore it; you must show a response. … In real life, you might get a random thought for no apparent reason, and as a consequence do or say something. But … fiction has to be better than life, clearer and more logical. …

Writers sometimes mess up their dialogue. Sometimes, without realizing it, they let their characters talk on and on, boringly, becoming windbags. … The great majority of your characters have to be more terse and logical than we often are in real life, if the dialogue on the page is to appear realistic. … whenever possible, set up your dialogue scenes so that they play out “one-on-one”, getting rid of other characters (who might interrupt and make the conversation more complicated). … Simplicity… directness… goal orientation… brevity. These are the hallmarks of modern story dialogue. …

If you have any doubt that the reader will understand the meaning of what someone in the story says or does, you must work in at once some method of pointing out what you may think is obvious. (<a href="http://www.scribd.com/doc/7391279/38-Most-Common-Fiction-Writing-Mistakes">more</a>; HT Eliezer Yudkowsky)

## [Why We Fight Over Fiction](#table-of-contents)
_Posted on 2020-11-29_

We tell stories with language, and so prefer to tell the kind of stories that ordinary language can describe well.

Consider how language can describe a space of physical stuff and how to navigate through that stuff. In a familiar sort of space, a few sparse words can evoke a vivid description, such as of a city street or a meadow. And a few words relating to landmarks in such a space can be effective at telling you how to navigate from one place to another.

But imagine an arbitrary space of partially-opaque swirling strangeness, in a highly curved 11-dimensional space. In principle our most basic and general spatial language could describe this too, and instruct navigation there. But in practice that would require a <em>lot</em> more words, and slow the story to a crawl. So few authors would try, though a filmmaker might try just using visuals.

Or consider stories with non-human minds. In principle those who study minds in the abstract can conceive of a vast space of possible minds, and can use a basic and general language of mental acts to describe how each such mind might make a decision, or send a communication, and what those might be. But in practice such descriptions would be long, boring, and unfamiliar to most readers.

So in practice even authors writing about aliens or AIs stick to describing human-like minds, where their usual language for describing what actors decide and say is fast, fluid, and relatable. Authors even prefer human characters with familiar minds, and so avoid characters who think oddly, such as those with autism.

Just as authors focus on telling stories in familiar spaces with familiar minds, they also focus on telling stories in familiar moral universes. This effect is, if anything, even stronger than the space and mind effects, as moral colors are even more central to our need for stories. Compared to other areas of our lives, we especially want our stories to help us examine and affirm our moral stances.

In a familiar moral universe, there many be competing considerations re what acts are moral, making it sometimes hard to decide if an act is moral. Other considerations may weigh against morality, and reader/viewers may not always sympathize most with the most moral characters, who may not win in the end. Moral characters may have unattractive features (like being ugly). There may even be conflicts between characters who see different familiar moral universes.

These are the familiar sorts of “moral ambiguity” in stories said to have that feature, such as <em>The Sopranos</em> or <em>Game of Thrones</em>. But you’ll note that these are almost all stories told in familiar moral universes. By which I mean that we are quite familiar with how to morally evaluate the sort of actions that happen there. The set of acts is familiar, as are their consequences, and the moral calculus used to judge them.

But there is another sort of “moral ambiguity” that reader/viewers hate, and so authors studiously avoid. And that is worlds where we find it hard to judge the morality of actions, even when those actions have big consequences for characters. Where our usual quick and dirty moral language doesn’t apply very well. Where even though in principle our most basic and general moral languages might be able to work out rough descriptions and evaluations, in practice that would be tedious and unsatisfying.

And, strikingly, the large complex social structures and organizations that dominate our world are mostly <em>not</em> familiar moral universes to most of us. For example, big firms, agencies, and markets. The worlds of [Moral Mazes](how-idealists-aid-cheaters) and of [Pfeffer’s](advice-isnt-about-info) [Power](leadership-fantasies). (In fiction: <a href="https://www.metacritic.com/movie/jobs"><em>Jobs</em></a>.) Our stories thus tend to avoid such contexts, unless they happen to allow an especially clear moral calculus. Such as a firm polluting to cause cancer, or a boss sexually harassing a subordinate.
As I’ve [discussed](missing-work-stories) [before](industry-era-action-stories), our social world has changed greatly over the last few centuries. Our language has changed fast enough to describe the new physical objects and spaces that have arisen, at least those with which ordinary people must deal, if not the many new strange objects and spaces behind the scenes that enable our new world. But we have not [gone](spaceship-earth-in-culture-space) remotely as fast at coming to agree on moral stances toward the new choices possible in such social structures.
This is why our stories tend to take place in relatively old fashioned social worlds. Consider the popularity of the Western, or of pop science fiction stories like <em>Star Wars</em> that are essentially Westerns with more gadgets. Stories that take place in modern settings tend to focus on personal, romantic, and family relations, as these remain to us relatively familiar moral universes. Or on artist biopics. Or on big conflicts like war or corrupt police or politicians. For which we have comfortable moral framings.

Stories we write today set in say the 1920s feel to us more comfortable than do stories set in the 2020s, or than stories written in the 1920s and set in that time. That is because stories written today can inherit a century of efforts to work out clearer moral stances on which 1920s actions would be more moral. For example, as to our eyes female suffrage is clearly good, we can see any characters from then who doubted it as clearly evil in the eyes of good characters. As clear as if they tortured kittens. To our eyes, their world has now clearer moral colors, and stories set there work better as stories for us.

This is also why science fiction tends to make most people more wary of anticipated futures. The easiest engaging stories to tell about strange futures are on how acts there that seem to violate the rules in our current moral universe. Like about how nuclear rockets spread radioactivity near their launch site, instead of the solar civilization they enable. Much harder to describe how new worlds will induce new moral universes.

This highlights an important feature of our modern world, and an important process that continues within it. Our social world has changed a lot faster than has our shared moral evaluations of typical actions possible in our new world. And our telling stories, and coming to agree on which stories we embrace, is a big part of creating such a fluid language of shared moral evaluations.

This helps to explain why we invest so much time and energy into fiction, far more than did any of our ancestors. Why story tellers are given high and activist-like status, and why we fight so much to convince others to share our beliefs on which stories are best. Our moral evaluations of the main big actions that influence our world today, and that built our world from past worlds, are still up for grabs. And the more we build such shared evaluations, the more we’ll be able to tell satisfying stories set in the world in which we live, rather than set in the fantasy and historical worlds with which we must now make do.

(This post is an elaboration of <a href="https://twitter.com/robinhanson/status/1332846381783011333">this</a> Twitter thread.)

## [Stories Are Like Religion](#table-of-contents)
_Posted on 2012-05-08_

Small children (age 4-6) who were exposed to a large number of children's books and films had a significantly stronger ability to read the mental and emotional states of other people. ... The more absorbed subjects were in the story, the more empathy they felt, and the more empathy they felt, the more likely the subjects were to help when the experimenter "accidentally" dropped a handful of pens… Reading narrative fiction … fosters empathic growth and prosocial behavior. …

Fiction’s happy endings seem to warp our sense of reality. They make us believe in a lie: that the world is more just than it actually is. But believing that lie has important effects for society—and it may even help explain why humans tell stories in the first place. (<a href="http://www.bakadesuyo.com/does-tv-increase-empathy">more</a>)

People who mainly watched drama and comedy on TV—as opposed to heavy viewers of news programs and documentaries—had substantially stronger “just-world” beliefs. … Fiction, by constantly exposing us to the theme of poetic justice, may be partly responsible for the sense that the world is, on the whole, a just place. (<a href="http://www.bakadesuyo.com/do-stories-rule-our-lives-would-that-be-good">more</a>)

Psychologists have found that people who watch less TV are actually more accurate judges of life’s risks and rewards than those who subject themselves to the tales of crime, tragedy, and death that appear night after night on the ten o’clock news. That’s because these people are less likely to see sensationalized or one-sided sources of information, and thus see reality more clearly. (<a href="http://www.bakadesuyo.com/how-can-you-improve-your-ability-to-judge-lif">more</a>)

Imagine that all you know about someone is that they have zero interest in stories. Not movies, not novels, not nothing. They prefer instead to stay focused on the real world. The only “stories” they want are accurate histories of representative people. What do you think of this person?

You might want to hire this person. But would you trust them to be loyal? Would you date them? Marry them? Most people feel a little wary of such story-less people, just as they are wary of atheists. People fear that atheists will violate social norms because they do not fear punishment from gods and spirits. Similarly, people fear that story-less people have not internalized social norms well – they may be too aware of how easy it would be to get away with violations, and feel too little shame from trying.

Thus in equilibrium, people are encouraged to consume stories, and to deludedly believe in a more just world, in order to be liked more by others. This is similar to how people have long been encouraged to be religious, so that they could similarly be liked more by others.

A few days ago I [asked](what-use-far-truth) why not become religious, if it will give you a better life, even if the evidence for religious beliefs is weak? Commenters eagerly declared their love of truth. Today I’ll ask: if you give up the benefits of religion, because you love far truth, why not also give up stories, to gain even more far truth? Alas, I expect that few who claim to give up religion because they love truth will also give up stories for the same reason. Why?
One obvious explanation: many of you live in subcultures where being religious is low status, but loving stories is high status. Maybe you care a lot less about far truth than you do about status.

## [More Stories As Religion](#table-of-contents)
_Posted on 2014-07-14_

Most people who say they are atheist or agnostic still believe in supernatural powers:

In the United States, 38% of people who identified themselves as atheist or agnostic went on to claim to believe in a God or a Higher Power. While the UK is often defined as an irreligious place, a recent survey … found that … only 13 per cent of adults agreed with the statement “humans are purely material beings with no spiritual element”. …

When researchers asked people whether they had taken part in esoteric spiritual practices such as having a Reiki session or having their aura read, the results were almost identical (between 38 and 40%) for people who defined themselves as religious, non-religious or atheist.

This is plausibly reinforced by fiction, which (as I’ve [said](stories-are-like-religion)) serves similar functions to religion:
In almost all fictional worlds, God exists, whether the stories are written by people of a religious, atheist or indeterminate beliefs.

It’s not that a deity appears directly in tales. It is that the fundamental basis of stories appears to be the link between the moral decisions made by the protagonists and the same characters’ ultimate destiny. The payback is always appropriate to the choices made. An unnamed, unidentified mechanism ensures that this is so, and is a fundamental element of stories—perhaps the fundamental element of narratives.

In children’s stories, this can be very simple: the good guys win, the bad guys lose. In narratives for older readers, the ending is more complex, with some lose ends left dangling, and others ambiguous. Yet the ultimate appropriateness of the ending is rarely in doubt. If a tale ended with Harry Potter being tortured to death and the Dursley family dancing on his grave, the audience would be horrified, of course, but also puzzled: that’s not what happens in stories. Similarly, in a tragedy, we would be surprised if King Lear’s cruelty to Cordelia did not lead to his demise.

Indeed, it appears that stories exist to establish that there exists a mechanism or a person—cosmic destiny, karma, God, fate, Mother Nature—to make sure the right thing happens to the right person. Without this overarching moral mechanism, narratives become records of unrelated arbitrary events, and lose much of their entertainment value. In contrast, the stories which become universally popular appear to be carefully composed records of cosmic justice at work.

In manuals for writers (see “Screenplay” by Syd Field, for example) this process is often defined in some detail. Would-be screenwriters are taught that during the build-up of the story, the villain can sin (take unfair advantages) to his or her heart’s content without punishment, but the heroic protagonist must be karmically punished for even the slightest deviation from the path of moral rectitude. The hero does eventually win the fight, not by being bigger or stronger, but because of the choices he makes.

This process is so well-established in narrative creation that the literati have even created a specific category for the minority of tales which fail to follow this pattern. They are known as “bleak” narratives. An example is A Fine Balance, by Rohinton Mistry, in which the likable central characters suffer terrible fates while the horrible faceless villains triumph entirely unmolested.

While some bleak stories are well-received by critics, they rarely win mass popularity among readers or moviegoers. Stories without the appropriate outcome mechanism feel incomplete. The purveyor of cosmic justice is not just a cast member, but appears to be the hidden heart of the show. (<a href="http://www.science20.com/writer_on_the_edge/blog/scientists_discover_that_atheists_might_not_exist_and_thats_not_a_joke-139982">more</a>)

## [This is the Dream Time](#table-of-contents)
_Posted on 2009-09-28_

<em>Aboriginals believe in … [a] “dreamtime”, more real than reality itself. Whatever happens in the dreamtime establishes the values, symbols, and laws of Aboriginal society. … [It] is also often used to refer to an individual’s or group’s set of beliefs or spirituality. … It is a complex network of knowledge, faith, and practices that derive from stories of creation. </em><em> </em> <a href="http://en.wikipedia.org/wiki/Dreamtime">Wikipedia</a>.

We will soon [enter an era](how-is-our-era-unique) where most anyone can at any time talk directly with most anyone else who can talk.  Cheap global talk and travel continue to tie our global economy and culture more closely together.  But in the distant future, our descendants will probably have spread out across space, and redesigned their minds and bodies to explode Cambrian-style into a vast space of possible creatures. If they are free enough to choose where to go and what to become, our distant descendants will fragment into diverse local economies and cultures.
Given a similar freedom of fertility, most of our distant descendants will also live near a subsistence level.  Per-capita wealth has only been rising lately because income has grown faster than population.  But if income only doubled every century, in a million years that would be a factor of 10<sup>3000</sup>, [which seems](limits-to-growth) impossible to achieve with only the 10<sup>70</sup> atoms of our galaxy available by then.  Yes we have seen a remarkable demographic transition, wherein richer nations have fewer kids, but we already see contrarian subgroups like Hutterites, Hmongs, or Mormons that grow much faster.  So unless strong central controls prevent it, over the long run such groups [will easily grow](future-fertility) faster than the economy, making per person income drop to near subsistence levels.  Even so, they will be [basically happy](poor-folks-do-smile) in such a world.
Our distant descendants will also likely have hit diminishing returns to discovery; by then most everything worth knowing will be known by many; truly new and important discoveries will be quite rare. Complete introspection will be feasible, and immortality will be available to the few who can afford it.  Wild nature will be mostly gone, and universal coordination and destruction will both be far harder than today.

So what will these distant descendants think of their ancestors?  They will find much in common with our distant hunting ancestors, who also continued for ages at near subsistence level in a vast fragmented world with slow growth amid rare slow contact with strange distant cultures.  While those ancestors were quite ignorant about their world, and immersed in a vast wild nature instead of a vast space of people, their behavior was still pretty well adapted to the world they lived in.  While they suffered many misconceptions, those illusions rarely made them much worse off; their behavior was usually adaptive.

When our distant descendants think about our era, however, differences will loom larger.  Yes they will see that we were more like them in knowing more things, and in having less contact with a wild nature.  But our brief period of very rapid growth and discovery and our globally integrated economy and culture will be quite foreign to them.  Yet even these differences will pale relative to one huge difference: our lives are far more dominated by consequential delusions: wildly false beliefs and non-adaptive values that matter.  While our descendants may explore delusion-dominated virtual realities, they will well understand that such things cannot be real, and don’t much influence history.  In contrast, we live in the brief but important “dreamtime” when delusions drove history.  Our descendants will remember our era as the one where the human capacity to sincerely believe crazy non-adaptive things, and act on those beliefs, was dialed to the max.

Why is our era so delusory?<span id="more-19914"></span>

<ol>
<li>Our knowledge has been growing so fast, and bringing such radical changes, that many of us see anything as possible, so that nothing can really be labeled delusion.</li>
<li>Rich folks like us have larger buffers of wealth to cushion our mistakes; we can live happily and long even while acting on crazy beliefs.</li>
<li>We humans evolved to signal various features of ourselves to one another via delusions; we usually think that the various things we do to signal are done [for other reasons](politics-isnt-a).  For example, we think we pay for docs to help our loved ones get well, rather than to [show that we care](showing-that-yo).  We think we do politics because we want to help our nation, rather than to signal our character and loyalty.  We are overconfident in our abilities in order to convince others to have confidence in us, and so on.  But while our ancestors’ delusions were well adapted to their situations, and so didn’t hurt them much, the same delusions are not nearly as adapted to our rapidly changing world; our signaling induced delusions hurt us more.</li>
<li>Humans seem to have evolved to emphasize signaling more in good times than in bad.  Since very few physical investments last very long, the main investments one can make in good times that last until bad times are allies and reputation.  So we are built to, in good times, spend more time and energy on leisure, medicine, charity, morals, patriotism, and so on.  Relative to our ancestors’ world, our whole era is one big very good time.</li>
<li>Our minds were built with a near mode designed more for practical concrete reasoning about things up close, and a far mode [designed more](a-tale-of-two-tradeoffs) for presenting a good image to others via our abstract reasoning about things far away.  But our minds must now deal with a much larger world where many relevant things are much further away, and abstract reasoning is more useful.  So we rely more than did our ancestors on that abstract far mode capability.  But since that far mode was tuned more for presenting a good image, it is much more tolerant of good-looking delusions.</li>
<li>Tech now enables more exposure to mood-altering drugs and arts, and specialists make them into especially potent “<a href="http://lesswrong.com/lw/h3/superstimuli_and_the_collapse_of_western/">super-stimuli</a>.” Our ancestors used drugs and went into art appreciation mode rarely, e.g., around the campfire listening to stories or music, or watching dances.  Since such contexts were relatively safe places, our drug and art appreciation modes are relatively tolerant of delusions.  But today drugs are cheap, we can hear music all the time, most surfaces are covered by art, and we spend much of our day with stories from TV, video games, etc.       And all that art is made by organized groups of specialists far better than the typical ancestral artist.</li>
<li>We were built to be influenced by the rhetoric, eloquence, [difficulty](academias-function), drama, and repetition of arguments, not just their logic.  Perhaps this once helped us to ally us with high status folks.  And we were built to show our ideals via the stories we like, and also to like well-crafted stories.  But today we are exposed to arguments and stories by folks far more expert than found in ancestral tribes.  Since we are built to be quite awed and persuaded by such displays, our beliefs and ideals are highly influenced by our writers and story-tellers.  And these folks in turn tell us what we want to hear, or what their patrons want us to hear, neither of which need have much to do with reality.</li>
</ol>
These factors combine to make our era the most consistently and consequentially deluded and unadaptive of any era ever.  When they remember us, our distant descendants will be shake their heads at the demographic transition, where we each took far less than full advantage of the reproductive opportunities our wealth offered.  They will note how we instead spent our wealth to buy products we saw in ads that talked mostly about the sort of folks who buy them.  They will lament our obsession with super-stimili that highjacked our evolved heuristics to give us taste without nutrition.   They will note we spent vast sums on things that didn’t actually help on the margin, such as on medicine that didn’t make us healthier, or education that didn’t make us more productive.

Our descendants will also remember our adolescent and extreme mating patterns, our extreme gender personalities, and our unprecedentedly fierce warriors.  They will be amazed at the strange religious, political, and social beliefs we acted on, and how we preferred a political system, democracy, designed to emphasize the hardly-considered fleeting delusory thoughts of the median voter rather than the considered opinions of our best experts.

Perhaps most important, our descendants may remember how history hung by a precarious thread on a few crucial coordination choices that our highly integrated rapidly changing world did or might have allowed us to achieve, and the strange delusions that influenced such choices.  These choices might have been about global warming, rampaging robots, nuclear weapons, bioterror, etc.  Our delusions may have led us to do something quite wonderful, or quite horrible, that permanently changed the options available to our descendants.  This would be the most lasting legacy of this, our explosively growing dream time, when what was once adaptive behavior with mostly harmless delusions become strange and dreamy unadaptive behavior, before adaptation again reasserted a clear-headed relation between behavior and reality.

Our dreamtime will be a time of legend, a favorite setting for grand fiction, when low-delusion heroes and the strange rich clowns around them could most plausibly have changed the course of history.  Perhaps most dramatic will be tragedies about dreamtime advocates who could foresee and were horrified by the coming slow stable adaptive eons, and tried passionately, but unsuccessfully, to prevent them.

## [DreamTime](#table-of-contents)
_Posted on 2010-06-05_

The most common voluntary activity is not eating, drinking alcohol, or taking drugs. It is not socializing with friends, participating in sports, or relaxing with the family. While people sometimes describe sex as their most pleasurable act, time-management studies find that the average American adult devotes just four minutes per day to sex.

Our main leisure activity is, by a long shot, participating in experiences that we know are not real. When we are free to do whatever we want, we retreat to the imagination—to worlds created by others, as with books, movies, video games, and television (over four hours a day for the average American), or to worlds we ourselves create, as when daydreaming and fantasizing. …

This is a strange way for an animal to spend its days. Surely we would be better off pursuing more adaptive activities—eating and drinking and fornicating, establishing relationships, building shelter, and teaching our children. Instead, 2-year-olds pretend to be lions, graduate students stay up all night playing video games, young parents hide from their offspring to read novels, and many men spend more time viewing Internet pornography than interacting with real women. …

One solution to this puzzle is that the pleasures of the imagination exist because they hijack mental systems that have evolved for real-world pleasure. We enjoy imaginative experiences because at some level we don’t distinguish them from real ones. …

Just as artificial sweeteners can be sweeter than sugar, unreal events can be more moving than real ones. There are three reasons for this.  First, fictional people tend to be wittier and more clever than friends and family, and their adventures are usually much more interesting. I have contact with the lives of people around me, but this is a small slice of humanity, and perhaps not the most interesting slice. My real world doesn’t include an emotionally wounded cop tracking down a serial killer, a hooker with a heart of gold, or a wisecracking vampire. As best I know, none of my friends has killed his father and married his mother. But I can meet all of those people in imaginary worlds.

Second, life just creeps along, with long spans where nothing much happens. The O.J. Simpson trial lasted months, and much of it was deadly dull. Stories solve this problem—as the critic Clive James once put it, “Fiction is life with the dull bits left out.” This is one reason why Friends is more interesting than your friends.

Finally, the technologies of the imagination provide stimulation of a sort that is impossible to get in the real world. A novel can span birth to death and can show you how the person behaves in situations that you could never otherwise observe. In reality you can never truly know what a person is thinking; in a story, the writer can tell you. (<a href="http://chronicle.com/article/The-Pleasures-of-Imagination/65678">more</a>)

Yes modern stories and art are more enticing than were those of our distant forager ancestors.  But their stories and art also occupied much of their time, especially when food was plentiful.  It seems rather implausible that this was only because “imagination … hijack[s] mental systems that have evolved for real-world pleasure.”  Surely our foragers would have evolved a resistance to such imagination, if it in fact wasted valuable time.  I’m pretty confident that since foragers had stories and art, then stories and art must have served, and still serve, important functions.

Modern humans often prefer to believe that the activities which they most treasure have no evolutionary function – that they were accidents.  This attitude helps them stay blind to those functions, awareness of which would make their treasured activities seem less noble.

## [Dreamtime Social Games](#table-of-contents)
_Posted on 2019-09-27_

Ten years ago, I posted one of my most popular essays: “[This is the Dreamtime](this-is-the-dream-time).” In it, I argued that, because we are rich,
> Our descendants will remember our era as the one where the human capacity to sincerely believe crazy non-adaptive things, and act on those beliefs, was dialed to the max.

Today I want to talk about dreamtime social games.

For at least a million years, our ancestors wandered the Earth in small bands of 20-50 people. These groups were so big that they ran out of food if they stayed in one place, which is why they wandered. But such groups were big and smart enough to spread individual risks well, and to be relative safe from predators.

So in good times at least, the main environment that mattered to our forager ancestors was each other. That is, they succeeded or failed mostly based on winning social games. Those who achieved higher status in their group gained more food, protection, lovers, and kids. And so, while foragers pretended that they were all equal, they actually spent much of their time and energy trying to win such status games. They tried to look impressive, to join respected alliances, to undermine rival alliances, and so on. Usually in the context of grand impractical leisure and play.

As I described [recently](status-apps-are-coming), status is usually based on a wide range of clues regarding one’s impressiveness, and the relative weight on these clues does vary across cultures. But there are many generic clues that tend to be important in most all cultures, including strength, courage, intelligence, wit, art, loyalty, social support etc.
When an ability was important for survival in a local environment, cultural selection tended to encourage societies to put more weight on that ability in local status ratings, especially when their society felt under threat. So given famine, hunters gain status, given war warriors gain status, and when searching for a new home explorers gain status.

But when the local environment seemed less threatening, humans have tended to revert back to a more standard human social game, focused on less clearly useful abilities. And the more secure a society, and the longer it has felt secure, the more strongly it reverts. So across history the social worlds of comfortable elites have been remarkably similar. In the social worlds such as Versailles, Tales of Genji, or Google today, we see less emphasis on abilities that help win in larger harsher world, or that protect this smaller world from larger worlds, and more emphasis on complex internal politics based on beauty, wit, abstract ideas, artistic tastes, political factions, and who likes who.

That is, as people feel safer, local status metrics and social institutions drift toward emphasizing likability over effectiveness, popularity and impressiveness over useful accomplishment, and art and design over engineering. And as our world has been getting richer and safer for many centuries now, our culture has long been [moving](two-types-of-people) toward emphasizing such forager values and attitudes. (Though crises like wars often push us back temporarily.)
“Liberals” tend to have moved further on this path than “conservatives”, as [indicated](conservative-vs-liberal-jobs) by typical jobs:
> jobs that lean conservative … [are] where there are rare big bad things that can go wrong, and you want workers who can help keep them from happening. … Conservatives are more focused on fear of bad things, and protecting against them. … Jobs that lean liberal… [have] small chances that a worker will cause a rare huge success … [or] people who talk well.

Also, “conservative” attitudes toward marriage have focused on raising kids and on a division of labor in production, while “liberal” attitudes have focused on sex, romance, and sharing leisure activities.

Rather than acknowledging that our status priorities change as we feel safer, humans often give lip service to valuing useful outcomes, while actually more valuing the usual social game criteria. So we pretend to go to school to learn useful class material, but we actually gain prestige while learning little that is useful. We pretend that we pick lawyers who win cases, yet don’t bother to publish track records and mainly pick lawyers based on institutional prestige. We pretend we pick doctors to improve health, but also don’t publish track records and mainly pick via institutional prestige, and don’t notice that there’s little correlation between health and medicine. We pretend to invest in hedge funds to gain higher returns, but really gain status via association with impressive fund managers, and pay via lower average returns.

I recently realized that, alas, my desire to move our institutions more toward “[paying for results](radical-pay-for-results)” is at odds with this strong social trend. Our institutions could be much more effective at getting us the things we say we want out of them, but we seem mostly content to let them be run by the usual social status games. We put high status people in change and give them a lot of discretion, as long as they give lip service to our usual practical goals. It feels to most people like a loss in collective status if they let their institutions actually focus too much on results.
A focus on results would probably result in the rise to power of less impressive looking people who manage to get more useful things done. That is what we’ve seen when firms have adopted prediction markets. At first firms hope that such markets may help them identify the best informed employees. But are are disappointed to learn that winners tend not to look socially impressive, but are more nerdy difficult inarticulate contrarians. Not the sort they actually want to promote.

Paying more for results would feel to most people like having to invite less suave and lower class engineers or apartment sups to your swanky parties because they are useful as associates. Or having to switch from dating hip hunky Tinder dudes to reliable practical guys with steady jobs. In status terms, that all feels less like admiring prestige and more like submitting to domination, which is a forager no-no. Paying for results is the sort of thing that poor practical people have to do, not rich prestigious folks like you.

Of course our society is full of social situations where practical people get enough rewards to keep them doing practical things. So that the world actually works. People sometimes try to kill such things, but then they suffer badly and learn to stop. But most folks who express interest in social reforms seem to care more about projecting their grand hopes and ideals, relative to making stuff work better. Strong emotional support for efficiency-driven reform must come from those who have deeply felt the sting of inefficiency. Perhaps regarding [crime](who-vouches-for-you)?
Ordinary human intuitions work well for playing the usual social status games. You can just rely on standard intuitions re who you like and are impressed by, and who you should say what to. In contrast, figuring out how to actually and effectively pay for results is far more complex, and depends more on the details of your world. So good solutions there are unlikely to be well described by simple slogans, and are not optimized for showing off one’s good values. Which, alas, seems another big obstacle to creating better institutions.

## [We Moderns Are Status-Drunk](#table-of-contents)
_Posted on 2021-06-27_

Twelve years ago I [posted](this-is-the-dream-time) on how our era is a rare unique “dreamtime” of fast growth, wide cultural integration, and delusional beliefs. But I think I missed a big reason why we have the delusions we do: <em>as we get rich, we each increasingly over-estimate our relative social status</em>. Let me explain.
The core idea of evolutionary psychology is that evolution shaped our behaviors to be adaptive in our ancestral environments. That is, we do stuff that gives us more descendants. But because our ancestors only experienced a limited range of environments, we only evolved behavior rules sufficient to induce adaptive behavior in those actual environments. This made our behavior indeterminate in the other new environments which humans have experienced since then. So a re-run of the process of evolution could easily lead to different behaviors in these new environments. That is, human behavior today results not just from adaptation to ancestral environments, but also from the many random ways that evolution happened to encode our behavior in rules.

For example, our ancestors needed to drink water to avoid dehydration, but because in their environments water always had the same combination of water smell and water feel, we could have evolved either to check that stuff is water by its smell, or by its feel. If those two water features always go together, and if both methods are just as easy, then this difference won’t make much difference to behavior. We find water, check that it is water, and drink it. But if later we encountered stuff that had water smell but not water feel, or water feel but not water smell, then these two different ways to detect water might lead to very different behaviors. For example, water-smell humans might drink stuff that smells but doesn’t feel like water, while water-feel humans would not drink such stuff.

In this post, I want to suggest that much of the “modern” human style which has arisen since the industrial revolution results from a particular way that evolution happened to encode human detection of relative status. This has made human history go surprisingly well in some ways, and surprisngly badly in others. Had evolution happened to have coded our status detection machinery differently, these last few centuries might have played out very differently. And perhaps they did, in alien histories. But before we get into that, let us first see how our status detection methods have shaped the modern human style.<span id="more-32881"></span>

Most social animals have status ladders, and humans are no exception. Selfishly optimal animal behavior depends on where an animal sits in such ladders. Thus animals need ways to detect the relative status rank of themselves and potential interaction partners. The same applies to humans, though humans had some new ways to mark and assert status, and so needed some new ways to judge status.

My key hypothesis is this: evolution had humans use their <em>absolute</em> income/wealth to judge their <em>relative</em> status. (I’m talking here about overall status in the larger community, not status relative to particular associates; we have many better clues to judge that.) Yes, this method would work badly in environments where communities varied greatly in average levels of absolute income/wealth. In that case, someone rich might think that they had high relative status, when in fact most everyone in their society was also rich.

But before the industrial revolution there were few persistent differences in average income/wealth across societies. Yes, there were temporary famines and pandemics, and so good times and bad, but these periods were short relative to human lifetimes. So until recently absolute wealth, averaged over many years, was in fact a good indicator of relative status.

However, for the first time in history the industrial revolution enabled income/wealth to grow faster than did human population, inducing a rapid increase in average income/wealth, an increase that has been continuing for several centuries now. As a result, our status detection systems have severely misfired. They tell us each that, because we are rich, we have high relative status. And the richer we have become, the more severe has been this error.

To judge how this has this distorted our behavior, we mainly just need to know how humans had previously evolved to adjust their behavior to relative status. For forager and farmer era humans, what behaviors were more adaptive for the high in status? We can find many such differences.

For example, for most social mammals, being higher status protects you more from stressful life events, so that you less often invoke the standard mammal stress response. By not spending on stress, you body invests more in growth and immunity. So higher status primates are less sick, and live longer. Thus this theory predicts that humans came to live much longer after the industrial revolution. You might think that this outcome is also predicted by our being able to afford more medicine, nutrition, clean water, and other public health measures. But in fact these factors do a poor job of explaining the magnitude and steadiness of the mortality fall over the last few centuries. Changes in these other factors have been weaker and less steady than declining mortality.

Higher status animals also tend more to be group leaders, and thus to be peacemakers regarding local disputes. Yes, the leaders of a group may manage its disputes with outsiders, and then they may need to act tough. But leaders are supposed to less take sides regarding internal disputes, and more try to resolve them peacefully. That is, they have a wider moral circle, and are more cooperative and pro-social. Thus higher status animals less often pick fights with associates, so they are on average more peaceful. And low status humans are consistently more violent than are high status ones. Thus this theory predicts what we have seen: declining rates of violence and conflict, less war, and widening moral circles.

However, even as wars get rare, the fact that soldiers are higher status means that more people expect to participate in wars when they happen; soldiering has become more democratic. This wider view of leaders seems to be implemented in part via leaders taking on more abstract/far [views](near-far-summary), relative to concrete/near views. This predicts that we moderns increasingly take on far views, relative to near views, and this seems roughly right.
As status markers tend to complement each other, it makes sense for people with some markers to work harder to acquire more such markers. Also, the high in status tend to have more resources and better abilities, both of which suggest higher returns from investing in more status markers. Thus people who believe they are high status naturally try to invest more in rising even further in status. What specifically they will do depends on what counts more for status in their society for their age, gender, etc. For example, they might do sports, combat, poetry, music, art, crafts, travel, scholarship, invention, etc. But the key prediction is: we are more mad for status, as we think we already have a lot of it.

Also, as status is often conferred for showing range and variety in such abilities, we pursue such range and variety. And as most of these things require training, this predicts more school, as does the fact that school tends to confer status directly. Over the last few centuries we have in fact seen a consistent rise in the fraction of their time and energy spent on all these things, and also a rise in their emphasis on variety in such things. We do more school, even [though](who-wants-school) we don’t seem to learn much useful there. We have slowly spent more time on leisure as we’ve become richer, but this decline has been slower than many had expected. Plausibly this is because work also gives us great status, and it is mainly the pressure for variety in our status markers than makes us also pursue non-work status.
In most societies, investments in fertility take time and time and energy away from investments in status. Yes fertility confers some status, but in our world not as much. As people get rich, [they](billandme) [are](status-seeking-as-context-neglecting-value) tempted to invest less in immediate fertility in order to gain in status, which could help them or their children later become a high status “king” or “queen”, a role that could then allow much higher fertility later. For example, a young woman might delay fertility to invest in poetry, music, etc., hoping to then be chosen as queen, which would allow her kids to have many grandkids. Or parents might choose to have fewer children, so that they can invest in more status markers for each child that they have. Both strategies reduce overall fertility, and in fact fertility has fallen dramatically over the last few centuries, seemingly in response to local wealth levels. The other explanations offered for this fertility fall are mostly quite unsatisfactory.
While in most firms various political factions vie for dominance, low level workers are often well advised to “keep their head down”, and just do their job. But high level managers must pick sides and play the game. More generally, high status people are expected to participate in elite conversation and governance. That is, they are more expected to take on formal governance roles, and also speak up and express opinions on the issues of the day. Which will naturally result in them allying with political factions. And to do this well they need to keep up with gossip and the news. Also, we all tend to rise in status when we seem to influence the behavior of others, but fall when lower status others seem to influence us.

All this induces higher status people to track more news, and to talk more, more visibly, and more politically. It induces us to make and push more behavior recommendations, and to try harder to govern everything, creating more governance roles to fill. As democracy allows more people to participate in governance, we predict more democracy. And in fact over the last few centuries we have seen people more eager for news, talk, politics, democracy, government, and paternalistic policies.

As high status people are held to higher standards regarding social and moral norms, we hold ourselves to higher standards, but are also more willing to criticize others who see claim high status but fail to meet such standards. Regarding religion, our seeing ourselves as higher status makes us more expect to be prophets, priests, monks, martyrs, and activists, but less to be the prototypical attendee of religious services, the meek supplicant to whom religion offers comfort and meaning in their hard life. And in fact we are more moral, more morally critical, seek more to be prophets and activists, but less attend church.

The high in status tend to have relationships and projects that last longer, so that they need to attend to longer timescales. And they will suffer less theft and loss of relations which can discourage long term investments. Thus the high in status discount the future less. And we do in fact see over time less discounting and longer time horizons, expressed in particular in lower interest rates.

All told, this theory seems pretty successful to me. The assumption that evolution had humans estimate their relative status vis their absolute income/wealth predicts many trends and unique styles of the industrial era, including rising lifespans, lower fertility, falling violence, more school, more effort into art/travel/invention/etc., and much more. We now have a deeper understanding of how and why we modern humans have a different style from ancient humans. Note that as evolution should slowly correct our mistaken non-adaptive way to estimate relative status, this modern era won’t last forever; we will eventually wake from our dreamtime.

Science fiction often depicts alien worlds with very advanced technology, and yet with social styles and attitudes more like those of our ancients. I always thought that a mistake, but this analysis suggests it isn’t so crazy. Had evolution had us use relative wealth to estimate our relative status, most of these changes would have not happened, or been much weaker. We might well have continued more with ancient human styles in the industrial era and beyond.

Early in the industrial era many expressed great [fears](the-industrial-revolution-continues) for where it might go, and while those fears seem to have been overblown, this analysis suggests that they weren’t crazy. A more ancient-style industrial era would have had more violence and war, shorter lives, more work and less emphasis on variety in leisure, and thus more regimentation of leisure as well as work. There’d also be less democracy and politics, and less obsession with social media. A dramatically different world that might have been, and may well have actually existed in alien histories.
<strong>Added 28Jun:</strong> During the forager era, humans had strong direct contact with everyone in their band, and so had relatively clear signals about their status relative to each such person. Which easily added up to one’s relative status overall. So it may have been the introduction of larger communities (~1000) in the farming era that created a need for ways to estimate one’s status relative to people with which one did not have much contact. That is where it would have been handy to be able to just look at yourself to infer your relative status. Looking at your personal wealth would have worked well then.

## [Earth: A Status Report](#table-of-contents)
_Posted on 2023-01-02_

In a universe that is (so far) almost entirely dead, we find ourselves to be on a rare planet full not only of life, but now also of human-level intelligent self-aware creatures. This makes our planet a roughly a once-per-million-galaxy rarity, and if we ever get [grabby](http://grabbyaliens.com/) we can expect to meet other grabby aliens in roughly a billion years.

We see that our world, our minds, and our preferences have been shaped by at least four billions years of natural selection. And we see that evolution going especially fast lately, as we humans pioneer many powerful new innovations. Our latest big thing: larger scale organizations, which have induced our current brief [dreamtime](https://www.overcomingbias.com/2009/09/this-is-the-dream-time.html), wherein we are unusually rich.

For preferences, evolution has given us humans a mix of (a) some robust general preferences, like wanting to be respected and rich, (b) some less robust but deeply embedded preferences, like preferring certain human body shapes, and (c) some less robust but cultural plastic preferences, such as which particular things each culture finds more impressive.

My main reaction to all this is to feel grateful to be a living intelligent creature, who is compatible enough with his world to often get what he wants. Especially to be living in such a rich era. I accept that I and my descendants will long continue to compete (in part by cooperating of course), and that as the world changes evolution will continue to change my descendants, including as needed their values.

Many see this situation quite differently from me, however. For example, “anti-natalists” see life as a terrible crime, as the badness of our pains outweigh the goodness of our pleasures, resulting in net negative value lives. They thus want life on Earth to go extinct. Maybe, they say, it would be okay to only create really-rich better-emotionally-adjusted creatures. But not the humans we have now.

Many kinds of “conservatives” are proud to note that their ancestors changed in order to win prior evolutionary competitions. But they are generally opposed to future such changes. They want only limited changes to our tech, culture, lives, and values; bigger changes seem like abominations to them.

Many “[socialists](https://www.overcomingbias.com/?s=socialism)” are furious that some of us are richer and more influential than others. Furious enough to burn down everything if we don’t switch soon to more egalitarian systems of distribution and control. The fact that our existing social systems won difficult prior contests does not carry much weight with them. They insist on big radical changes now, and disavow any failures associated with prior attempts made under their banner. None of that was “real” socialism, you see.

Due to continued global competition, local adoption of anti-natalist, conservative, or socialist agendas seems insufficient to ensure these as global outcomes. Now most fans of these things don’t care much about long term outcomes. But some do. Some of those hope that global social pressures, via global social norms, may be sufficient. And others suggest using stronger global governance.

In fact, our scales of governance, and level of global governance, have been increasing over centuries. Furthermore, over the last half century we have [created](https://www.overcomingbias.com/2020/09/the-world-forager-elite.html) a world community of elites, wherein global social norms and pressures have strong power.

However, competition at the largest scales has so far been our only robust [solution](https://www.overcomingbias.com/2022/04/will-design-escape-selection.html) to system [rot](https://www.overcomingbias.com/2021/11/will-world-government-rot.html) and [suicide](https://www.overcomingbias.com/2018/11/world-government-risks-collective-suicide.html), problems that may well apply to systems of global governance or norms. Furthermore, centralized rulers may be [reluctant](https://www.overcomingbias.com/2021/07/the-coming-cosmic-control-conflict.html) to allow civilization to expand to distant places which they would find it harder to control.

This post resulted from Agnes Callard asking me to comment on Scott Alexander’s essay _[Meditations On](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/) [Moloch](https://slatestarcodex.com/2014/08/01/misperceptions-on-moloch/)_, wherein he takes similarly stark positions on these grand issues. Alexander is irate that the world is not adopting various utopian solutions to common problems, such as ending corporate welfare, smaller militaries, and common hospital medical record systems. He seems to blame all of that, and pretty much anything else that has ever gone wrong, on something he personalizes into a monster “Moloch.” And while Alexander isn’t very clear on what exactly that is, my best read is that it is the general phenomenon of competition (at least the bad sort); that at least seems central to most of the examples he gives.

Furthermore, Alexander fears that, in the long run, competition will force our descendants to give up absolutely everything that they value, just to exist. Now he has no empirical or theoretical proof that this will happen; his post is instead mostly a long passionate primal scream expressing his terror at this possibility.

(Yes, he and I are aware that cooperation and competition systems are often nested within each other. The issue here is about the largest outer-most active system.)

Alexander’s solution is:

> Elua. He is the god of flowers and free love and all soft and fragile things. Of art and science and philosophy and love. Of niceness, community, and civilization. He is a god of humans. … Only another god can kill Moloch. We have one on our side, but he needs our help. We should give it to him.

By which Alexander means: start with a tiny weak AI, induce it to “[foom](https://www.overcomingbias.com/?s=foom)” (sudden growth from tiny to huge), resulting in a single “super-intelligent” AI who rules our galaxy with an iron fist, but wrapped the velvet glove of being “friendly” = “aligned”. By definition, such a creature makes the best possible utopia for us all. Sure, Alexander has no idea how to reliably induce a foom or to create an aligned-through-foom AI, but there are some people pondering theses questions (who are generally not very optimistic).

My response: yes of course if we could easily and reliably create a god to mange a utopia where nothing ever goes wrong, maybe we should do so. But I see enormous [risks](https://www.overcomingbias.com/2022/06/why-not-wait-on-ai-risk.html) in trying to induce a single AI to grow crazy fast and then conquer everything, and also in trying to control that thing later via pre-foom design. I also fear many other risks of a single global system, including rot, suicide, and preventing expansion.

Yes, we might take this chance if we were quite sure that in the long term all other alternatives result in near zero value, while this remained the only scenario that could result in substantial value. But that just doesn’t seem remotely like our actual situation to me.

Because: competition just isn’t as bad as Alexander fears. And it certainly shouldn’t be blamed for everything that has ever gone wrong. More like: it should be credited for everything that has ever gone right among life and humans.

First, we don’t have good reasons to expect competition, compared to an AI god, to lead more reliably to the extinction either of life or of creatures who value their experiences. Yes, you can fear those outcomes, but I can as easily fear your AI god.

Second, competition has so far reigned over four billion years of Earth life, and at least a half billion years of Earth brains, and on average those seem to have been brain lives worth living. As have been the hundred billion human brain lives so far. So empirically, so far, given pretty long time periods, competition has just not remotely destroyed all value.

Now I suspect that Alexander might respond here thus:

> The way that evolution has so far managed to let competing creatures typically achieve their values is by having those values change over time as their worlds change. But I want descendants to continue to achieve their values _without_ having to change those values across generations.

However, relatively soon on evolutionary timescales, I’ve [predicted](https://www.overcomingbias.com/2021/12/on-evolved-values.html) that, given further competition, our descendants will come to just directly and abstractly value reproduction. And then after that, no descendant ever need to change their values. But I think even that situation isn’t good enough for Alexander; he wants our (his?) _current_ human values to be the ones that continue and never change.

Now taken very concretely, this seems to require that our descendants never change their tastes in music, movies, or clothes. But I think Alexander has in mind only keeping values the same at some _intermediate_ level of abstraction. Above the level of specific music styles, but below the level of just wanting to reproduce. However, not only has Alexander not been very clear regarding which exact value abstraction level he cares about, I’m not clear on why the rest of us should agree to with him about this level, or care as much as he does about it.

For example, what if most of our descendants get so used to communicating via text that they drop talking via sound, and thus also get less interesting in music? Oh they like artistic expressions using other mediums, such as text, but music becomes much more of a niche taste, mainly of interest to that fraction of our descendants who still attend a lot to sound.

This doesn’t seem like such a terrible future to me. Certainly not so terrible that we should risk everything to prevent it by trying to appoint an AI god. But if this scenario does actually seem that terrible to you, I guess maybe you should join Alexander’s camp. Unless all changes seem terrible to you, in which case you might join the conservative camp. Or maybe all life seems terrible to you, in which case you might join the anti-natalists.

Me, I accept the likelihood and good-enough-ness of modest “[value drift](https://www.overcomingbias.com/2018/02/on-value-drift.html)” due to future competition. I’m not saying I have no preferences whatsoever about my descendants’ values. But relative to the plausible range I envision, I don’t feel greatly at risk. And definitely not so much at risk as to make desperate gambles that could go very wrong.

You might ask: if I don’t think making an AI god is the best way to get out of bad equilibria, what do I suggest instead? I’ll give the usual answer: innovation. For most problems, people have thought of plausible candidate solutions. What is usually needed is for people to test those solution in smaller scale trials. With smaller successes, it gets easier to entice people to coordinate to adopt them.

And how do you get people to try smaller versions? Dare them, inspire them, lead them, whatever works; this isn’t something I’m good at. In the long run, such trials tend to happen anyway, by accident, even when no one is inspired to do them on purpose. But the goal is to speed up that future, via smaller trials of promising innovation concepts.

**Added 5Jan:** While I was presuming that Alexander had intended substantial content to his claims about Moloch, many are saying no, he really just mean to say “bad equilibria are bad”. Which is just a mood well-expressed, but doesn’t remotely support the AI god strategy.

## [On Teen Angst](#table-of-contents)
_Posted on 2010-06-12_

Two complementary theories of teen angst:

<ol>
<li>Our homo hypocritus ancestors overtly followed idealistic norms, such as against dominance and bragging, but covertly violated them.  They also cheated often on norms of sexual fidelity.  An important part of growing up in such a world was learning to see that acts oft deviate from spoken ideals, and to affirm ideals via outrage at such hypocrisy, before one was old enough to have been very hypocritical oneself.  And since the young seek to displace the old in the positions of highest status, old hypocrisy makes a good rallying cry.</li>
<li>In the vast majority of the past, and the vast majority of the future, people grow up in a world for which they were designed – their inborn expectations and intuitions are good guides to their world.  But in this the great Dreamtime, only ten thousand years old, mostly done, and near its peak, our inborn intuitions are poor guides – we awake into a world we find strange, fake, and wrong.  So when young, we are drawn to stories about righting those wrongs by exposing this fake world, replacing it with a true one, and in the process having an adventure where we prove our mettle and impress potential mates and allies.</li>
</ol>
Below are quotes on teen angst in fiction.  They inspire this open letter of mine:

<strong>Dear angsty teen,</strong>

As you suspect, the world into which you have been born is indeed strange, fake, and wrong, relative to your inborn intuitions. Adults have not been frank with you, or themselves, about how often they fail to live up to your ideals or theirs. In fact, much of the function of school and other ways adults shape your youth is to use social pressure to get you to replace your inborn ideals with new given ideals, and to accept your and others’ hypocrisies.

There maybe be places you could move which better fit your inborn ideals and expectations, and there may be ways to change your current place to better fit such things. You may even devote some energy to such moving or changing.  But the vast majority of you will mostly forget your angst, eagerly trading your inborn ideals for the hope of social approval and respect. A few of you will hold the most strongly to your inborn ideals, paying great costs to move or change. Some such efforts will even succeed, moving your world closer to your inborn ideals.

But know that your world is stable enough so that if you actually “fight the power,” you will on average lose.  Most of what looks like young “rebels” winning is actually part of the established order.  New art, tech, political groups, etc. often replace old ones with rhetoric about how the change better achieves natural ideals.  Such rhetoric can bind “rebels” together, helping them beat rivals. But most such changes do little about hypocrisy or idealism overall, and the few that do mostly reflect larger trends, not a triumph of some group’s moral fervor.

On average, real rebels who most hold to their inborn ideals do not thereby gain social approval or respect – they lose it.  Real rebels are little like the heroes of your teen angst fiction, who accumulate fascinating stories while proving their mettle and impressing potential mates and allies. While some real rebels succeed in exposing more hypocrisy to those willing to listen, it is the willingness to listen that is the main block. Those willing to look for hypocrisy can find it easily enough themselves, most anywhere they look.

Finally, pause for a moment and ask: how sure can you be that your inborn ideals are really better than the ideals society wishes to imprint on you? Your inborn ideals were adaptive to a world that is long gone, and only then in conjunction with lots of hypocrisy; the ideals adults want to imprint on you instead seem better adapted to your current world. There is no solid rock on which you can stand; we all float in a sea of choice; choose your ideals, and your level of hypocrisy, and pay the price.

Now for those quotes.  On <a href="http://www.nytimes.com/2010/01/29/books/29appraisal.html">JD Salinger</a>:<span id="more-23358"></span>

Mr. Salinger had such unerring radar for the feelings of teenage angst and vulnerability and anger … Mr. Salinger’s people tend to be outsiders — spiritual voyagers shipwrecked in a vulgar and materialistic world, misfits who never really outgrew adolescent feelings of estrangement. … Such characters have a yearning for some greater spiritual truth, but they are also given to an adolescent either/or view of the world and tend to divide people into categories: the authentic and the phony, those with an understanding … and those coarse, unenlightened morons who will never get it — a sprawling category, it turns out, that includes everyone from pompous college students parroting trendy lit crit theories to fashionable, well-fed theater-goers to self-satisfied blowhards who recount every play in a football game or proudly wear tattersall vests.

On <a href="http://www.newyorker.com/arts/critics/atlarge/2010/06/14/100614crat_atlarge_miller">Dystopian Teen Fiction</a>:

A recent boom in dystopian fiction for young people. … Intricately imagined worlds.  … For example, all sixteen-year-olds undergo surgery to conform to a universal standard of prettiness. … Teen-age boys awaken, all memories of their previous lives wiped clean, in a walled compound surrounded by a monster-filled labyrinth. The books tend to end in cliff-hangers. … There are, or will soon be, books about teen-agers slotted into governmentally arranged professions and marriages or harvested for spare parts or genetically engineered for particular skills or brainwashed by subliminal messages embedded in music or outfitted with Internet connections in their brains. Then, there are the post-apocalyptic scenarios in which humanity is reduced to subsistence farming or neo-feudalism. … A new, better way of life can be assembled from the ruins. …

Dystopian fiction … it’s about what’s happening, right this minute, in the stormy psyche of the adolescent reader. “The success of ‘Uglies,’ … is partly thanks to high school being a dystopia.” … As a tool of practical propaganda, the [Hunger Games] don’t make much sense. … If, on the other hand, you consider the games as a fever-dream allegory of the adolescent social experience, they become perfectly intelligible. Adults dump teen-agers into the viper pit of high school, spouting a lot of sentimental drivel about what a wonderful stage of life it’s supposed to be. The rules are arbitrary, unfathomable, and subject to sudden change. A brutal social hierarchy prevails, with the rich, the good-looking, and the athletic lording their advantages over everyone else. To survive you have to be totally fake. Adults don’t seem to understand how high the stakes are; your whole life could be over, and they act like it’s just some “phase”! Everyone’s always watching you, scrutinizing your clothes or your friends … but no one cares who you really are or how you really feel about anything.

The typical arc of the dystopian narrative mirrors the course of adolescent disaffection. First, the fictional world is laid out. It may seem pleasant enough. Tally … looks forward to the surgery that will transform her into a Pretty. … Then somebody new, a misfit, turns up, or the hero stumbles on an incongruity. A crack opens in the façade. If the society is a false utopia, the hero discovers the lie at its very foundation: the Pretties are lobotomized when they receive their plastic surgery. … If the society is frankly miserable or oppressive, the hero will learn that, contrary to what he’s been told, there may be an alternative out there, somewhere. Conditions at home become more and more unbearable until finally the hero, alone or with a companion, decides to make a break for it, heading out across dangerous terrain. …

Incorporating the particular flavor of contemporary kid culture. Waking up in a hostile, confined place without an identity or any notion of what you’re supposed to do or how you can get out … is a scenario often found in video games. … There’s more hand-to-hand combat in these dystopias. … Some [kids] will surely grow up to write dystopian tales of their own, incited by technologies or social trends we have yet to conceive. By then, reality TV and privacy on the Internet may seem like quaint, outdated problems. But the part about the world being broken or intolerable, about the need to sweep away the past to make room for the new? That part never gets old.

<strong>Added 13June:</strong> Reports of teen angst seem more common in industry and among farmer aristocrats than elsewhere.  This could be because such folks are more articulate, and have high enough status to complain. If not, this fact seems to favor the second of the two explanations I offered above.

## [Prediction Markets “Fail” To Moloch](#table-of-contents)
_Posted on 2012-07-19_

What is new about prediction markets? To many, the key new idea is “crowd-sourcing”, which to many means that if you can “gamify” your problem enough, hobbyists will solve it for fun, much cheaper than if you had to hire employees. To me the key new idea is instead the “information prize”, a way to offer to pay others to find the info you want. (If they don’t find, you don’t pay.)

Intrade is a typical gamified market – you might pay Intrade a little to put up a question, and if traders like it they’ll pay Intrade to answer your question. A market-maker-driven corporate prediction market (such as firms pay <a href="http://www.consensuspoint.com/">Consensus Point</a> to set up) is very different – to answer key business questions, firms <em>pay</em> lots to create markets, to fund employee participants, and to subsidize market makers.

Alas there’s been a lot more interest over the last few years in the get-work-for-free concept than in the pay-for-info concept. And more alas, recent discussions of “prediction market failures” are mostly on their failures to mooch. Case in point #1, Casey Mulligan today:

The efficient-prediction perspective presumes that the market exists on a scale large enough to create significant rewards for those with accurate predictions. Another perspective, “no-trade,” says prediction market participation will be low, if not zero, because traders suspect that a person would take the opposite side of their trades only because he had superior knowledge about the outcome. A market cannot survive if its only participants are “insider traders.”

Paradoxically, a prediction market cannot succeed unless it multitasks – it must serve an additional purpose separate from predictions, so that the participants with information about the outcome have counterparts to take the other side of their trades. In the case of sports and political markets, that additional purpose is entertainment – people enjoy engaging in the activity and are willing to participate even if their expected profits are zero or negative. These people are participating for various reasons other than prediction. (<a href="http://economix.blogs.nytimes.com/2012/07/18/can-prediction-markets-show-us-the-way/">more</a>)

No, a market <em>can</em> single-task, with no other function than prediction, and no other trader motive than selfish financial profit, <em>if</em> someone who wants the info will pay to subsidize the market. It is only when you want people to answer your question for free that you’ll have to piggyback on their having some other reason to trade.

Of course there’s no guarantee that your willingness to pay for some info exceeds other folks’ cost to supply that info. Supply and demand curves need not intersect at a positive quantity. But that’s hardly a failure of an info exchange mechanism.

Case in point #2, Snowberg, Wolfers, & Zitzewitz’s new paper “<a href="http://www.brookings.edu/~/media/research/files/papers/2012/6/13%20prediction%20markets%20wolfers/13%20prediction%20markets%20wolfers.pdf">Prediction Markets for Economic Forecasting</a>“, for the <em>Handbook of Economic Forecasting</em>, also sees not getting stuff for free as “failure”:

3.1 Why They (Sometimes) Fail

Although prediction markets generally function quite well, design flaws sometimes prevent reliable forecasts. These flaws generally lead to a lack of noise traders (or thin markets) that reduces incentives for discovering, and trading on the basis of, private information. In order to attract noise traders, the subject of a prediction market must be interesting and information must be widely dispersed. Prediction market contracts must be well specifed, so that it is clear when they will (and will not) pay off. However, this specicity may be in tension with making a contract interesting for traders. …

Noise traders may quite rationally choose not to trade in markets where there is a high degree of insider information. For example, despite the high intrinsic interest in who a Supreme Court nominee will be, markets on this topic have routinely failed. This may be due to the fact that most traders are aware that there are very few people with actual information on who the President’s choice will be. This anecdote underlines the importance of prohibiting insider trading: for instance, a market to predict the Institute for Supply Management’s (ISM’s) business confidence measure would be unlikely to function if it were well known that ISM employees were trading in it. …

Corporations are attracted to prediction markets as they can potentially pass unbiased information from a company’s front-line employees to senior management. However, many questions of interest to executives are not widely interesting, nor are there many employees that have relevant information. This creates a lack of liquidity in markets, perhaps leading to no trading, or, worse, inaccurate predictions. Microsoft has responded to this problem by using a market-making algorithm. ….

The stories of failure above lead to some straight-forward rules for designing prediction markets: make sure the question is well-defined, that there is dispersed information about the question, and that there is sufficient interest in the question to ensure liquidity. (<a href="http://www.brookings.edu/~/media/research/files/papers/2012/6/13%20prediction%20markets%20wolfers/13%20prediction%20markets%20wolfers.pdf">more</a>)

Noise traders are traders who subsidize your market for free, for reasons of their own, such as risk-hedging, idiocy, etc. If you fail to attract noise traders, you fail to get their free subsidy. But you can still offer to directly pay for your info, by subsidizing the market, as the Microsoft sentence in the quote indicates. Similarly, if employees find executive questions uninteresting, that just means they won’t answer such questions as freely in their spare time. But that hardly means firms can’t pay employees to address key firm questions. Here we are only talking about a “failure” of prediction markets to mooch stuff for free!

Snowberg, Wolfers, & Zitzewitz also err in saying that sometimes there is “no” info:

An extreme form of information not being widely dispersed is when there is no information at all to aggregate. For example, prediction markets on whether weapons of mass destruction (WMDs) would be found in Iraq predicted they would very likely be found. The false confidence that could be inspired by such an estimate ignores the fact that there was no information being aggregated by these markets. That is to say, it was unlikely that anyone in Iraq, who might actually have some information (perhaps based on rumors, past experience, or informal discussions with friends and relatives in the government) about whether Iraq’s WMD program was likely to exist or not, was trading in these markets.

Yes particular info, such as direct personal observations of WMD efforts, existed out there somewhere, but was prohibitively expensive to supply. That is, the price to buy info offered by the Intrade markets was too low to induce folks with such info to supply it. But that hardly meant there was <em>no</em> info available on the subject! There were a great many cheap but relevant clues available for making rough guesses on the subject, and I’m pretty sure that a lot of trading in that market was based on such clues.

When you offer to pay a certain price for info, an efficient info exchange mechanism will typically induce some supply of that info, but only up to the point where the marginal cost of supplying info reaches the price you have offered to pay. It is no failure of an exchange mechanism when buyers cannot always buy everything they want at as low a price as they want.

Finally Snowberg, Wolfers, & Zitzewitz end with this stunner:

We believe the real promise of prediction markets comes not from their ability to predict particular events. Rather, the real promise lies in using these markets, often several at a time, to test particular economic models, and use these models to improve economic forecasts.

This seems like saying the real promise of democracy is that academics can study votes to refine their theories of human behavior. Really?!

<strong>Added</strong>: Alas <em>The Economist</em> blog <a href="http://www.economist.com/blogs/freeexchange/2012/07/prediction-markets">swallows</a> this “no info” error whole.

<strong>Added 20July</strong>: Scott Sumner <a href="http://www.themoneyillusion.com/?p=15446">says</a> people similarly say that his NGDP futures proposal “fails” if they don’t come for free.

## [Seeking Robust Credible Expertise Buyers](#table-of-contents)
_Posted on 2020-07-01_

On Jan 19, 2000, I posted <a href="https://extropians.weidai.com/extropians.1Q00/0991.html">an email</a> to the Extropians mailing list, giving the first public mention of the futarchy idea. (I also have a detailed PPT on the idea dated June 22, 2000, and the first <a href="http://hanson.gmu.edu/futarchy2000.pdf">pdf paper</a> I posted is dated “July 2000.”) So the general idea is just over two decades old now.

Coincidentally, some new prediction platforms have been announced recently, and some have asked me why I do not act more excited about them. So this seems a good time to review my agenda.

I seek to <em><strong>jumpstart stable decision-advising info markets,</strong></em> <em>wherein</em> <strong><em>bias-robust widely-credible expertise</em></strong> <em>is <strong>bought</strong></em> <strong>and</strong> <strong><em>sold</em></strong>. Let’s walk through these terms one at a time.

By <em><strong>jumpstart stable</strong></em> I mean that I’m seeking to start a new regular practice, not just proof-of-concept demonstrations of related technologies. I’m okay with some party subsidizing them at first, to help move to a new equilibria. But that sponsoring party either needs to stay indefinitely, or the market must soon find a way to pay its way without that subsidy. To become a regular practice, relevant parties need to see a long enough track record of how such info markets have worked and performed in their particular topic areas.

By <strong><em>decision-advising info</em></strong> I mean that my goal isn’t to add to or change general talk, gossip, and chatter, much of which is too vague to see what exactly it means, and most of which influences little outside the world of chatter. My goal is instead to influence real and important decisions, via better info. So I want to see info markets that sell clear, precise consensus estimates that can be understood in probability terms, so they can be fed into traditional decision analysis.

To better influence decisions, these estimates should also be as actionable as possible. That is, estimates should sit clearly close to actual decisions, so that decision-makers can see their relevance, and see how different estimates naturally lead to different decisions.

By <strong><em>bought</em></strong> <strong>and</strong> <strong><em>sold</em></strong>, I mean that we need two kinds of participants, buyers and sellers. While there will sometimes be an overlap, in general the people who know things, the info sellers, just aren’t the same as the people who want to know things, the info buyers. And we can’t presume that the sellers will sell info for free. Instead, buyers must offer sufficient rewards to distract sellers from alternate activities.

By <em><strong>markets</strong></em> I mean to integrate these new systems with our many other markets in our mostly market economy. This isn’t a world apart. Most individuals and organizations in our society should be free to participate, if they so choose, as either buyers or sellers of info. And we should expect money to be the usual currency used to make deals.

By <strong><em>expertise</em></strong>, I mean that estimates should be accurate, due to embodying more information. While we must accept that there will be error, i.e., differences between estimates and truth, but on average errors should be minimized. More precisely, for each topic on which the markets offer an estimate, I want that estimate to be as accurate as possible given the costs paid for it. And it should usually be possible to pay more to get more accuracy.

By <em><strong>credible</strong></em>, I mean that estimates need to not just <em>be</em> accurate, but also to <em>seem</em> accurate to key audiences. And by <strong><em>widely</em></strong> I mean credible not just to a few audiences, but to many audiences. There should be a widely held common belief in their accuracy. For the set of topics to which they are said to apply, and holding constant the cost spend, these estimates need not usually seem <em>more</em> accurate than other key sources, but they should rarely seem to be much <em>less</em> accurate.

So I’m not just trying to create a tool that some people will see as useful, if they have certain compatible abilities and attitudes, and after they’ve practiced with it and developed a personal style of usage. Not just a private advisor who might happen to be trusted by a particular decision maker. I’m instead looking for an institution that many people with different goals and agendas can <em>share</em>, and trust together. That is, I seek the most accurate institution that many can share, even if some Individuals think they know of better sources.

For example, the accuracy of estimates shouldn’t depend greatly on the quality of management by key central administrators. Unless most everyone can agree on a reliable way to achieve high management quality, it just isn’t enough to have some people believe in a high quality of current management, if many others are skeptical. If any parts of these markets require central management, we need ways to pick managers that which don’t require unusual and unshared confidence in particular administrators.

The key attraction of widely credible info markets is that they can be used by decision makers who seek not just to make good decisions, but also to convince key audiences that they have made good decisions. And this can help us all to more easily trust agents who make decisions on our behalf. By checking that decisions made match the estimates from related info markets, we can check on decision makers. Or if market estimates can be make directly relevant and actionable enough, we might must put them directly in charge of key decisions.

By <strong><em>robust</em></strong>, I indicate that I want estimate accuracy to be high not just sometimes, but across a wide range of topics and information contexts. And by <strong><em>bias-robust</em></strong> I mean that I want estimates that are robust to situations where many parties would like to bias and distort the estimates, consciously or unconsciously, to influence decision makers. It is no good having something that works well in the lab, or on small unimportant topics, but falls apart when the stakes get high. To be a shared institution on important topics for parties with differing goals and agendas, we need a wide perception that accuracy persists even when many parties seek to distort and manipulate the estimates.

Okay, now that I’ve explained what I want, I can better explain when I get excited.

In the last few decades, dozens of groups have written new software to support info markets of varying forms. Such software is almost always tied to a particular project, and when that project fails the software almost never becomes available for other projects. And most of these groups see software and management as the only project parts worth paying for, in cash, stock, etc. Other parts are left as an exercise for to-be-determined “users”. So I find it hard to get excited about software unless it is tied to an exciting further project. Even software that comes with new features.

Sometimes sponsors are found to help pay to collect a set of regular users (i.e., info sellers) who talk on a set of regular topics. Sometimes it is the users themselves who are the sponsors, willing to pay in time and money to express their opinions on topics of interest to them. But rarely do such projects put much effort into soliciting participation and support from particular info buyers, choosing topics close to their key decisions. And, alas, the rare projects that at least pitch to potential info buyers tend to pick system designs sensitive to management quality, and less clearly robust to manipulation efforts.

Yet to my mind it is the info buyers who should come first in info market project planning. Info sellers are second, and software last. First find a set of estimates that would be useful in advising some set of important decisions. Especially where there’s a plausible trust advantage from widely-credible estimates, so that key audiences can better trust decision makers. Find parties to whom more credible accuracy would be valuable, and ask them how much they’d be willing to pay for it. They don’t need to be convinced of such accuracy in the start, but they do need to be willing to pay once sufficient accuracy is demonstrated. If you can’t find info buyers, you can’t make info markets.

Yes, when many potential info buyers want similar info, they can each be tempted to free ride on the efforts of others. So it makes sense to look more to cases where info gains are concentrated in a few parties. Alas, an even larger obstacle to finding info buyers is that we often justify our activities in terms of info collection and processing, when those activities are better described as local politics. We pretend to want accurate info far more often than we actually do.

I’m quite willing to work with most any group that seems to have at least a chance of putting together all the needed parts. But my best guess for the most promising project is still the one I first [posted](if-i-had-a-mill) on over 24 years ago: fire the CEO markets re the Fortune 500. I doubt I have another 24 years, so I do hope someone tries this before then. For this project the plausible info buyers are firm investors, represented by the board of directors, who subsidize these markets. Likely info sellers are stock analysts and stock traders, who would profit from trading in these markets.
Simple money-based conditionally-called-off stock markets should produce bias-robust widely-credible estimates, at least if trading liquidity is high enough. That has been a widely shared belief on speculative financial markets for many decades. To get high liquidity, use large market-maker based subsidies on only a few firms to start with, firms chosen via a prize system as most likely to see fire-CEO recommendations. Once these prices get enough attention, especially from CEOs trying to manipulate them to make themselves look good, their liquidity can be self-reinforcing, and subsidies can be transferred to the next set of firms.

Yes, this fire-the-CEO project faces substantial legal obstacles if anyone is allowed to participate; may have to do this one offshore. Legal issues are much less of a problem for most projects that ask firm employees and contractors to advise firm decisions, as the firm can pay for their initial stake. For those projects the main obstacle is political disruption; existing players in the firm tend to be bothered to see their advice contradicted by a system with higher proven accuracy.

Of course I can get excited by a great many other project concepts; I’ve posted on many here over the years. But to get excited about an info market concept, I need to at least hear about the intended info buyers willing to pay to get bias-robust widely-credible expertise. A mere project to develop software, or even to collect a regular set of users, not so much.

## [Prediction Markets Need Trial and Error](#table-of-contents)
_Posted on 2022-05-15_

We economists have a pretty strong consensus on a few key points: 1) innovation is the main cause of long-term economic growth, 2) social institutions are a key changeable determinant of social outcomes, and 3) inducing the collection and aggregation of info is one of the key functions of social institutions. In addition, better institutional-methods for collecting and aggregating info (ICAI) could help with the key meta-problems of making all other important choices, including the choice of our other institutions, especially institutions to promote innovation. Together all these points suggest that one of the best ways that we today could help the future is to innovate better ICAI.

After decades pondering the topic, I’ve concluded that prediction markets (and closely related techs) are our most promising candidate for a better ICAI; they are relatively simple and robust with a huge range of potential high-value applications. But, alas, they still need more tests and development before wider audiences can be convinced to adopt them.

The usual (good) advice to innovators is to develop a new tech first in the application areas where it can attract the highest total customer revenue, and also where customer value can pay for the highest unit costs. As the main direct value of ICAI is to advise decisions, we should thus seek the body of customers most willing to pay money for better decisions, and then focus, when possible, on their highest-value versions.

Compared to charities, governments, and individuals, for-profit firms are more used to paying money for things that they value, including decision advice. And the decisions of such firms encompass a large fraction, perhaps most, of the decision value in our society. This suggests that we should seek to develop and test prediction markets first in the context of typical decisions of ordinary business, slanted when possible toward their highest value decisions.

The customer who would plausibly pay the most here is the decision maker seeing related info, not those who want to lobby for particular decisions, nor those who want to brag about how accurate is their info. And they will usually prefer ways to elicit advice from their associates, instead of from distant curated panels of advisors.

We have so far seen dozens of efforts to use prediction markets to advise decisions inside ordinary firms. Typically, users are satisfied and feel included, costs are modest, and market estimates are similarly or substantially more accurate than other available estimates. Even so, experiments typically end within a few years, often due to political disruption. For example, market estimates can undermine manager excuses (e.g., “we missed the deadline due to a rare unexpected last-minute problem”), and managers dislike seeing their public estimates beaten by market estimates.

Here’s how to understand this: “Innovation matches elegant ideas to messy details.” While general thinkers can identify and hone the elegant ideas, the messy details must usually come from context-dependent trial and error. So for prediction markets, we must search in the space of detailed context-dependent ways to structure and deploy them, to find variations that cut their disruptions. First find variations that work in smaller contexts, then move up to larger trials. This seems feasible, as we’ve already done so for other potentially-politically-disruptive ICAI, such as cost-accounting, AB-tests, and focus groups.

Note that, being atheoretical and context-dependent, this needed experimentation poorly supports academic publications, making academics less interested. Nor can these experiments be enabled merely with money; they crucially need one or more organizations willing to be disrupted by many often-disruptive trials.

Ideally those who oversee this process would be flexible, willing and able as needed to change timescales, topics, participants, incentives, and who-can-see-what structures. An d such trials should be done where those in the org feel sufficiently free to express their aversion to political disruption, to allow the search process to learn to avoid it. Alas, I have so far failed to persuade any organizations to host or fund such experimentation.

This is my best guess for the most socially valuable way to spend ~<$1M. Prediction markets offer enormous promise to realize vast social value, but it seems that promise will remain only potential until someone undertakes the small-scale experiments needed to find the messy details to match its elegant ideas. Will that be you?

## [New-Hire Prediction Markets](#table-of-contents)
_Posted on 2022-05-16_

In my [last post](https://www.overcomingbias.com/2022/05/prediction-markets-need-experimentation.html), I suggested that the most promising place to test and develop prediction markets is this: get ordinary firms to pay for mechanisms that induce their associates to advise their key decisions. I argued that what we need most is a regime of flexible trial and error, searching in the space of topics, participants, incentives, etc. for approaches that can add value here while avoiding the political disruptions that have plagued previous trials.

If you had a firm willing to participate in such a process, you’d want to be opportunistic about the topics of your initial trials. You’d ask them what are their most important decisions, and then seek topics that could inform some of those decisions cheaply, quickly, and repeatedly, to allow rapid learning from experimentation. But what if you don’t have such a firm on the hook, and instead seek a development plan to attract many firms?

In this case, instead of planning to curate a set of topics specific to your available firm, you might want to find and focus on a general class of topics likely to be especially valuable and feasible in roughly the same way at a wide range of firms. When focused on such a class, trials at any one firm should be more informative about the potential for trials at other firms.

One plausible candidate is: deadlines. A great many firms have projects with deadlines, and are uncertain on if they will meet those deadlines. They should want to know not only the chance of making the deadline, but how that chance might change if they changed the project’s resources, requirements, or management. If one drills down to smaller sub-projects, whose deadlines tend to be sooner, this can allow for many trials within short time periods. Alas, this topic is also especially disruptive, as markets here tend to block project managers’ favorite excuses for deadline failure.

Here’s my best-guess topic area: new hires. Most small firms, and small parts of big firms, hire a few new people every year, where they pay special attention to comparing each candidate to small pool of “final round” candidates. And these choices are very important; they add up to a big fraction of total firm decision value. Furthermore, most firms also have a standard practice of periodically issuing employee evaluations that are comparable across employees. Thus one could create prediction markets estimating the N-year-later (N=2?) employee evaluation of each final candidate, conditional on their being hired, as advice about whom to hire.  
Yes, having to wait two years to settle bets is a big disadvantage, slowing the rate at which trial and error can improve practice. Yes, at many firms employee evaluations are a joke, unable to bear any substantial load of criticism or attention. Yes, you might worry about work colleauges trying to sabotage the careers of new hires that they bet against. And yes, new hire candidates would have to agree to have their application evaluated by everyone in the potential pool of market participants, at least if they reach the final round.

Even so, the value here seems so large as to make it well worth trying to overcome these obstacles. Few firms can be that happy with their new hire choices, reasonably fearing they are missing out on better options. And once you had a system working for final round hire choices, it could plausibly be extended to earlier hiring decision rounds.

Yes, this is related to my proposal to use prediction markets to fire CEOs. But that’s about firing, and this is about hiring. And while each CEO choice is very valuable, there is far more total value encompassed in all the lower personnel choices.

## [Shoulda-Listened Futures](#table-of-contents)
_Posted on 2021-04-27_

Over the decades I have written many times on how prediction markets might help the intellectual world. But usually my pitch has been to those who want to get a better actionable info out of intellectuals, or to help the world to make better intellectual progress in the long run. Problem is, such customers seem pretty scarce. So in this post I want to outline an idea that is a bit closer to a business proposal, in that I can better identify concrete customers who might pay for it.

For every successful intellectual there are (at least) hundreds of failures. People who started out along a path, but then were not sufficiently rewarded or encouraged, and so then either quit or persisted in relative obscurity. And a great many of these (maybe even a majority) think that the world done them wrong, that their intellectual contributions were underrated. And no doubt many of them are right. Such malcontents are my intended customers.

These “world shoulda listened to me” customers might pay to have some of their works evaluated by posterity. For example, for every $1 saved now that gains a 3% real rate of return, $19 in real assets are available in a century to pay historians for evaluations. At a 6% rate of return (or 3% for 2 centuries), that’s $339. Furthermore, if future historians needed only to randomly evaluate 1% of the works assigned them, then if malcontents paid $10 per work to be maybe evaluated, historians could spend $20K (or $339K) per work they evaluate. Considering all the added knowledge and tools to which future historians may have access, that seems enough to do a substantial evaluation, especially if they evaluate several related works at the same time.

Given a substantial chance (1% will do) that a work might be evaluated by historians in a century or two, we could then create (conditional) prediction markets now estimating those future evaluations. So a customer might pay their $20 now, and get an immediate prediction market estimate of that future evaluation for their work. That $20 might pay $10 for the (chance of a) future evaluation and another $10 to establish and subsidize a prediction market over the coming centuries until resolution.

Finally, if customers thought market estimate regarding their works looked too low, then they could of course try to bet to raise those estimates. Skeptics would no doubt lie waiting to bet against them, and on average this tendency of authors to bet to support their works would probably subsidize these markets, and so lower the fees that the system needs to charge.

Of course even with big budgets for evaluations, if we want future historians to make reliable enough formal estimates that we can bet on in advance, then we will need to give them a well-defined-enough task to accomplish. And we need to define this task in a way that discourages future historians from expressing their gratitude to all these people who funded their work by giving them all an A+.

I suggest we have future historians estimate each work’s ideal attention: how much attention each particular work should have been given during some time period. So we should pick some measure of attention, a measure that we can calculate for works when they are submitted, and track over time. This measure should weigh if the dissertation was approved, the paper was published and where, how many cites did it get, etc. If we add up all the initial attention for submitted works, then we can assign historians the task of (counterfactually) reallocating this total attention across all the submitted works. So to give more attention to some, they’d have to take away attention from others.

Okay, so now they can’t give every work an A+. (And we ensure that bet assets have bounded values.) But our job isn’t done. We also need to give them a principle to follow when allocating attention among all these prior works. What objective would they be trying to accomplish via this reallocation of attention?

I suggest that the objective just be intellectual progress, toward the world having access to more accurate and useful beliefs. A set of works should have gotten more attention if in that case the world would have been more likely to have more quickly come to appreciate valuable truths. And this task is probably easier if we ask future historians to use their future values in this task, instead of asking them to try to judge according to our values today.

These evaluation tasks probably get easier if historians randomly pick related sets of works to evaluate together, instead of independently picking each work to evaluate. And this system can probably offer scaled fees, wherein the chance that your work gets evaluated rises linearly with the price you paid for that chance. There are probably a lot more details to work out, but I expect I’ve already said enough for most people to decide roughly how much they like this idea.

Once there were many works in this system, and many prediction markets estimating their shoulda-been attention, then we could look to see if market speculators see any overall biases in today’s intellectual worlds. That is, topics, methods, disciplines, genders, etc. to which speculators estimate that the world today is giving too little attention. That could be pretty dramatic and damning evidence of bias, by someone, evidence to which we’d all be wise to attend.

One obvious test of this approach would be to assign historians today the task of reallocating attention among papers published a century or two ago. Perhaps assign multiple independent groups, and see how correlated are their evaluations, and how that correlation varies across topic areas. Perhaps repeating in a decade or two, to see how much evaluations drift over time.

Showing these correlations to potential customers might convince them that there’s a good enough chance that such a system will later correctly vindicate their neglected contributions. And these tests may show good scopes to use, for related works and time periods to evaluate together, and how narrow or broad should be the expertise of the evaluators.

This whole shoulda-listened-futures approach could or course also be applied to many other kinds of works, not just intellectual works. You’d just have to establish your standards for how future historians are to allocate shoulda attention, and trust them to actually follow those standards. Doing tests on works from centuries ago here could also help to show if this is a viable approach for these kinds of works.

<strong>Added 7am 28Apr:</strong> On average more assets will be available to pay for future evaluations if the fees paid are invested in risky assets. So instead of promising a particular percentage chance of evaluation, it may make more sense to specify how fees will be invested, set the (real) amount to be spent on each evaluation, and then promise that the chance of evaluation for each work will be set by the investment return relative to the initial fee paid. Yes that induces more evaluations in state of the world where investments do better, but customers are already accepting a big chance that their work will never be directly evaluated.

## [Brand Truth Narrowly](#table-of-contents)
_Posted on 2022-07-07_

_McDonalds_ is a famous food brand. Not everyone likes what they sell, but many do, and under this brand they can reliably find a kind of food they like at a predictable price which is below their value.

Imagine you were hungry and came across a _Capitalist Food_ joint. You’ve never heard of them before, but their pitch is that you should like them because they are capitalist, and all the best food comes from capitalists. E.g., _McDonalds_. Which is in fact true.

But this should not persuade you much to buy from them. Yes, if you could choose only between a generic capitalist food place and generic non=profit food place, you’d probably do better with the capitalist one. But the reason the best food comes from capitalists is that they usually develop much narrower food brands. Like _McDonalds_.

Imagine that you knew how to make  a better yet cheaper burger. Most people who like burgers and who tried your burger for a half dozen times would conclude that yours are better. But by itself knowing how to make your better burger would not let you profit from selling them. Because you’d also need to create (or merge with) an acceptable brand to go with them.

For example, if your burger was branded with disliked and low status associations, people might avoid it even if your burgers were better. Such as being associated with the Russian side of their war with Ukraine when your customers live in Europe, or with Trump when your customers live in Seattle. Or being associated with insects, such as if your burger meat was made out of them.

Now consider prediction markets. We have good reasons to think that speculative markets are a great way to generate parameter estimates and decision advice. And many good people are now trying to sell this as a truth brand, that is, as a generic way to find truth. They set up a website where such markets can exist, put in a few sample claims, invite folks to suggest more claims, and step back. Somewhat like _Capitalist Food_ as a brand_._ 

But the thing that I’ve long been struggling to explain to these good folks is: _that is too wide a brand to work well._ Few people want truth in general. Yes decision theory says that people want truth near their decisions, and want it more the biggest their decision. But there are many kinds of truths that they positively do not want, and many more truths where their generally positive value for truth is below its cost of production.

In fact, most of the claims on most of these prediction market sites are actually of this sort: general world events, politics, and celebrity gossip topics. Topics where people care a bit about truth, all else equal, but aren’t much willing to pay to improve on the level of truth that results from the usual news, gossip, and punditry on such topics. A few people are willing to pay to gamble on these topics just for fun, and that can support a few small businesses that serve them. But that leaves the huge social potential of prediction markets unrealized.

A related failure happens when other good people see lamentably low levels of truth in public conversations, and decide that their fix is to just think honestly and carefully and tell the truth as they see it. The problem is that their audiences cannot reliably distinguish sources that are actually more accurate due to being truly honest and careful, from the many other sources that look just like those, yet merely like to tell themselves that they are being honest and careful, but are actually motivated and sloppy.

That is, these good people have failed to create a _brand_ to distinguish their superior truth product. Most individually honest and careful people don’t live long enough or have consistent enough reliability to enable most audiences to distinguish them via personal topic-specific brands. So we mainly distinguish them via larger existing truth brands, e.g., via academic or news media brands. But to gain such brand approval, they must make the many usual compromises re honesty and truth that such brands demand.

A solution here I think is: _application-specific prediction market brands_. For example, a brand that specializes in estimating the chance of making project deadlines, sold to orgs that actually want to know if they will make their deadlines. Or a brand that specializes in [estimating](https://www.overcomingbias.com/2022/05/new-hire-prediction-markets.html) the two-year-later employee evaluation that each new hire candidate would have if hired, sold to orgs that actually want to evaluate new hires.

Such brands would [invest](https://www.overcomingbias.com/2022/05/prediction-markets-need-experimentation.html) in early trials, first to learn the many details of how to do these applications well, and then second to collect a track record proving such knowledge. And they would also do what it takes to acquire and maintain whatever prestige associations their customers demand, and to avoid disliked associations that put off customers. Which yes could be a lot more work than just putting up a betting website with a few sample questions on current events.

But this is the work that needs to happen to create narrow-enough truth brands to be useful. Don’t try to sell _Capitalist Food_, but instead create your version of _McDonalds_ in the truth space. Find the particular kinds of truths whose value of use is plausibly more than its cost of production, learn how to increase value and lower costs in that particular area, and then prove your learning to potential customers via a statistically-validated track record.

## [Fixing Academia Via Prediction Markets](#table-of-contents)
_Posted on 2014-05-05_

When I first got into prediction markets twenty five years ago, I called them “idea futures”, and I focused on using them to reform how we deal with controversies in science and academia (see <a href="http://hanson.gmu.edu/gamble.html">here</a>, <a href="http://hanson.gmu.edu/ifextropy.pdf">here</a>, <a href="http://hanson.gmu.edu/infoprize.html">here</a>, <a href="http://hanson.gmu.edu/ifexper.html">here</a>). Lately I’ve focused on what I see as the much higher value application of advising decisions and reforming governance (see <a href="http://hanson.gmu.edu/decisionmarkets.pdf">here</a>, <a href="http://hanson.gmu.edu/impolite.pdf">here</a>, [here](if-i-had-a-mill), <a href="http://hanson.gmu.edu/futarchy.html">here</a>). I’ve also talked a lot lately about what I see as the main social functions of academia (see [here](impressive-power), [here](academic-credit-for-blogs), [here](who-wants-unbiased-journals), [here](academias-function)). Since prediction markets don’t much help to achieve these functions, I’m not optimistic about the demand for using prediction markets to reform academia.
But periodically people do consider using prediction markets to reform academia, as [did](academic-stats-prediction-markets) Andrew Gelman a few months ago. And a few days ago Scott Alexander, who I once praised for his understanding of prediction markets, [posted](hail-scott-siskind) a utopian proposal for using prediction markets to reform academia. These discussions suggest that I revisit the issue of how one might use prediction markets to reform academia, if in fact enough people cared enough about gaining accurate academic beliefs. So let me start by summarizing and critiquing Alexander’s proposal.
Alexander proposes prediction markets where anyone can post any “theory” broadly conceived, like “grapes cure cancer.” (Key quotes below.) Winning payouts in such market suffer a roughly 10% tax to fund experiments to test their theories, and in addition some such markets are subsidized by science patron orgs like the NSF. Bettors in each market vote on representatives who then negotiate to pick someone to pay to test the bet-on theory. This tester, who must not have a strong position on the subject, publishes a detailed test design, at which point bettors could leave the market and avoid the test tax. “Everyone in the field” must make a public prediction on the test. Then the test is done, winners paid, and a new market set up for a new test of the same question. Somewhere along the line private hedge funds would also pay for academic work in order to learn where they should bet.

That was the summary; here are some critiques. First, people willing to bet on theories are not a good source of revenue to pay for research. There aren’t many of them and they should in general be subsidized not taxed. You’d have to legally prohibit other markets to bet on these without the tax, and even then you’d get few takers.

Second, Alexander says to subsidize markets the same way they’d be taxed, by adding money to the betting pot. But while this can work fine to cancel the penalty imposed by a tax, it does <em>not</em> offer an additional incentive to learn about the question. Any net subsidy could be taken by anyone who put money in the pot, regardless of their info efforts. As I’ve discussed often before, the right way to subsidize info efforts for a speculative market is to subsidize a market maker to have a low bid-ask spread.

Third, Alexander’s plan to have bettors vote to agree on a question tester seems quite unworkable to me. It would be expensive, rarely satisfy both sides, and seems easy to game by buying up bets just before the vote. More important, most interesting theories just don’t have very direct ways to test them, and most tests are of whole bundles of theories, not just one theory. Fourth, for most claim tests there is no obvious definition of “everyone in the field,” nor is it obvious that everyone should have opinion on those tests. Forcing a large group to all express a public opinion seems a huge cost with unclear benefits.

OK, now let me review my proposal, the result of twenty five years of thinking about this. The market maker subsidy is a very general and robust mechanism by which research patrons can pay for accurate info on specified questions, at least when answers to those questions will eventually be known. It allows patrons to vary subsidies by questions, answers, time, and conditions.

Of course this approach does require that such markets be legal, and it doesn’t do well at the main academic function of credentialing some folks as having the impressive academic-style mental features with which others like to associate. So only the customers of academia who mainly want accurate info would want to pay for this. And alas such customers seem rare today.

For research patrons using this market-maker subsidy mechanism, their main issues are about which questions to subsidize how much when. One issue is topic. For example, how much does particle physics matter relative to anthropology? This mostly seems to be a matter of patron taste, though if the issue were what topics should be researched to best promote economic growth, decision markets might be used to set priorities.

The biggest issue, I think, is abstraction vs. concreteness. At one extreme one can ask very specific questions like what will be the result of this very specific experiment or future empirical measurement. At the other extreme, one can ask very abstract questions like “do grapes cure cancer” or “is the universe infinite”.

Very specific questions offer bettors the most protection against corruption in the judging process. Bettors need worry less about how a very specific question will be interpreted. However, subsidies of specific questions also target specific researchers pretty directly for funding. For example, subsidizing bets on the results of a very specific experiment mainly subsidizes the people doing that experiment. Also, since the interest of research patrons in very specific questions mainly results from their interest in more general questions, patrons should prefer to directly target the more general questions directly of interest to them.

Fortunately, compared to other areas where one might apply prediction markets, academia offers especially high hopes for using abstract questions. This is because academia tends to house society’s most abstract conversations. That is, academia specializes in talking about abstract topics in ways that let answers be consistent and comparable across wide scopes of time, space, and discipline. This offers hope that one could often simply bet on the long term academic consensus on a question.

That is, one can plausibly just directly express a claim in direct and clear abstract language, and then bet on what the consensus will be on that claim in a century or two, if in fact there is any strong consensus on that claim then. Today we have a strong academic consensus on many claims that were hotly debated centuries ago. And we have good reasons to believe that this process of intellectual progress will continue long into the future.

Of course future consensus is hardly guaranteed. There are many past debates that we’d still find to hard to judge today. But for research patrons interested in creating accurate info, the lack of a future consensus would usually be a good sign that info efforts in that area less were valuable than in other areas. So by subsidizing markets that bet on future consensus <em>conditional</em> on such a consensus existing, patrons could more directly target their funding at topics where info will actually be found.

Large subsidies for market-makers on abstract questions would indirectly result in large subsidies on related specific questions. This is because some bettors would specialize in maintaining coherence relationships between the prices on abstract and specific questions. And this would create incentives for many specific efforts to collect info relevant to answering the many specific questions related to the fewer big abstract questions.

Yes, we’d  probably end up with some politics and corruption on who qualifies to judge later consensus on any given question – good judges should know the field of the question as well as a bit of history to help them understand what the question meant when it was created. But there’d probably be less politics and lobbying than if research patrons choose very specific questions to subsidize. And that would still probably be less politics than with today’s grant-based research funding.

Of course the real problem, the harder problem, is how to add mechanisms like this to academia in order to please the customers who want accuracy, while not detracting from or interfering too much with the other mechanisms that give the other customers of academia what they want. For example, should we subsidize high relevant prestige participants in the prediction markets, or tax those with low prestige?

Those promised [quotes](hail-scott-siskind):<span id="more-30779"></span>
The Angel of Evidence … [is a] centralized nationwide prediction market. Anyone with a theory can list it there. … Suppose you become convinced that eating grapes cures cancer. So you submit a listing to the Angel: “Eating grapes cures cancer”. Probably most people doubt this proposition and the odds are around zero. So you do some exploratory research. You conduct a small poorly controlled study of a dozen cancer patients. … Gradually a couple of people … make bets … maybe saying there’s only a 10% chance that you’re right, but it’s enough. The skeptics, and there are many, gladly bet against them, hoping to part gullible fools from their money. …

These research prediction markets are slightly negative-sum. Maybe the loser loses $10, but the winner only gets $9. When enough people have bet on the market, the value of this “missing money” becomes considerable. This is the money that funds a confirmatory experiment. … Suppose the experiment returns positive results. … Either everyone is entirely convinced that grapes cure cancer. … Or the controversy continues, … [and] a bet can be placed on the prediction market for the success or failure of a replication. …

Who is going to bet for or against the proposition that the Higgs boson has a mass greater than 140 GeV? Only a couple of physicists even understand the question, and physicists as a group don’t command large sums of spare capital. So what happens is that scientific bodies – the Raikothin equivalent of our National Science Foundation – subsidize the prediction markets. … They donate $1 million to the Angel of Evidence to make the prediction market more lucrative. Suddenly the market is positive-sum; maybe you lose $10 if you’re wrong, but gain $11 if you’re right. The lure of free money is very attractive. … “Science hedge funds” would try to figure out what mass the Higgs boson is likely to have, knowing they will win big if they’re right. Although the National Science Fund type organization funds the experiments indirectly, it is the money of these investors that directly goes to CERN to buy boson-weighing machinery. ..

How are the actual experiments conducted? … Having any strong opinion on the issue at hand is immediate disqualification for a consultant scientist to perform a confirmatory experiment. The consultant scientist is selected by the investors in the prediction market. Corporate governance type laws are used to select a representative from both sides. … Then they will meet together and agree on a consultant. If they cannot agree, sometimes they will each hire their own consultant scientist and perform two independent experiments, with the caveat that a result only counts if the two experiments return the same verdict. …

<p style="padding-left: 30px;">The consultant … decides upon an experimental draft and publishes it in a journal. … It is the exact published paper that will appear in the journal when the experiment is over, except that all numbers in the results section have been replaced by a question mark. … First, investors get one final chance to sell their bets or bow out of the experiment without losses. … This decreases the amount of money available for the experiment. That comes out of the consultant scientist’s salary, giving her an incentive to make as few people bow out as possible. … Second, everyone in the field is asked to give a statement (and make a token bet) on the results. This is the most important part. .. When the draft is published, if you think there are flaws in the protocol, you speak then or forever hold your peace. ([more](hail-scott-siskind))

## [Intellectual Prestige Futures](#table-of-contents)
_Posted on 2022-04-16_

As there’s been an uptick of interest in prediction markets lately, in the next few posts I will give updated versions of some of my favorite prediction market project proposals. I don’t own these ideas, and I’d be happy for anyone to pursue any of them, with or without my help. And as my [first](http://hanson.gmu.edu/gamble.html) reason to consider prediction markets was to reform academia, let’s start with that.

Back in 2014, I [restated](https://www.overcomingbias.com/2014/05/how-to-reform-academia-with-prediction-markets.html) my prior proposals that research patrons subsidize markets, either on relatively specific results likely to be clearly resolved, such as the mass of the electron neutrino, or on simple abstract statements to be judged by a distant future consensus, conditional on such a consensus existing. [Combinatorial](https://www.overcomingbias.com/2012/07/we-can-do-low-treewidth-combinatorial-prediction-markets.html) markets connecting abstract questions to more specific ones could transfer their subsidizes to those the latter topics.

However, I fear that this concept tries too hard to achieve what academics and their customers _say_ they want, intellectual progress, relative to what they more _really_ want, namely affiliation with credentialed impressiveness. This other priority better explains the usual behaviors of academics and their main customers, namely students, journalists, and patrons. (For example, it was a bad sign [when](https://www.overcomingbias.com/2019/01/replication-markets-team-seeks-journal-partners-for-replication-trial.html) few journals showed interest in using prediction market estimates of which of their submissions were likely to replicate.) So while I still think the above proposal could work, if patrons cared enough, let me now offer a design better oriented to what everyone cares more about.

I’d say what academics and their customers want more is a way to say which academics are “good”. Today, we mostly use recent indicators of endorsement by other academics, such as publications, institutional affiliations, research funding, speaking invitations, etc. But we claim, usually sincerely, to be seeking indicators of long term useful intellectual impact. That is, we want to associate with the intellectuals about whom we have high and trustworthy shared estimates of the difference that their work will make in the long run toward valuable intellectual progress.

A simple way to do this would be to create markets in assets on individuals, where each asset pays as a function of a retrospective evaluation of that individual, an evaluation made in the distant future via detailed historical analysis. By subsidizing market makers who trade in such assets, we could today have trustworthy estimates to use when deciding which individuals among us we should consider for institutional affiliations, funding, speaking invitations, etc. (It should be easy for trade on assets that merge many individuals with particular features, such as Ph.Ds from a particular school.)

Once we had a shared perception that these are in fact our best available estimates, academics would prefer them over less reliable estimates such as publications, funding, etc. As the value of an individual’s work is probably non-linear in their rank, it might make sense to have people trade assets which pay as a related non-linear function of their rank. This could properly favor someone with a low median rank but high variance in that rank over someone else with a higher median but lower variance.

Why wait to evaluate? Yes, distant future evaluators would know our world less well. But they would know much better which lines of thought ended up being fruitful in a long run, and they’d have more advanced tech to help them study intellectual connections and lineages. Furthermore, compound interest would give us access to a _lot_ more of their time. For example, at the 7% post-inflation [average](http://www.moneychimp.com/features/market_cagr.htm) return of the S&P500 1871-2021, one dollar becomes one million dollars in 204 years. (At least if the taxman stays aside.)

Furthermore, such distant evaluations might only be done on a random fraction, say one percent, of individuals, with market estimates being conditional on such a future evaluation being made. And as it is likely cheaper to evaluate people who worked on related topics, it would make sense to randomly pick large sets of related individuals to evaluate together.

Okay, but having ample resources to support evaluations by future historians isn’t enough; we also need to get clear on the evaluation criteria they are to apply. First, we might just ask them to sort a sample of intellectuals relative to each other, instead of trying to judge their overall quality on some absolute scale. Second, we might ask them to focus on an individual’s contributions to helping the world figure out what is true on important topics; being influential but pushing in the wrong directions might count against them. Third, to correct for problems caused by scholars who play organizational politics, I’d rather ask future historians to rate how influential an individual _should_ have been, if others had been a bit more fair in choosing to whom to listen.

The proposal I’ve sketched so far is relatively simple, but I fear it looks too stark; forcing academics to admit more than they’d like that the main thing they care about is their relative ranking. Thus we might prefer to pay a mild complexity cost to focus instead on having future historians rate particular works by intellectuals, such as their journal articles or books. We could ask future historians to rate such works in such a way that the total value of each intellectual was reasonably approximated by the sum of the values of each of their work’s.

Under this system, intellectuals could more comfortably focus on arguing about the the total future impact of each work. Derivatives could be created to predict the total value of all the works by an individual, to use when choosing between individuals. But everyone could claim that is just a side issue, not their main focus.

To pursue this project concept, a good first step would be to fund teams of historians to try to rank the works of intellectuals from several centuries ago. Compare the results of different historian teams assigned to the same task, and have teams seek evaluation methods that can be both reliable and also get at the key questions of actual (or counterfactual) impact on the progress that matters. Then figure out which kinds of historians are best suited to applying such methods, and which funding methods best induce them to do such work in a cost-effective manner.

With such methods in hand, we could with more confidence set up markets to forecast the impact of particular current intellectuals and their works. We’d probably want to start with particular academic fields, and then use success there to persuade other fields to follow their example. This seems easier the higher the prestige of the initial academic fields, and the more open are they all to using new methods.

## [Academic Stats Prediction Markets](#table-of-contents)
_Posted on 2014-03-20_

In a column, Andrew Gelman and Eric Loken <a href="http://www.stat.columbia.edu/~gelman/research/published/ChanceEthics10.pdf">note</a> that academia has a problem:

Unfortunately, statistics—and the scientific process more generally—often seems to be used more as a way of laundering uncertainty, processing data until researchers and consumers of research can feel safe acting as if various scientific hypotheses are unquestionably true.

They consider prediction markets as a solution, but largely reject them for reasons both bad and not so bad. I’ll respond here to their article in unusual detail. First the bad:

Would prediction markets (or something like them) help? It’s hard to imagine them working out in practice. Indeed, the housing crisis was magnified by rampant speculation in derivatives that led to a multiplier effect.

Yes, speculative market estimates were mistaken there, as were most other sources, and mistaken estimates caused bad decisions. But speculative markets were the first credible source to correct the mistake, and no other stable source had consistently more accurate estimates. Why should the most accurate source should be blamed for mistakes made by all sources?

Allowing people to bet on the failure of other people’s experiments just invites corruption, and the last thing social psychologists want to worry about is a point-shaving scandal.

What about letting researchers who compete for grants, jobs, and publications write critical referee reports and publish criticism, doesn’t that invite corruption too? If you are going to forbid all conflicts of interest because they invite corruption, you won’t have much left you will allow. Surely you need to argue that bet incentives are <em>more</em> corrupting that other incentives.<span id="more-30722"></span>

And there are already serious ways to bet on some areas of science. Hedge funds, for instance, can short the stock of biotech companies moving into phase II and phase III trials if they suspect earlier results were overstated and the next stages of research are thus underpowered.

So by your previous argument, don’t you want to forbid such things because they invite corruption? You can’t have it both ways; either bets are good so you want more, or bets are bad so you want less, or you must distinguish the good from the bad somehow.

More importantly, though, we believe that what many researchers in social science in particular are more likely to defend is a general research hypothesis, rather than the specific empirical findings. On one hand, researchers are already betting—not just money (in the form of research funding) but also their scientific reputations—on the validity of their research.

No, the whole problem here that we’d like to solve is that scientific reputations are <em>not</em> tied very strongly to research validity. Folks often gain enviable reputations from publishing lots of misleading research.

On the other hand, published claims are vague enough that all sorts of things can be considered as valid confirmations of a theory (just as it was said of Freudian psychology and Marxian economics that they can predict nothing but explain everything).

Now we have a not-so-bad reason to avoid prediction markets: people are often unclear about what they mean, and they often don’t really want to be clear. And honestly, many of their patrons don’t want them to be clear either. We might create a prediction market on if what they meant will ever be clear. But they won’t want to pay for it, and others paying for it might just be mean.

And scientists who express great confidence in a given research area can get a bit more cautious when it comes to the specifics.

Yeah, that’s the problem with being clear; you might end up being clearly wrong.

For example, our previous ethics column, “Is It Possible to Be an Ethicist Without Being Mean to People,” considered the case of a controversial study, published in a top journal in psychology, claiming women at peak fertility were three times more likely to wear red or pink shirts, compared to women at other times during their menstrual cycles. After reading our published statistical criticism of this study in Slate, the researchers did not back down; instead, they gave reasons for why they believed their results (Tracy and Beall, 2013). But we do not think that they or others really believe the claimed effect of a factor of 3. For example, in an email exchange with a psychologist who criticized our criticisms, one of us repeatedly asked whether he believed women during their period of peak fertility are really three times more likely to wear red or pink shirts, and he repeatedly declined to answer this question.

What we think is happening here is that the authors of this study and their supporters separate the general scientific hypothesis (in this case, a certain sort of connection between fertility and behavior) from the specific claims made based on the data. We expect that, if forced to lay down their money, they would bet that, in a replication study, women in the specified days in their cycle would be less than three times more likely to wear red or pink, compared to women in other days of the cycle. Indeed, we would not be surprised if they would bet that the ratio would be less than two, or even less than 1.5. But we think they would still defend their hypothesis by saying, first, that all they care about is the existence of an effect and not its magnitude, and, second, that if this particular finding does not replicate, the non-replication could be explained by a sensitivity to experimental conditions.

Those authors might well be right that an expected replication ratio of 1.5 does indeed support their key hypothesis of the existence of a substantial effect with a certain sign. This doesn’t seem a reason not to bet on what that replication ratio would be, conditional on a replication being tried. One could also bet on long term consensus opinion on this general hypothesis; not all bets have to be about specifics. One could even bet on if such a long term consensus opinion will ever form.

In addition, betting cannot be applied easily to policy studies that cannot readily be replicated. For example, a recent longitudinal analysis of an early childhood intervention in Jamaica reported an effect of 42% in earnings (Gertler et al., 2013). The estimate was based on a randomized trial, but we suspect the effect size was being overestimated for the usual reason that selection on statistical significance induces a positive bias in the magnitude of any comparison, and the reported estimate represents just one possible comparison that could have been performed on these data (Gelman, 2013a). So, if the study could be redone under the same conditions, we would bet the observed difference would be less than 42%. And under new conditions (larger-scale,modern-day interventions in other countries), we would expect to see further attenuation and bet that effects would be even lower, if measured in a controlled study using pre-chosen criteria. Given the difficulty in setting up such a study, though, any such bet would be close to meaningless. Similarly, there might be no easy way of evaluating the sorts of estimates that appear from time to time in the newspapers based on large public-health studies.

Bets on new studies might be “meaningless” for evaluating old studies – but we should care more about evaluating policy than about evaluating studies. If there will be <em>some</em> large studies conducted in the future, prediction market estimates of their future estimate values could be very useful to inform policy, even more useful than the actual estimates those studies will find.

That said, scientific prediction markets could be a step forward, just because it would facilitate clear predictive statements about replications. If a researcher believes in his or her theory, even while not willing to bet his or her published quantitative finding would reappear in a replication, that’s fine, but it would be good to see such a statement openly made. We don’t know that such bets would work well in practice—the biggest challenge would seem to be defining clearly enough the protocol for any replications—but we find it helpful to think in this framework, in that it forces us to consider, not just what is in a particular past data set, but also what might be happening in the general population.

On that, we can mostly agree.

## [How To Fund Prestige Science](#table-of-contents)
_Posted on 2018-11-10_

How can we best promote scientific research? (I’ll use “science” broadly in this post.) In the usual formulation of the problem, we have money and status that we could distribute, and they have time and ability that they might apply. They know more than we do, but we aren’t sure who is how good, and they may care more about money and status than about achieving useful research. So we can’t just give things to anyone who claims they would use it to do useful science. What can we do? We actually have many options.<span id="more-31927"></span>

A relatively easy case is science that might be useful for improving a product or service in the near future. In this case providers of such goods or services can have good incentives to find the best people and to give them good incentives to help improve their offerings. At least this can work well if researchers can keep their improvements secret within their organizations for a long enough duration.

A related case is where we can generally distinguish discrete inventions (e.g., creations, insights, or techniques) that have commercial value, and where we can identify which small group is responsible for creating each one. In this case, we can set up a system of intellectual property, wherein we give each inventor a property right to control the use of their invention. This can create incentives to develop commercially-valuable inventions that are hard to keep secret for long, and this works even when providers are bad at identifying who to hire how to create such improvements. This does, however, impose substantial transaction and enforcement costs.

Another relatively easy case is where we have something particular that we want accomplished, like figuring out how to find a ship’s <a href="https://en.wikipedia.org/wiki/Longitude_rewards">longitude</a> at sea. At least this is easy when only one small group is responsible for each accomplishment, we can later verify both that the accomplishment happened and identify who caused it. In this case we can offer a prize to whomever causes this accomplishment. Prizes [were](prizes_versus_g) <a href="http://mason.gmu.edu/~rhanson/whygrant.pdf">actually</a> a far more common method of science funding centuries ago, during and soon after the scientific revolution.
Another very common method from centuries ago was to subsidize complements to scientific research, complements not very useful for other purposes. Long ago science patrons would subsidize scientific journals, libraries, meeting places, equipment, etc. Similarly, today governments often offer research tax credits. This method requires that someone else also fund science via better targeted incentives. For example, long ago many aristocratic scientists funded their own research.

Sometimes what we want are not particular accomplishments, but accurate estimates on particular questions, and increases in that accuracy over time. For example, we might want to know if most dark matter is made of axions, or if adopting the death penalty reduces the crime rate. We want the best current estimate on such things, and then want to learn more over time. For questions in areas where there’s a decent chance that we eventually know and agree on the answers, subsidized prediction markets <a href="http://hanson.gmu.edu/infoprize.html">can</a> <a href="http://hanson.gmu.edu/ifexper.html">both</a> induce accurate-as-possible current estimates, and robust incentives to increase that accuracy over time, even when credit for such increases should be spread widely over thousands of contributors. It is enough that each trader is reward for moving the price toward its final more-accurate position. Combinatorial versions <a href="http://www.scicast.org">can</a> <a href="https://www.jair.org/index.php/jair/article/view/11249">allow</a> added info on any one question to propagate efficiently to update all the other questions. These methods haven’t been used much so far, but their potential remains large.

For many topics and areas of science, however, the above methods seem awkward and difficult to apply. If these methods were all we had, then of course we’d make do as best we could. But in fact we have another general way that we use to promote science, one that actually gets disproportionate status and attention. Students, media interviewers, and grant funders all crave public affiliation with prestigious scientists, and in effect pay scientists and their institutions for such affiliation. Scientists and the larger world collectively create prestige rankings of papers, people and institutions, including schools, journals, media, and patrons. In this context, individual scientists try to do impressive things, including scientific research, to gain prestige. They also play politics, in order to slant the prestige consensus in their favor.

This prestige system has many problems. For example, mutual admiration societies often form to reinforce each others’s prestige votes on each other, even when these groups don’t actually seem very impressive to outsiders. Even so, this system has long been big and influential, and it can claim substantial credit for many of our most famous and lauded scientific achievements.

In the rest of this post I will make a proposal that I hope can improve this general prestige system. I don’t propose to replace the other better grounded ways to fund science that I described above. If you can use one of them, you probably should. But the demand for prestige affiliation is strong, and may never go away, so let’s see if we can’t find a better way to harness it to encourage scientific research.

I don’t propose to turn the focus of this prestige system away from prestige; that seems a bridge too far. I propose to instead turn its attention toward a better proxy of true prestige: careful future historian evaluations of deserved prestige. Today, we make crude prestige judgments based on past accomplishments and past prestige judgments by others. But we should see these judgments as noisy and error-prone, relative to what future historians could produce if many good ones spent a lot of time looking at any one paper, person, or institution. When science makes progress, we learn better what science claims to believe, and can then better credit prestige to those who advanced such claims.

In particular, I propose that we create speculative markets which estimate the future prestige of each scientific paper, person, project, and institution, and that we treat market prices socially as our main consensus on how prestigious are such things. The historians who make these judgments will be themselves evaluated by yet further-future historians. Let me explain each of these in turn.

For each scientific paper, there is a (perhaps small) chance that it will be randomly chosen for evaluation in, say, 30 years. If it is chosen, then at that time many diverse science evaluation historians (SEH) will study the history of that paper and its influence on future science, and will rank it relative to its contemporaries. To choose this should-have-been prestige-rank, they will consider how important was its topic, how true and novel were its claims, how solid and novel were its arguments, how influential it actually was, and how influential it could have been had it received more attention.

Different independent groups of SEH using different approaches and unaware of each other might be used, with their median choice becoming the official rank. Large areas of related papers, etc. might be judged together at the same time to reduce judging costs. Future SEH can similarly be asked to rank individual people, projects, or institutions.

Assets are created that pay if a paper is selected to be evaluated, and that pay in proportion to the key prestige parameter: some monotonic function (perhaps logarithm) of the rank of that paper relative to its contemporaries. As the number of contemporary papers is known, the max and min of this prestige parameter is known. Similar assets can be created regarding the prestige of a person, project, or institution.

Using these assets, markets can be created wherein anyone can trade in the prestige of a paper conditional on that paper being later evaluated. Yes traders have to wait a long time for a final payoff. But they can sell their assets to someone else in the meantime, and we do regularly trade 30 year bonds today. Some care will have to be taken to make sure the base “cash” assets that are bet are stable, but this seems feasible.

The prices of such markets are visible and recommended as best current estimates of the prestige of such things. Markets can also be created that estimate prestige conditional on favorable treatment by current institutions, to advise the choices of such institutions. For example, one can estimate the prestige of a paper conditional on it being published in a particular journal, or the prestige of a person condition on that person be hired into a particular job. A journal might publish the submissions expected to be most prestigious, given publication.

Scientific institutions should be embarrassed if market price estimates of prestige differ greatly from other markers of prestige, such as jobs, awards, publications, grants, etc. They should work to reduce these differences, both by changing who gets jobs, grants, etc., and by trading to change market prices. In addition to the usual funding of journals, jobs, grants, libraries, etc., science funding could now also take the form of subsidizing historian judging, subsidizing market trading via market makers, and financing hedge funds to trade in these markets. Science funding could also support the collection of whatever data historians say is useful for making later evaluations. This may include prediction market price histories on the chance that key experiments would replicate, that key claims are true, etc.

The SEH who judge papers, etc. from 30 years ago should themselves be judged by possible re-evaluation attempts by SEH another 30 years into the future, and so on to infinity. That is, there should be market prices estimating how much future SEH would agree with the evaluations of particular current SEH, and those prices should be used to rank and choose SEH today. Ideally most SEH are young and expect to be alive in 30 years when they may be judged, personally have a lot of money and status riding on that judgement, and there’s a big chance (say 1/3) of that happening. Assets for all future evaluations could be currently available to trade, and it should be a scandal to see clear differences in market prestige estimates for the same thing evaluated at different future times. Of course other historians who do ordinary history research could be judged in the usual way.

The approach I’ve just described assumes a common prestige ranking of topics and people across all space and time. In contrast, some funders may want to promote science within a topic area or nation without subjecting their choices to uncertainty about the prestige that generic future SEH will assign their topic area or nation. For example, someone might want to fund work on cancer in the US regardless of how important future SEH judge US cancer to be. If so, one might vary the above approach to tie current choices to market estimates of future historians rankings <em>relative</em> to a particular chosen topic area or nation. Generic SEH can probably produce such rankings at a relatively small additional cost, if they are already doing a general evaluation of some paper, person, project, or institution.

Okay, I’ve outlined a pretty grand vision here. What needs to happen next to explore this idea? One very useful exercise would be to hire historians to try and evaluate and rank random scientific work from 30 to 300 years ago. We’d like to see how such rankings vary with time elapsed, topic areas, effort levels, and different random historian teams assigned. This would give us a much better idea of what timescales and effort levels to try in such a system.

(The 30 year time duration is for illustration purposes. The duration should probably be tied to overall rates of economic and scientific change, and thus get shorter if such rates speed up.)

<strong>Added 13Dec:</strong> I discuss more why this is feasible [here](can-foundational-physics-be-saved).

## [Medical Doubts OpEd](#table-of-contents)
_Posted on 2022-01-21_

<em>An editor asked me to write this OpEd, but then he never responded when I gave it to hm. So I submitted it to several other editors, but now I’m out of contacts to try. So I’m giving up and posting this here:</em><span id="more-33054"></span>

Europeans in 1600 likely prided themselves on the ways in which their “modern” medicine was superior to what “primitives” had to accept. But we today aren’t so sure: seventeenth century medical theory was based on the four humors, and bloodletting was a common treatment. When we look back at those doctors, we think they may well have done more harm than good.

When we look at our own medical practices, however, we tend to be confident we are in good hands, and that the money that goes to buying medical care–in 2020, it was 19.7% of our G.D.P. –is well spent. Most of us know of a family member who credits their life to modern medicine. My own dad said this about his pacemaker, and I, too, am a regular customer: I’m vaccinated, boosted, and recently had surgery to fix a broken arm.

We believe in medicine, and this faith has comforted us during the pandemic. But likewise the patients of the seventeenth century; they could probably also have named a relative cured by bloodletting. Yet health outcomes are typically too random for the experience of one family to justify medical confidence. How do we know our belief is justified?

This might seem like a silly question: in Europe of the seventeenth century, the average lifespan was in the low 30s. Now it’s the low 80s. Isn’t that difference due to medicine? In fact, the consensus is now that historical lifespan gains are better explained by nutrition, sanitation, and wealth.

So let’s turn to medical research. Every year, there are a million new medical journal articles suggesting positive benefits of specific medical treatments. That’s something they didn’t have in the seventeenth century. Unfortunately, we now know the medical literature to be plagued by serious biases, such as data-dredging, p-hacking, selection, attrition, and publication biases. For example, in a recent attempt to replicate 53 findings from top cancer labs, 30 papers could not be replicated due to issues like vague protocols and uncooperative authors, and less than half of the others yielded results like the original findings.

But surely modern science must have some reliable way to study the aggregate value of medicine? Yes, we do. The key is to keep a study so simple, pre-announced, and well-examined that there isn’t much room for authors to “cheat” by data-dredging, p-hacking, etc. Large trials where we randomly induce some people to consume more medicine overall, and then track how their health differs from a control population–those are the key to reliable estimates. If trials are big and expensive enough, with lots of patients over many years, no one can possibly hide their results in a file drawer.

Thankfully, we do have a few such studies. Yes, they have limits. They may not include all patient ages, or all kinds of medical care, and they can only see marginal health effects, of the medicine that some get that others do not. But for now, they are the best we have.

Which brings us to the biggest medical news of the 2021, at least for those less inclined to give medicine the benefit of the doubt. We now have one new such study: the Karnataka hospital insurance experiment. From May 2015 to August 2018, 52,293 non-poor but otherwise typical residents of the Karnataka region of India were randomly assigned to get free hospital insurance, an option to buy such insurance, or a control condition.

While the study saw large effects on hospital insurance purchases and on hospital visits, when looking at 82 health outcome changes over a five-year period the study authors “cannot reject the hypothesis that the distribution of p-values from these estimates is consistent with no differences. (P=0.31)” That is, they saw no net effects; people who got more medicine were not on average healthier.

This result is, alas, consistent with most other high quality randomized aggregate medical experiments. For example, few health effects were seen in the 1974-1982 RAND health insurance experiment on 7700 U.S. residents over 3-5 years each, or in the 2008 Oregon Health Insurance experiment wherein 30,000 of 75,000 Oregon poor were randomly allowed to apply for Medicaid. In both studies, more health care did not translate into more health.

A 2019 U.S. tax notification experiment did, maybe, see an effect. When 0.6 of 4.5 million eligible households were randomly not sent a letter warning of tax penalties, the households warned were 1.1% more likely to buy insurance, and 0.06% less likely to die, over the next two years. Now that last death result was only significant at the 1% level, which is marginal. So there’s a decent chance this study is just noise.

Bottom line: we spend 20% of G.D.P. on medicine, most people credit it for their long lives, and millions of medical journal articles seem to confirm its enormous value. Yet our lives are long for other reasons, those articles often show huge biases, and when we look to our few best aggregate studies to assuage our doubts, they do no such thing. And the biggest news of 2021 is: we now have one more such study.

It seems we have three options: we can stick our head in the sand and ignore this unwelcome news, we can accept the difficult truth that medicine just isn’t that useful, or we can hope there’s some mistake here and check again. A mere 0.1% of U.S. annual medical spending, or $4.2 billion, could fund a far larger experiment, and hopefully settle the matter. What do you choose?

## [Medical Market Failures](#table-of-contents)
_Posted on 2009-07-21_

Academic economists often publish analyses of the consequences of possible policies; in fact, I’d say it is the bread and butter of what economists do.   Such analyses usually rate policies using economist’s standard evaluation criteria: economic efficiency.  When a policy of more government intervention is ranked higher that a policy of less government intervention, that is usually because of an identified “market failure”, i.e., a reason why a low regulation “market” situation would not achieve high economic efficiency.

Some economists also act as pundits, arguing for or against current policy proposals to wider audiences.   Such arguments often draw on a wide variety of kinds of reasoning.  But since these people are economists, and not just pundits, I want to hold them to a higher standard: they should offer <em>market failure</em> arguments for their conclusions, and clearly distinguish these arguments from other reasons.  If a big part of the reason people listen to econo-pundits is that they are economists, having been certified by the economics community as having economic expertise, it would be nice to see signs that they continue to apply their economics expertise to their punditry topics, rather than just using their pulpit to preach policies they like for other reasons.

<span id="more-19096"></span>Econo-pundits have largely met this standard on global warming and carbon regulation; the potential market failure is so obvious: local actions by carbon consumers and producers should ignore global climate effects.  They also meet this standard at least somewhat on the financial crisis, discussing external benefits and costs of spending and regulation.  On medical reform, however, econo-pundits seem to have completely dropped the ball.

All I’ve found is a few posts quoting and responding to Krugman <a href="http://krugman.blogs.nytimes.com/2009/06/28/health-care-is-not-a-bowl-of-cherries/">citing</a> Arrow as showing:

The standard competitive market model just doesn’t work for health care: adverse selection and moral hazard are so central to the enterprise that nobody, nobody expects free-market principles to be enough.

But of course to meet the standard I suggest, it is far from sufficient to rattle off a market-failure buzz-phrase or two; one should offer <em>arguments</em> connecting the specific policies one favors or opposes to the market failures one sees as relevant. And so as not to constrain only pro-intervention econopundits, we should expect anti-intervention econopundits to at least say which plausible market failures come closest to justifying intervention.

Let me now try to meet my own standard, by outlining the main possible medical market failures, and the sorts of policies that such failures might justify.

<ol>
<li><strong>Insurance moral hazard</strong> – Insurance is less attractive because it makes people take less care.  Governments can’t help with this, though via limits on what insurance prices can condition on, they can make it worse.</li>
<li><strong>Insurance adverse selection</strong> – If those who privately know their risks are lower buy less insurance, too little insurance gets bought.  In this case forcing everyone to buy insurance some can improve welfare.  However, it turns out we don’t see this problem when insurers can price based on everything they know; usually low risk folks buy <em>more</em> insurance.  We do see adverse selection, however, when insurers hands are tied, such as by law or in optional group insurance.</li>
<li><strong>General altruism</strong> – If we just generally cared about the welfare of others, such as the poor, we might give them cash; spending the cash to give them medicine instead would be worse.</li>
<li><strong>Health altruism</strong> – If we cared more about others’ health than their happiness, we might tax things that hurt health, like smoking, and subsidize things that help health, like exercise, clean air, etc.  We’d have to do a lot of this before we even considered subsidizing medicine, since the correlation between health and medicine is very low.</li>
<li><strong>Emergency room altruism</strong> – If we just can’t help but provide basic medical services to folks in crisis who show up empty handed to an emergency room, we might reasonably require folks to buy catastrophic insurance to cover such situations.  Since it would cover far less, this insurance would be far cheaper than most medical insurance today.</li>
<li><strong>Medicine altruism</strong> – If we just like knowing that other people can get the medicine they want, regardless of whether it makes them happy or healthy, we might subsidize such medicine for them.</li>
<li><strong>Under-appreciation of  medicine bias</strong> – If in fact most people are irrationally biased to underestimate the value of medicine, it might make sense to subsidize medicine.  Of course in this scenario voters would be unlikely to endorse such a subsidy.  And if anything the bias seems to go the other way, which would more justify taxing medicine.</li>
<li><strong>Leaky info</strong> – If private producers under-provide info on the quality of medical procedures, professionals, or providers, because they have insufficient property rights to the info they create, governments might subsidize such info products.  This includes research.</li>
<li><strong>Large scale economics</strong> – if the efficient scale of some medical production is very large, we might consider allowing very large organizations but regulating their price.</li>
<li><strong>Contracting and agency costs</strong> – organizations give their agents, e.g., employees and contractors, rules and incentives to ensure they will behave as intended.  Contracts between customers and producers must try to consider a wide variety of possible circumstances.   These contracts, rules, and incentives require monitoring and enforcement costs.  Unless government organizations have incentives or contracts available to them unavailable to the private sector, they should have no advantages on these issues.</li>
<li><em>I’ll add more items here as needed.</em></li>
</ol>
The item here that I take most seriously is leaky info, especially on new procedures, including drugs.  Something like medicine altruism is also a big factor, though I see it as signaling loyalty to locals, and so hurting foreigners.

## [Paternalism Is About Status](#table-of-contents)
_Posted on 2019-08-11_

> … children, whom he finds delightful and remarkably self-sufficient from the age of 4. He chalks this up to the fact that they are constantly lied to, can go anywhere and in their first years of life are given pretty much anything they please. If the baby wants the butcher knife, the baby gets the butcher knife. This novel approach may not sound like appropriate parenting, but Kulick observes that the children acquire their self-sufficiency by learning to seek out their own answers and by carefully navigating their surroundings at an early age. … the only villagers whom he’s ever seen beat their children are the ones who left to attend Catholic school. (<a href="https://www.washingtonpost.com/outlook/as-a-language-dies-who-will-mourn-should-anyone/2019/08/08/82758330-9ccc-11e9-9ed4-c9089972ad5a_story.html">more</a>)
> Bofi forager parenting is quite permissive and indulgent by Western standards. Children spend more time in close physical contact with parents, and are rarely directed or punished by parents. Children are allowed to play with knives, machete, and campfires without the warnings or interventions of parents; this permissive patently style has been described among other forager groups as well. (<a href="https://books.google.com/books?id=8CExDwAAQBAJ&amp;pg=PT351&amp;lpg=PT351">more</a>)

Much of the literature on paternalism (including [my paper](a-model-of-paternalism)) focuses on justifying it: how much can a person A be helped by allowing a person B to prohibit or require particular actions in particular situations? Such as parents today often try to do with their children. Most of this literature focuses on various deviations from simple rational agent models, but my paper shows that this is not necessary; B can help A even when both are fully rational. All it takes is for B to sometimes know things that A does not.
However, this focus on justification distracts from efforts to explain the actual variation in paternalism that we see around us. Sometimes third parties endorse and support the ability of B to prohibit or require actions by A, and sometimes third parties oppose and discourage such actions. How can we best explain which happens where and when?

First let me set aside situations where A authorizes B to, at some future date, limit or require actions by A. People [usually](paternalism-can-be-kind-just-not-to-present-you) justify this in terms of self-control, i.e., where A today disagrees with future A’s preferences. To me this isn’t real paternalism, which I see as more essentially about the extra info that B may hold.
Okay, let’s start with a quick survey of some of the main observed correlates of paternalism.<span id="more-32178"></span>

<ol>
<li>While forager parents were not very paternalistic toward kids, farmer and industry era parents have become much more paternalistic, and are especially so toward younger kids. To a lessor extent, we are also somewhat paternalistic toward the elderly, especially older ones. Being economically self-sufficient often triggers much less paternalism.</li>
<li>Much of the law, including property, contract, and tort, functions to prevent people from forcing others to follow their advice. Paternalism is an exception to this more general and typical rule.</li>
<li>We are much more sympathetic toward paternalism regarding the [poor](naked-classism), stupid, and low educated. Rich enough people are excused from investment regulations, for example. Many regulations only apply to those who can’t manage the complex paperwork needed to excuse them, and others can be relatively easily evaded by the clever. Many regulations are hardly enforced at all, and thus only punish those with too little social intelligence to notice this fact, or too much sincerity to act on it.</li>
<li>We tend to be more paternalistic regarding consumer choices in response to business, especially large businesses, relative to choices that respond to others. For example there’s far less support for paternalism regarding choices in response to offers by charities, religions, [schools](why-college-cores), clubs, friends, family, and lovers.</li>
<li>Many of the following can be seen, at least in part, as paternalism by governments re customers: product &amp; service regulation, investment regulation, building codes, professional licensing, limits on who voters can elect, limits on allowed contracts, obstacles to emmigration, and banned media.</li>
<li>Paternalism is sometimes justified in terms of emotional irreversible choices by the young, choices that have large consequences made under great ignorance. Yet in our society we see less inclination for paternalism regarding lovers and marriage, moving to new cities or nations, or [pursuing](why-no-job-paternalism) risky careers such as in music, art, or acting.</li>
<li>Health and safety are especially popular rationales for paternalism, and yet there’s little regulation of quite dangerous sports like mountain climbing, or [base jumping](inconsistent-pa). About 1% of those who climb Everest die, for example.</li>
<li>My wife is a social worker, and she tells me that doctors are the medical professionals who treat patients most paternalistically. Some just declare their advice without even giving reasons, and doctors often don’t bother to notice whether patients understand their words. There’s more paternalism by everyone toward poor, stupid, and low educated patients, who get worse outcomes due in part to others not listening as much to them.</li>
<li>Social workers are relatively low status among medical professionals, and they take the most effort to ask patients what they want, to present advice in terms of options, and to explain advice in simple words. Medical patients often complain of feeling out of control, having so little control of their lives due to medical treatment. Social workers help patients focus on life areas where they retain more control. Patients are often “non-compliant”, due in part to wanting to assert control, and in part to not understanding medical instructions.</li>
<li>When a top executive announces that they will retire in six months, all the sudden people [stop](advice-isnt-about-info) asking for meetings with them to ask them for advice.</li>
<li>In general, people seem eager to give advice to others, and are often reluctant to be seen as following advice, especially from rivals or lower status people. People often prefer advice from high status people like celebrities, relative to those who who have more expertise in the topic area.</li>
<li>Rules re who voters can elect or listen to [tend to](good-bad-ugly-leaders) elevate locals and the old, and to punish the young, foreigners, and business.</li>
</ol>
Okay, there’s a lot of variation here to be explained, and no doubt many factors contribute. Even so, we can and should ask: what are the biggest contributions?

The most obvious pattern I see is [social status](puzzling-paternalism). Regarding paternalism by B on A, we are more likely to support paternalism by high status person B regarding a low status person A. And when A’s acton is to choose a person C, an act which benefits C, we are more likely to support paternalism that forces the choice C to be higher status.
Even with simple advice taken voluntarily, the advice giver tends to rise in status and the advice receiver tends to fall in status. It seems plausible that policies which force people to adopt advice can be seen as making this effect stronger. That is, if you are forced to take someone else’s advice, instead of accepting it voluntarily, that lowers your status even more and raises their status even more.

As humans care a lot about the social status of themselves and their allies and rivals, the simplest most straight-forward explanation for status-related paternalism support is that people use paternalism policy as a way to influence who has what status, to show their support for high status folks they hope will become allies. and to distance themselves from the low status.

However, people tend to deny having this motive, and some will argue that we instead use status mainly as a proxy for info and quality. That is, we force people to take the advice of high status people because they are better informed, we don’t force high status people to take advice from others as they are informed well enough, and we force people to pick high status associates because those associates are higher quality.

But this flies in the face of all that we know about the theory of paternalism. First, we know a great many situations in which higher status people are not the best informed about particular actions, and yet our paternalism  policies tend not to make exceptions for such situations. And as high status associates tend to cost more, they aren’t always the most cost-effective associates to choose.

More importantly, our best theories about the process of paternalism, such as [my](a-model-of-paternalism) game theory model, just do not in any clear or direct way support the claim that, to help people A, it is better to empower more informed people B to limit or force the actions of people A, especially when the A tend to be less informed.
You see, paternalism is fundamentally about the choice between recommending and forcing advice, and thus it is about the <em>errors</em> that person A would make regarding the advice by person B in a particular situation. If A correctly estimates how much info is embodied in B’s advice, then A’s action after listening to advice from B will embody that info, leaving no need to force A to take B’s advice. But we know of no simple reliable patterns regarding the errors that A will make regarding B advice, patterns we could use to set helpful paternalism policy. We certainly don’t know of simple patterns in terms of who has more info overall, or who has a higher social status. Most people are presumably well aware of the usual expected correlation between status and info, and so don’t make big errors about that.

As I outlined in my [last post](a-model-of-paternalism), we can say something more general about the usefulness of paternalism in terms of the direction of the differing interests of the two parties. For example, giving B the power to ban an activity is worse for both parties when B thinks that A would pick too little of that kind of activity for any given quality level. But don’t to much to explain our paternalism policies, as those don’t seem to have any obvious relation to indicators of this interest difference sign.
Thus for now, I’ll stick with the simple status story, that we justify paternalism in terms of how it will help A, but actually support paternalism mostly for status reasons: to raise the status of some, lower the status of others, show our support for the high status, and distance ourselves from the low status. Sometimes that happens to help A, and other times it happens to hurt A, but it isn’t fundamentally designed to do either.

Note that paternalism illustrates the complex relation between prestige and dominance, the two subcomponents of status. Freely following someone’s advice would be an affirmation of their prestige, but forcing others to take your advice is an act of dominance. We are apparently often comfortable giving the power of dominance to people who would otherwise be treated as prestigious.

<strong>Added 23Sep:</strong> See also [this](who-vouches-for-you).

## [Rulesy Folks Push Paternalism](#table-of-contents)
_Posted on 2020-09-30_

> “The Tudor landowning justice of the peace (J.P.) was the greatest of of paternalists, rivaled only by the Tudor judges and privy councilors who who controlled the J.P.s. … They wanted to regulate the prices of bread, beer, and wool, the games one played, the amount one drank, the nature of one’s apprenticeship, and the clothes one wore. They arrested drunkards, fined those who did not attend church, and penalized the adulterous. …<span class="Apple-converted-space">  </span>a paternal state … only the 20th century has come to eclipse it” (<a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=Kj-TDAAAQBAJ">more</a>)

I spent most of the day Tuesday reading papers on paternalism, which was the topic of my [job talk](a-model-of-paternalism) paper long ago, and one that I’ve thought a lot about over the years. Alas, almost all writings on the topic seek to argue for or against paternalism, rather than trying to explain it. Now if it were typically efficient, that would in fact be a reasonable explanation. And there are many papers that reasonably argue for the plausible efficiency of mild paternalistic “nudges”, weakly enforced.
But in actual fact we see a huge amount of quite strong paternalism, vigorously enforced. People are greatly discouraged from suicide, and prevented from selling themselves into slavery. Professional licensing limits who can do what, and sex laws limit who can do what with who. Censorship limits what you can read or see. Regulations limit the availability and uses of land, buildings, cars, planes, power plants, food, drugs, and much much more. To prevent “exploitation”, many prices are regulated, purchase is required of schools, doctors, and more. Finally, contract law greatly limited the kinds and levels of penalties that contracts can impose, and the kinds of contracts to which you can agree. And by far the most common rationale offered for all of this is that you are being protected from hurting yourself, not from hurting others.<span class="Apple-converted-space"> </span>

This is [another](checkmate-on-blackmail) one of those subjects where everyone thinks they know the answer, but they all know different answers, almost none of which actually hold up under scrutiny. The most commonly offered explanation is that regulators know more than those they regulate. But then why can’t regulators just tell what they know, such as via very visible certification? If the info for certification is underproduced, why not subsidize it. If it is too easy to forget to check certification, why not offer “<a href="http://mason.gmu.edu/~rhanson/wouldhavebanned.html">would have banned</a>“ stores, where customers must pass a test showing they understand it only sells stuff is otherwise have been banned by regulations.<span class="Apple-converted-space"> </span>
Of course it is plausible that some parties extract big selfish gains from these rules, and we do see <a href="https://www.econlib.org/escaping-paternalism-book-club-part-4/">many examples</a>, such as professionals whose wages are increased via the supply cuts caused by professional licensing. But we need to explain why most everyone <em>else</em> goes along – most actual paternalism is in fact very popular among most people. So for that we’ll need benefits that are much more widely distributed. (In the usual “Bootleggers and Baptists” story, we need to explain the Baptists.)<span class="Apple-converted-space"> </span>

The closest I can find to an efficiency explanation is the idea that people make random but correlated mistakes, at which times they are too proud to listen to advice, and at other times they can’t accept that this might later happen to them. Temporary mistakes are easy to fix by requiring modest waiting periods, and temporary individual mistakes can be fixed by requiring groups of associates to choose something together. (Or equivalently, close associates who can veto individual choices.)

But the hypothesis here is that every once in a while a whole group of associates will all go kinda crazy, a “childish” kind of craziness which may last for quite a while. In this rare but correlated childish-crazy mode, this hypothesis says people tend to be especially unwilling to listen to advice, perhaps out of pride. Maybe they see themselves in a status contest with authorities, and are eager to show independence or defiance. Furthermore, people somehow just can’t accept that this problem might happen later to them, and so aren’t inclined to voluntarily choose to commit ahead of time to some more local paternalistic process which would protect them later.

That’s the best I can come up with, and yes this could in fact explain some paternalism. However, I just can’t see it as sufficient to explain the actual typical huge levels of paternalism that we see. So I must look elsewhere. A year ago, I favored this story:<span class="Apple-converted-space"> </span>

> Thus another possible explanation for min-quality regulations is that, by officially declaring common lower class choices to be bad choices, regulators support upper class claims to be better people. And by forcing everyone to visibly accept this declaration via their not visibly defying the bans, everyone appears to support this<span class="Apple-converted-space"> </span>claim that elite choices are better choices. … Why would so many non-elites support these policies as voters? Plausibly because they aspire to elite status, and by publicly displaying their agreement with elite attitudes, they affirm that they are themselves good candidates for higher status. ([More](quality-regs-say-high-is-good))

Prestige is a key human process, and a key element is that we all seek to copy the behavior of the prestigious, and to associate with them. So a strong eagerness to push everyone to do what elites do, and what they say that one aught to do, seems completely to be expected.<span class="Apple-converted-space"> </span>

Even so, this explanation has still seemed somewhat insufficient to me. There is <em>so</em> <em>so</em> much paternalism! So in this post, let me add one more factor that I think complements the above stories, but also adds substantially to them.<span class="Apple-converted-space"> </span>

The key idea is that there are many “rulesy” people in the world. (Think of Sheldon from <em>Big Bang Theory</em> and Dwight from <em>The Office</em>.) These people specialize in learning of and even creating rules, so that they can then find and reveal violations of these rules around them. This allows them to beat on their rivals, and also to raise their own status. It obviously raises their dominance via the power they wield, but they prefer to be instead seen as prestigious, enforcing rules whose purpose is more clearly altruistic. And what could be more altruistic than keeping people from hurting themselves?<span class="Apple-converted-space"> </span>

So many people who are especially good at noticing and applying rules, good at finding potential violations, good at framing situations as rule violations, and willing to at least gossip about violators, are eager for a supply of apparently-paternalism-motived rules they can enforce. So they take suggestions by elites regarding what is good behavior and work to turn them into rules they can enforce. They push to turn norms into laws, and to make norms out of the weak behavior patterns of elites, or from their patterns of praise and criticism.<span class="Apple-converted-space"> </span>

Now think of the incentives of observers. A declares that B has violated a rule, and audience C has a choice to support A or B in this situation. The rule might be obsolete, A may be stretching its meaning to fit this case, or declaring a new rule from related prior cases. Even so, if B is associated with C, it may seem like corruption for C to support B. If the rule is justified as protecting some folks, then by supporting B you seem to not care about those protected folks. And maybe folks will suspect C of wanting to violate this rule themselves, or of already having violated it. Most of these considerations seem to lean toward supporting A in their case against B.

For example, maybe at first some elites sometimes wear hats. Then they and others start to praise hat-wearers. Then more folks start to wear hats, and get proud of how they are good hat people. Good candidates for promotion to elite they are. Then hat fans start to insinuate that people who don’t wear hats are not the best sort of people in various ways, and are only hurting themselves. They say that word needs to get out about the advantages of hats. And those irresponsible people arguing against hats really need to be dealt with – everyone should be told that their arguments mostly don’t meet the highest possible standards of scientific rigor. (Though neither do most pro-hat arguments.)

It becomes a matter of pride to teach your children to wear hats. And to have hats taught in school. And to include the lack of hats in lists of problems that problem people have. Hat fans start to push the orgs of which they are part to promote hats, sometimes even requiring hats at org functions. Finally it is suggested that wouldn’t it be simpler and more efficient to just have the government require hats. Then foreigners who visit us won’t think we are such backward non-hat people. And its really for their own good, as we all know.

At every step along this path, people can gain by pushing for stricter and stronger hat norms and rules. They are good people, pushing a good thing, which just happens to let them dump harder on rivals. Which is plausibly why we tend to end up with just too many overly restrictive rules. Rules rise with the ratchet of crises that can be blamed on problems said to be fixed by adding new rules. And between the crises, we rarely take away or weaken our rules.<span class="Apple-converted-space"> </span>

This sort of tendency to create excess rules can help to explain why many organizations seems to be afflicted by excess “legalism”, including <a href="https://www.nationalaffairs.com/publications/detail/kludgeocracy-in-america">government</a>.

And I’m not sure exactly how, but I suspect that this process is mutually supportive of processes that push for a lot of discretion in rule enforcement:<span class="Apple-converted-space"> </span>

> To the extent that there are rules, there seems to be a preference for authorizing some people to have discretion to make arbitrary choices, regarding which they are not held strongly to account. … Most people mainly favor discretion … to project to associates an image of being the sort of person who is confidently supports the elites who have discretion, and who expects in general to benefit from their discretion. … The sort of people who are eager to have a fair neutral objective decision-making process tend to be losers who don’t expect to be able to work the informal system of favors well. ([More](simplerules))

## [Universal Basic Dorms](#table-of-contents)
_Posted on 2020-11-11_

Poor people have too little money. So why don’t we just give them more money, instead of giving them many specific things? The main theory in the literature is [paternalism](rulesy-folks-push-paternalism):
> The traditional justification for in-kind transfers has been “paternalism.” … If members of society care about the consumption patterns of the poorest rather than their utility, then the unconstrained choices of the poor may create negative externalities for those who care about them. … while income inequality per se may be acceptable, all individuals should receive adequate food, medical service, and housing. … Parents may not take full account of the utility of their children when making decisions, or they may neglect to factor in externalities. … attempt to redistribute from parents to children within the family. … a sense that the poor cannot be trusted with cash. … it is hard to escape the conclusion that paternalism remains a fundamental underlying rationale for in-kind transfers. (<a href="https://voxeu.org/article/why-kind-benefits">more</a>)

You may not be surprised to hear I have an elaboration of this account based on signaling and status. Let’s start with this result:

> [A] 2010 paper … makes a strong case that in fact the outcome of life satisfaction depends on the incomes of others only via income rank. (Two followup papers find the same result for outcomes of psychological distress and nine measures of health.) They looked at 87,000 Brits, and found that while income rank strongly predicted outcomes, neither individual (log) income nor an average (log) income of their reference group predicted outcomes, after controlling for rank (and also for age, gender, education, marital status, children, housing ownership, labor-force status, and disabilities). ([more](the-model-to-beat-status-rank))

So if satisfaction and preference go simply as income rank, then there is simply no way to to use income redistribution to [help](the-persistence-of-poverty) with such things. Whatever you give to some comes at the expense of others, for no net gain. You can still want to help the poor, but that requires that your concern about their lives not be mediated directly by their satisfaction or preferences. That is, what you want regarding them can’t equal what they want or what makes them happy.
For example, compared to the poor, you might put less weight on their desires to signal, and to rise in status via positional goods (like winning a sporting contest). Because you can see the negative externalities associated with such things. That is, you might put more weight than they do on the remainder of their preferences, which we might call their “direct welfare”.

If there are diminishing returns to direct welfare, then you can want to reduce the overall variance of direct welfare, by giving more of it to the poor, at the expense of the non-poor. But you can reasonably fear that if you just give them cash they will spend too much of that on the kinds of non-direct welfare that have negative externalities. So you can want to constrain their choices, to better ensure that it is direct welfare that you are giving them.

The big problem here is: how to distinguish the goods, i.e., products and services, that put more weight on signaling and status, relative to those that put more weight on direct welfare. The usual political equilibrium is to give the poor the sorts of goods that people tend to praise and admire, at least for others. Like jobs, medicine, education, libraries, art, churches, and fresh vegetables. And to withhold from the poor goods that people tend to criticize and dislike, at least for others. Like parties, drugs, sex, fast food, social media, movies, and video games.

But alas, typical admired goods don’t obviously have smaller positional components, nor do they obviously contribute less to signaling. For example, both education and medicine, widely given to the poor, <a href="http://elephantinthebrain.com">have</a> huge signaling and status components, plausibly even larger than for most goods. If we cannot in practice distinguish the goods that do more to promote direct welfare, we should give up on in-kind transfers and just give the poor cash. And then only to the extent that we think direct welfare has strongly diminishing returns in terms of cash.

I have a (perhaps not original) idea. We have good reasons to [think](what-cost-variety) <a href="https://theconsequentialist.wordpress.com/2020/11/09/competitive-universal-basic-services/">that</a> in general most product and service variety emphasizes signaling and status, relative to standard goods that can achieve large scale economies. So if we can make especially cheap cars, homes, clothes, food, etc. via mass production with a small range of variety, then we should prefer to spend our budget on helping the poor via such standard goods.
That is, assume that we the non-poor have a budget that we are willing to spend on helping the poor. We have two reasons to prefer to spend this budget on standard goods. First, standard goods can be provided much more cheaply, allowing us to give more to each person, or to help more people. Second, because these goods have a lower signaling and status components, the poor who consume them hurt the rest of us less via making us look worse by comparison, and rising in status relative to us. We should thus be wiling to increase the budget that we spend on the poor, in compensation.

Of course the poor may resent this policy, even if it results in larger budgets. Exactly because we choose these standard goods to have lower signaling and positional components, the poor will know that others who see that they are using such standard goods designed for the poor may think less of them as a result. That may not be quite the same as a “stigma” assigned to such goods, but it may have a similar effect. Even so, this looks like an efficient arrangment.

So how exactly can we give standard goods to the poor? There’s an obvious risk that the government, or a charity, managing such poverty assistance might overly micromanage the specific standard goods offered, inducing their design, variations, production, and maintenance. Resulting in greatly increased costs and lower quality, as we’ve often <a href="https://www.theatlantic.com/ideas/archive/2019/11/public-housing-fundamentally-flawed/602515/">seen</a> <a href="http://www.peterdreier.com/wp-content/uploads/2016/06/Public-Housing_What-Went-Wrong.pdf">with</a> public housing.<br/>
So it seems better to just offer the poor credits that they could spend at any competing private supplier of standard goods packages.

That is, let us offer to pay qualifying private “dorms” a fixed budget per resident served per day. Each dorm would give its residents its standard package of a bed, clothes, food, and services for transport, entertainment, schooling, and medicine. Then the qualified poor could choose the dorms they see as offering the best combination of location, other residents, and standard services. If there were many competing dorms, and if residents could change dorms frequently (say at least once a year), competition should force dorms to offer efficient and relatively attractive packages. (We may also need to change zoning and regulation to allow such dorms to be offered.)

History seems to suggest that direct welfare can often be provided more cheaply and reliably via dorm-like living. Such as we’ve seen in colleges, the military, orphanages, hospitals, prisons, and retirement homes. So it doesn’t seem a crazy idea to help the poor live cheaply via dorms as well. But I do see a few complications.

<strong>Limiting Variety</strong> – If not constrained by some rules, competing private dorms won’t necessarily offer standard goods, instead of more varied goods of a lower quality. After all, that is what the poor seem to choose in the market today. So there would have to be some rules enforcing a degree of standardization of the goods offered. Thus there would be limits on the variety that dorms offer in their rooms, clothes, food etc. I don’t have specific proposals regarding these rules to offer now.

<strong>Unusual Variety</strong> – Some of the poor will actually be unusually different from the others, and thus need unusually different services. Such as some disabled folks, or parents with young children. These might be offered something different than the standard options proposed here.

<strong>Added Variety</strong> – Even if we push for less variety and more standardization in what we give the poor, there’s no need to go to extremes on this. Internet connections would offer the usual immense variety of outside sites to which one could connect. And some fraction of the budget paid to dorms might be paid directly to the poor as “allowances,” from which residents could pay for unusual expenses and added variety, such as for some food or clothing occasionally bought outside the dorm. Perhaps the poor who insist on more variety than this dorm system offers could be offered a cash budget to spend on their own, a substantially lower amount than the budget offered to qualified dorms to host them.

<strong>Transitions</strong> – It is probably more expensive for dorms to deal with residents who come and go, relative to residents who stay. So it would probably be better if dorms were paid some extra amount for resident changes. Residents might have a limited budget of such changes, or maybe they’d have to pay for change fees out of an allowance.

<strong>Universal Offer</strong> – I see no reason not to allow dorms to let the non-poor to pay to be residents. In addition, some may argue for paying dorms the same budgets to let anyone stay there, not just the qualified poor. Then anyone could choose to live in these dorms, and save on living expenses. This would be “Universal Basic Dorms” instead of a “Universal Basic Income”. It would be much cheaper than U.B.I., as fewer ordinary people would be willing to stay in such dorms.

<strong>Valuing Neighbors</strong> – If these dorms only housed the qualified poor, then such poor may have a worse pool of associates and role models. Thus it might make sense to pay dorms some added amount per non-poor residents who mingle with their poor residents. This makes sense not only in terms of benefits to the poor, but also in terms of compensating such non-poor residents for the status hit they take by moving to such dorms.

<strong>Crime</strong> – My understanding is that crime is the other big thing that tended to go very wrong with public housing, in addition to mismanagement. So things might go very wrong if dorms were not allowed to reject residents they deem too likely to cause problems. In addition, things would probably go even better with a crime [voucher](who-vouches-for-you) system. Offer each resident a budget to pay a voucher to cover all the crime they may commit as a resident. If they can’t get a voucher to agree at that price, no matter what the other contract terms, such a resident does not qualify for dorm living. This might be a good test environment for a crime voucher system.
And that’s my proposal. Offer to pay dorms per resident who stays there, with rules to encourage less-varied more-standard dorm goods which achieve scale economies. Residents would then get more direct welfare, even if they’d gain less status and send less attractive signals. That lower status should make the rest of us more willing to increase dorm budgets well over the cash budget we might offer the poor to live outside dorms.

## [Elites Must Rule](#table-of-contents)
_Posted on 2022-12-24_

While I’ve spent much of my life doing institution/mechanism design, I’ve only lately come to see that, at least on prestigious topics, most people want relevant institutions to take the following ideal form:

> _Masses recognize elites, who oversee experts, who pick details._

While _experts_ are known by other experts to be knowledgeable and skilled regarding particular topic areas, _elites_ must be widely seen by the masses as having connections and features that are admirable and appropriate for leadership and governance roles. (More on [experts](https://www.overcomingbias.com/2021/02/experts-versus-elites.html) [vs](https://www.overcomingbias.com/2021/02/more-on-experts-vs-elites.html) [elites](https://www.overcomingbias.com/2022/12/yet-more-elites-vs-experts.html).)

For example, with democratic governance the voter masses elect politician elites, who appoint mixed-elite-expert agency heads, who oversee expert agency personnel, who choose details. And debates on who should get to vote can be seen as debates on voter eliteness.

Long ago, it was seen as sufficient for the masses to recognize the natural eliteness of kings and aristocrats, who descended from or choose each other. You might think we are past that now, but academia and medical regulation both work this way. That is, we are all supposed to recognize that top academics and doctors are sufficiently elite to be trusted to run those orgs, and to pick their own successors, all without substantial outside oversight. And actually, this is how regulation works in many prestigious areas in many nations; the elites of each area are given a free hand to run those areas, and choose their successors.

If one sees the typical rich investor as sufficiently elite, then one can accept investors overseeing CEOs who oversee middle managers who oversee line workers, in a progression from relatively elite to relatively expert. The main reason that people object to this arrangement is that they are reluctant to see investors as sufficiently elite; they [prefer](https://www.overcomingbias.com/2021/10/the-profit-socialism-challenge.html) instead to have government officials decide which firms get funding, and to have workers elect their managers.

Most people don’t like direct democracy, and dislike it more more when ordinary voters have more influence over the proposals on which to vote. And most people aren’t actually that comfortable with legal juries, unless jury choices are greatly limited by elite judges, and advised by expert lawyers. The U.S. uses juries today much less than it used to, and most nations have little use for them. It seems that most people instead want something closer to the ideal form described above.

Back in 2003 my [Policy Analysis Market](https://mason.gmu.edu/~rhanson/policyanalysismarket.html) project hit the news, and was immediately killed. While the loudest complaints against it, of sabotage and price manipulation, had little basis in fact, this oft mentioned criticism was solid: such prediction markets would somewhat displace prestigious intelligence elites with more ordinary people. Markets producing better decisions via more accurate estimates was _not_ seen as sufficient justification.

In the CIA today they tolerate internal prediction markets, but only under the rule that market estimates are never to be cited in official reports, which are the coin of prestige in that realm. Having prestigious reports cite prediction markets would let low level CIA folk somewhat displace CIA elites.

Wariness of many of my other institution ideas, such as [tax career agents](https://www.overcomingbias.com/2022/10/new-tax-career-agent-test.html), [life maintenance orgs](https://www.overcomingbias.com/2007/10/buy-health-not.html), and [crime vouchers](https://www.overcomingbias.com/2019/09/who-vouches-for-you.html), might also be attributed to their more overtly distrusting elites in various ways, and displacing the usual elites somewhat by for-profit firms and financial investors. And these are parties that many see as insufficiently elite. Producing more cost-effective outcomes is _not_ seen as sufficient justification.

While I will continue to try to persuade people to weaken this constraint, and to consider institutions that rely less on our just trusting prestigious elites, I will now also try to take this seriously as a design constraint, and design institutions that conform better to the ideal form described above. I’ll start in my [next post](https://www.overcomingbias.com/2022/12/status-app-concept.html).

## [Status App Concept](#table-of-contents)
_Posted on 2022-12-25_

In my [last post](https://www.overcomingbias.com/2022/12/elites-must-rule.html) I suggested that we prefer institutions of this form:

> _Masses recognize elites, who oversee experts, who pick details._

However, our existing methods for doing that first step, _masses recognizing elites_, seem rather limited. _One_ simple method is to inherit a stable social consensus on the relevant weights to give different status markers, such as wealth, birth family, test scores, endorsement of prior elites, or winning between-elite fights. If we agree on such weights, we might quickly agree on who has how much status. Especially when we pick just one of these markers as our main marker of status.

A _second_ method is to use our ancient more complex, opaque, and instinctual human methods of gossip, displays, fights, and other social tricks to come to a shared consensus on who is higher status. As most human communities do in fact come to rough consensus relatively quickly on relative status judgments, humans clearly do have such mechanisms, even we don’t understand them very well. But such gossip, displays, and fights are often _very_ expensive.

A _third_ method is elections, wherein masses choose between elite political candidates, mostly based on the advice of other elites. While electoral systems usually only only have the capacity to set the status of a tiny number of top officials, those few top officials can sometimes set the status of more others below them.

However, it isn’t clear that any of these methods actually give the masses that much influence over elite policy, or even over the relative status of elites. Nor do they seem that great at preventing coalitions of elites from installing themselves as unaccountable dynasties. Nor are they obviously great at picking the best people to be elites. Can we do better?

In this post I will outline a concept for a more fined-grained and decentralized approach: a [status app](https://www.overcomingbias.com/2019/09/status-apps-are-coming.html). Though I haven’t figured out all the details, I’m posting this partial concept to entice you all to help me think about the remaining design issues. And then maybe implement something.

But first, let’s get clear on the relevant standards for evaluating such a proposal. Our other systems for agreeing on status induce great costs, and also suffer from strategic gaming, and a great many personal biases and agendas. Thus a new system doesn’t need to eliminate all such problems to be an improvement. It might be good enough if it just does better re some problems, and not worse re other problems. Furthermore, the first version of such a system needn’t be better than the status quo, if we can use trial and error to improve it, to eventually make a much better system.

Okay, here is my proposed concept. In a new status app, the core action is this: random triples of people X,A,B are selected, and then person X is asked which of the pair A, B they more respect, at least re elite social roles. Their answer is a “status bit”.

In my simplest reference design, the status app just fits a simple statistical model to all of the bits it has seen, a model with parameters that include each person’s current status, and the current info (vs noise) level of each person re their status bits. If these parameter estimates are made made public, the world could use them as input to many other social processes.

For example, the app might compute a status [Elo score](https://en.wikipedia.org/wiki/Elo_rating_system) for each person based on their “wins” vs. “losses” in each of their status bit “contests”. Each person’s info score could then be a time-weighted measure of how well their their status bits predict changes to target Elo scores in the time period after their bits.

Now let’s consider some design issues that might drive us to modify this reference design.

The _first_ issue is what triples X,A,B to use to create status bits. Yes one could choose them completely randomly. But to get bits that better help the app to estimate parameters, it would make sense to slant the triples toward X whose bits are more predictive, and toward A,B pairs whose status estimates are closer to each other. Also, toward triples X,A,B who have closer relations and more similarities to each other. And especially toward situations where X actually sees A and B interact, or sees them act in closely related contexts. Especially situations where status judgments are usually and naturally made.

When we can categorize the context type C for each status bit, it would make sense to have an info parameter for each such type, so that the expected error (squared) for each bit depends on both the particular person X as well as the context type C.

However, the more control that X,A,B have over which triples are evaluated when, the more they will try to game these choices, and the more inclined X might be to sacrifice their info score in order to reward or punish associates. There is thus an open question re what kinds of status judging contexts C to include in this system, and who to let cause or veto each one. Is it sufficient to just include an error adjustment parameter for each different context type C?

A _second_ issue is that even a decent statistical model of these status bits will likely have known errors, inducing participants to try to game them. This certainly happens re Chess Elo scores. If we believe that eventually sufficient data will be collected to make such errors small, so that earlier large errors are mostly temporary, then it may be sufficient to create prediction markets on future Elo estimates, and use current market prices as our best status and info estimates, instead of stat model estimates.

But if we can’t trust model errors to fade away with time, then we might instead want prediction markets that pay out based on random future status bits, using context types that are especially hard to game (as in [this post](https://www.overcomingbias.com/2022/10/complex-impact-futures.html)). This approach forces market traders to suffer higher risks, but is safer re model estimate errors.

A _third_ issue is who is allowed to see what status bits. One extreme is where everyone can see them all, while the opposite extreme is where only bit creators can see them. The closer that X,A,B, are to each other, the more risk there is of inducing problematic behaviors by X,A,B toward each other when they can the details of such bits. But the more people who can see more all the bit details, the better they might be able to correct model estimate errors in prediction market prices.

A _fourth_ issue is whether we can create a decentralized implementation of such an app concept, so that we don’t have to trust some center who might lie about or distort such a system.

A _fifth_ issue is that while asking people who they generally respect more seems a very direct way to elicit general status judgments, what we might really want as data are actions where people reveal who they actually fear (for dominance) or seek to emulate (for prestige). But could we really find a set of actions where (a) we could reliably extract relative status judgments from those acts, without too many other confounding effects, (b) the set of actions covers a wide enough range of status aspects to allow the estimation of general status, and not just one narrow aspect of status, and (c) such actions can be observed often enough to give sufficiently accurate estimates of individual status? This seems hard.

A _sixth_ issue is whether and how to give higher status people more weight in judging relative status. Will their info estimates naturally be higher in the basic stat model, or do we want to favor them more than would be done by this analysis, and if so how?

A _seventh_ issue is that sometimes data may be of the form of (X,Y,…) ranking many people (A,B,…,N). Can this be reduced to many bits of the form of ranking (A,B), or does a stat model need to handle this differently?

An _eighth_ issue is how to merge different kinds of status bits. That is, if X picks the higher of A and B several times re several different aspects of status, should the different kinds of status bits be analyzed separately, or would it be useful to estimate their correlation and use those to estimate each kind of status for each person?

I’ll add more issues here as I think of them.

## [Our Prestige Obsession](#table-of-contents)
_Posted on 2019-06-07_

Long ago our distant ancestors lived through both good times and bad. In bad times, they did their best to survive, while in good times they asked themselves, “What can I invest in now to help me in coming bad times?” The obvious answer was: good relations and reputations. So they had kids, worked to raise their personal status, and worked to collect and maintain good allies.

This has long been my favored explanation for why we now invest so much in medicine and education, and why those investment have risen so much over the last century. We subconsciously treat medicine as a way to show that we care about others, and to let others show they care about us. As we get richer, we devote a larger fraction of our resources to this plan, and to other ways of showing off.

I’d never thought about it until yesterday, but this theory also predicts that, as we get rich, we put an increasing priority on associating with prestigious doctors and teachers. In better times, we focus more on gaining prestige via closer associations with more prestigious people. So as we get rich, we not only spend more on medicine, we more want that spending to connect us to especially prestigious medical professionals.

This increasing-focus-on-prestige effect can also help us to understand some larger economic patterns. Over the last half century, rising wage inequality has been driven to a large extent by a limited number of unusual services, such as medicine, education, law, firm management, management consulting, and investment management. And these services tend to share a common pattern.

As a fraction of the economy, spending on these services has increased greatly over the last half century or so. The public face of each service tends to be key high status individuals, e.g., doctors, teachers, lawyers, managers, who are seen as driving key service choices for customers. Customers often interact directly with these faces, and develop personal relations with them. There are an increasing number of these key face individuals, their pay is high, and it has been rising faster than has average pay, contributing to rising wage inequality.

For each of these services, we see customers knowing and caring more about the prestige of key service faces, relative to their service track records. Customers seem surprisingly disinterested in big ways in which these services are inefficient and could be greatly improved, such as via tech. And these services tend to be more highly regulated.

For example, since 1960, the US has roughly doubled its number of doctors and nurses, and their pay has roughly tripled, a far larger increase than seen in median pay. As a result, the fraction of total income spent on medicine has risen greatly. Randomized trials comparing paramedics and nurse practitioners to general practice doctors find that they all produce similar results, even though doctors cost far more. While student health centers often save by having one doctor supervise many nurses who do most of the care, most people dislike this and insist on direct doctor care.

We see very little correlation between having more medicine and more health, suggesting that there is much excess care and inefficiency. Patients prefer expensive complex treatments, and are suspicious of simple cheap treatments. Patients tend to be more aware of and interested in their doctor’s prestigious schools and jobs than of their treatment track record. While medicine is highly regulated overall, the much less regulated world of animal medicine has seen spending rise a similar rate.

In education, since 1960 we’ve seen big rises in the number of students, the number of teachers and other workers per student, and in the wages of teachers relative to worker elsewhere. Teachers make relatively high wages. While most schools are government run, spending at private schools has risen at a similar rate to public schools. We see a strong push for more highly educated teachers, even though teachers with less schooling seem adequate for learning. Students don’t actually remember much of what they are taught, and most of what they do learn isn’t actually useful. Students seem to know and care more about the prestige of their teachers than about their track records at teaching. College students prefer worse teachers who have done more prestigious research.

In law, since 1960 we’ve similarly seen big increases in the number of court cases, the number of lawyers employed, and in lawyer incomes. While two centuries ago most people could go to court without a lawyer, law is now far more complex. Yet it is far from clear whether we are better off with our more complex and expensive legal system. Most customers know far more about the school and job prestige of the lawyers they consider than they do about such lawyers’ court track records.

Management consultants have greatly increased in number and wages. While it is often possible to predict what they would recommend at a lower cost, such consultants are often hired because their prestige can cow internal opponents to not resist proposed changes. Management consultants tend to hire new graduates from top schools to impress clients with their prestige.

People who manage investment funds have greatly increased in number and pay. Once their management fees are taken into account, they tend to give lower returns than simple index funds. Investors seem willing to accept such lower expected returns in trade for a chance to brag about their association should returns happen to be high. They enjoy associating with prestigious fund managers, and tend to insist that such managers take their phone calls, which credibly shows a closer than arms-length relation.

Managers in general have also increased in number and also in pay, relative to median pay. And a key function of managers may be to make firms seem more prestigious, not only to customers and investors, but also to employees. Employees are generally wary of submitting to the dominance of bosses, as such submission violates an ancient forager norm. But as admiring and following prestigious people is okay, prestigious bosses can induce more cooperative employees.

Taken together, these cases suggest that increasing wage inequality may be caused in part by an increased demand for associating with prestigious service faces. As we get rich, we become willing to spend a larger fraction of our income on showing off via medicine and schooling, and we put higher priority on connecting to more prestigious doctors, teachers, lawyers, managers, etc. This increasing demand is what pushes their wages high.

This demand for more prestigious service faces seems to not be driven by a higher productivity that more prestigious workers may be able to provide. Customers seem to pay far less attention to productivity than to prestige; they don’t ask for track records, and they seem to tolerate a great deal of inefficiency. This all suggests that it is prestige more directly that customers seek.

Note that my story is somewhat in conflict with the usual “skill-biased technical change” story, which says that tech changed to make higher-skilled workers more productive relative to lower-skilled workers.

<strong>Added 10June:</strong> Note that the so-called Baumol “cost disease”, wherein doing some tasks just takes a certain number of hours unaided by tech gains, can only explain spending increases proportional to overall wage increases, and that only if demand is very inelastic. It can’t explain how some wages rise faster than the average, nor big increases in quantity demanded even as prices increases.

<strong>Added 12Jun:</strong> This post inspired by reading & discussing <em><a href="https://www.mercatus.org/system/files/helland-tabarrok_why-are-the-prices-so-damn-high_v1.pdf">Why Are the Prices So Damn High?</a></em>

## [Yay Stability Rents](#table-of-contents)
_Posted on 2017-10-20_

Six years ago [I](office-by-combo-auction) [posted](city-by-combo-auction) [on](em-cities-by-combo-auction) the idea of using combinatorial auctions as a substitute for zoning. Since then, news on how badly zoning has been messing up our economy has only gotten worse. I included the zoning combo auction idea in my book <a href="http://ageofem.com"><em>The Age of Em</em></a>, I’ve continued to think about the idea, and last week I talked about it to several LA-based experts in combinatorial auctions.
I’ve been pondering one key design problem, and the solution I’ve been playing with is similar to a solution that also seems to help with patents. I asked Alex Tabarrok, whose office is next door, if he knew of any general discussion of such things, and he pointed me to a long (110 page) 2016 paper called “<a href="https://academic.oup.com/jla/article/9/1/51/3572441/Property-Is-Only-Another-Name-for-Monopoly">Property is another name for monopoly</a>” by <a href="http://ericposner.com/property-is-another-name-for-monopoly/">Eric Posner</a> and <a href="http://glenweyl.com/research/">Glen Weyl</a>. (See also this <a href="https://www.aeaweb.org/conference/2017/preliminary/paper/iiDK3dnr">technical paper</a>.) And that turned out to be a relatively general argument for using the specific mechanism that I was considering using in zoning combo auctions, get this, as a new standard kind of property right for most everything! Looking for web discussion, I find <a href="https://www.reddit.com/r/neoliberal/duplicates/6cjicl/property_is_only_another_name_for_monopoly/">a</a> <a href="https://ftalphaville.ft.com/2017/06/07/2189793/socialism-with-trolly-characteristics/">few</a> <a href="https://www.bloomberg.com/view/articles/2017-06-07/bank-bailouts-and-property-taxes">critical</a> responses, and one excellent 2014 <a href="http://www.interfluidity.com/v2/4911.html"><em>Interfuildity</em> post</a> on the basic idea. In this post I’ll go over the basic idea and some of its issues, including two that Posner and Weyl didn’t consider.<span id="more-31650"></span>

But let’s start with basics. Imagine that you are the sole power over some new large empty territory, able to do anything you want with it. You will soon invite many people to (pay to) enter your territory, and each of them will need some sort of local property rights to support their activities. So you will need to divide your large territory into many smaller property units. And you will want to divide things well, i.e., to “carve nature at its joints,” so that you can promote the productive use of this territory. After all, if people expect to be more productive in your territory, they’ll pay you more for your properties. Yes, if you bundle things together badly, people might be able to re-bundle them in better ways later. But that could be a slow and expensive process; better to get it right the first time.

For example, you’ll want to put things that need to be coordinated more closely together into the same bundles, and you’ll want boundaries between the units that are easy to monitor and enforce. You may also want each unit to contribute to and be subject to some sort of governance structure, to ensure a rule of law and sufficient production of public goods. Even in the best case of a single owner who can choose any property rules he or she likes, this general problem of designing efficient property rights is complex and hard. But the framework I’ve just outlined is in essence the usual account of ideal property rights within the law & economics field (a subject on which I teach often) .

One important dimension of property design is the strength of the property rights. For example, in land property you might let each owner do absolutely anything they want with their land, or your might limit some activities (like explosives, pollution, and blocking views or sunlight) if neighbors are likely to place a higher value on such limits than each owner would place on their absence. And one key area where property rights can be stronger or weaker is regarding one’s freedom to set a resale price.

When you announce an offer to sell your property at a certain price, you in effect give an option to buy that property to the rest of the world. The world values this option, and values it more the lower is its strike price. Options on properties given to the world at lower prices make it easier for others to buy the properties that they value, and to assemble and recombine property units into new bundles. So when you design a property rights system, you might want to add extra incentives for people to create and maintain lower price offers.

Consider the examples of people who bought internet domain names, like “walmart.com”, early on in the hope of being able to resell them to the big firms who go by those names. Or consider homeowners who demand huge prices to sell their land to someone trying to assemble a large area to build a shopping mall. In these and many other cases, economic harm is done because owners can pretend to value their property for much more than they actually do. This also seems a big problem in combo auctions; one-sided combo auctions, where everyone buys from one seller, seem to work much better than do two-sided auctions, where many sellers can demand very high prices even when their values are much lower.

In 1965 the economist Arnold Harberger (of Harberger triangle fame) published on self-assessment in property taxes. The idea is to let each owner assess their property at any value they want, but that assessed value becomes a price at which they offer to sell their property to anyone. (Harberger didn’t invent the idea; it goes back at least to ancient Rome.) Posner and Weyl consider applying this basic idea to most all property, and call it a “Harberger tax.” That is, a simple way to create an incentive for low offer prices on any kind of property is to require the owner of that property to continuously pay a fee proportional to their posted offer price, i.e., the price they announce at which they will sell their property. The lower their price, the less they pay.

Of course the owner of a property is well advised to not set their price much below the value that they actually have for this property, to avoid regret should someone accept their offer. When people become attached to a property, their value will be well above its market price, and given an honest valuation of it, the chance that it will be sold should be very low.

Now this fee is not mainly intended to raise revenue for redistribution or central services. It is instead intended mostly to promote efficient use of property, so that the best people use each item in the best ways. Because of this, I’d rather not call this fee a “tax”, and so I’ll instead call it an “stability rent.” Just as you can buy something by paying all at once or by making “installment” payments spread out across time, part of this stability rent is payment for the property.  But another part is a price you pay over time to increase your chance of keeping the property. That is, to increase the stability of your relation to this property. You can never guarantee perfect stability, however, as that would require an infinite rent.

At this point you may feel that the more familiar kind of property, where you buy something at once for good and then never have to sell it or pay any property tax on it, is the more “natural” kind. Relative to that, this alternate form may seem to you an unnatural mutant abomination. But note that in fact today you usually have to make continuing payments to preserve most property. You pay for maintenance, cleaning, repair, and for a place to store things. You usually also suffer substantial chances of losing most property, via obsolescence, destruction, misplacement, theft, and legal and government policies, such as eminent domain. And you already assign explicit monetary values to property when you consider if to buy or sell, and how much to insure it for. So these new continuing fees, property loss chances, and assessment tasks are mostly just changes in degree from what you accept now.

You can also think of familiar property as a special case of this new property regime, with the stability rent set to zero. And I am persuaded by the analysis of Posner and Weyl to believe that the most economically-efficient rent level is usually not zero. At least if we can ignore some complications, that I will mention soon. Yes, increasing the fee decreases the incentive of a property owner to maintain and improve that property. But for small fees that loss is outweighed by gains in making property easier to transfer to new owners. (And zoning combo auctions might work better.)

To deal with various problems, Posner and Weyl propose that owners be allowed to update prices at anytime, to lump items into bundles that must be sold together, to specify “non-additive prices on subsets of goods”, to deduct observable investments from rents, and to get up to several months to actually transfer property once sold. They also suggest that potential buyers can pay a small refundable fee to inspect properties, and that some (as yet not worked out) combination of blockchains and IOT (internet of things) be used to allow direct transfer of property control while protecting owner privacy and preventing mischief by officials. They note that private insurance could help people to ensure their ability to make future rent payments, and that private apps could help people to estimate and frequently update their many property values. With everyone always posting prices on all of their property, such apps would have a lot of data to work with.

I’ll also note that previous owners should be liable for any damage to property during a transfer period, and that if stability rents were distributed back to pools of owners with roughly the same wealth and other social characteristics, then individuals would only pay net stability rents when they put higher than average values on their properties, relative to others in their pool. Gramma is then only at risk of losing her longtime home if she values that home much more than do other people much like her.

Yes, you might not like having to pay more in this system to get stability of your property. But that’s because in this system you are paying more of the social cost of stability. Its like how polluters prefer a world where they can pollute for free, and don’t have to pay for the social cost of their pollution. The rest of us who have to breathe their pollution shouldn’t sympathize much with their plight.

Yes, this system adds some complexity to property law, but it can also greatly simplify many areas of law, including property tax assessment, eminent domain, necessity exceptions to contract, accident liability levels, corporate buyouts, and patent and copyright licensing,

Posner and Weyl identify several factors that influence the ideal fee level: property turnover rate, sensitivity of value to owner investment, variance and multimodality of value distribution, and whether value tends to grow or decline over time. Even so,

We would advocate a relatively coarse system with a small number of easily distinguishable categories such as natural resources, equipment, real estate, corporate securities, general personal property, keepsakes, and heirlooms. … Rates within these categories being set per a coarse and easily auditable heuristic, such as the currently observed turnover rate.

They estimate that an ideal fee level to be roughly half the rate of property turnover:

A 2.5% annual rate is likely to be nearly optimal on this basis for a wide range of assets, like factories, natural resources, and houses, where investment plays a significant role but allocation can also be seriously distorted. … [This] would transfer about a third of use value to [rent recipients] … [It] achieves 70-90% of the maximum possible allocative welfare gains and the investment losses erode only 10-20% of these gains. .. [This is] likely to raise .. 10-20% of national income in most developed countries. .. The [purchase] prices of assets would be only a quarter to a half of their current level.

Let me finish this post by highlighting two issues that Posner and Weyl don’t seem to discuss. First, social norms may often arise to treat those who buy property through this system as illicit “scabs” who betray a community. For example, stamp collectors might create a shared social norm to post low official prices on their stamps, and to shun anyone known to have bought stamps via such prices. This coordination could greatly reduce the average stability rents paid by stamp collectors, a gain that comes at the expense of non-stamp-collectors.

Similarly, in some community members might typically put <em>all</em> of their property into one single bundle for one entire price of everything they own. This community might then try to identify and shun anyone known to have purchased such entire bundles. Members of this community might then get into the habit of greatly reducing the prices they put on total bundles, and thus greatly reducing their stability rent payments. If a scab were to actually try to buy such a bundle, the target would correctly feel that this scab threatened to grab a large fraction of the value of everything they own. This target might well respond emotionally and fiercely, and his or her allies might respond similarly. The threat of violence could be very real here.

I thus suggest that the size of property bundles be limited to much smaller scales. After all, it is hard to see many people actually buying such large bundles, and thus hard to see how huge bundles can add much to allocative efficiency. I also suggest that stability rents be refunded via small enough pools of socially-connected people. The idea is for pools to be small enough to discourage groups from coordinating to reduce their payments relative to outsiders, and yet large enough for fees to give individuals strong incentives to reveal their property values.

The second issue I want to highlight is that a second enforcement mechanism exists. The mechanism that Posner and Weyl consider for encouraging people to declare accurate values for their property is the prospect that someone might come along and honestly value it at more than the declared price. In commodity futures markets, however, most traders are speculators who don’t actually intend to take delivery of the commodity traded; they instead hope to get out of the market before the official delivery date, and to profit from having bought low and sold high. Similarly, in this system there could also be speculators who seek to profit from mispricings, but who don’t want to actually take delivery of property.

When the owner A of some property puts a price offer P1 on it, and then B accepts that offer on date D, then A might have a month M until they have to actually make the property available for B to control on date D+M. Before date D+M, it is B who officially owns the property, and by the rules of this system B must offer a new price P2 for anyone else to take. But if C accepts this P2 offer during this period, it makes more sense for C to gain the right to take control of the property from A on date D+M, instead of forcing B to actually take delivery and then have another month to transfer it to C on date D+2M.

But if this is the rule, then an original owner A who had underbid and set price P1 below their actual value V might be tempted to pay a higher price P2 to get their property back again, with P1 < P2 < V. And a speculator might take the role of B hoping for just this outcome, from which they would gain a pure cash profit of P2-P1. Given a chance Q of A having underbid and thus being willing to pay P2 to get it back, taking on such a speculator role to challenge A’s claim of value P1 should on average be profitable if

Q*(P2-P1) > (1-Q)*(T+P1-P0),

where T is the speculator’s transaction cost and P0 is the price they expect to get selling the property to a third party.

People who are in the market to buy a property like this, but who are flexible on which particular property they buy, are in an especially good position to take on this speculation role, as they have an especially low added transaction cost. After all, they were already going to buy and pay a transaction cost. Thus professional speculators and flexible buyers will watch for clues that indicate underbid properties that can be profitably challenged. And if society applies a norm of honestly to value declarations, we may not feel much sympathy for people who are “exploited” due to lying about their values for property.

As a result, owners are induced to limit how far they set prices below their actual values <em>both</em> by the prospect that sincere buyers will arrive who sincerely value the property at more than their declared price, <em>and</em> also by speculators looking to profit from underbids. This means that for any given fee rate, declared values will be more honest. We can thus set fees to <del>lower</del> <em>higher</em> levels than we otherwise might and still get good allocative efficiency.

Overall I find this proposal quite promising, though still underspecified. Someone really needs to work out what sort of blockchain and IOT arrangements might actually be workable here. I’d also like to see something better for dealing with asymmetric info on property features than the inspection proposal given. And of course we should do lab experiments and field trials before we actually adopted such property rules on a large scale.

## [Conditional Harberger Tax Games](#table-of-contents)
_Posted on 2019-02-11_

> Baron Georges-Eugène Haussmann … transformed Paris with dazzling avenues, parks and other lasting renovations between 1853 and 1870. … Haussmann… resolved early on to pay generous compensation to [Paris] property owners, and he did. … [He] hoped to repay the larger loans he obtained from the private sector by capturing some of the increased value of properties lining along the roads he built. … [He] did confiscate properties on both sides of his new thoroughfares, and he had their edifices rebuilt. … Council of State … forced him to return these beautifully renovated properties to their original owners, who thus captured all of their increased value. (<a href="https://www.wsj.com/articles/city-of-light-review-the-man-who-lit-up-paris-11549031581">more</a>)

In my [last post](fine-grain-futarchy-zoning-via-harberger-taxes) I described abstractly how a [system](for-stability-rents) of conditional Harberger taxes (CHT) could help deal with zoning and other key city land use decisions. In this post, let me say a bit more about the behaviors I think we’d actually see in such a system. (I’m only considering here such taxes for land and property tied to land.)
First, while many property owners would personally manage their official declared property values, many others would have them set by an agent or an app. Agents and apps may often come packaged with insurance against various things that can go wrong, such as losing one’s property.

Second, yes, under CHT, sometimes people would (be paid well to) lose their property. This would almost always be because someone else credibly demonstrated that they expect to gain more value from it. Even if owners strategically or mistakenly declare values too low, the feature I suggested of being able to buy back a property by paying a 1% premium would ensure that pricing errors don’t cause property misallocations. The highest value uses of land can change, and one of the big positive features of this system is that it makes the usage changes that should then result easier to achieve. In my mind that’s a feature, not a bug. Yes, owners could buy insurance against the risk of losing a property, though that needn’t result in getting their property back.

In the ancient world, it was common for people to keep the same marriage, home, neighbors, job, family, and religion for their entire life. In the modern world, in contrast, we expect many big changes during our lifetimes. While we can mostly count on family and religion remaining constant, we must accept bigger chances of change to marriages, neighbors, and jobs. Even our software environments change in ways we can’t control when new versions are issued. Renters today accept big risks of home changes, and even home “owners” face big risks due to job and financial risks. All of which seems normal and reasonable. Yes, a few people seem quite obsessed with wanting absolute guarantees on preservation of old property usage, but I can’t sympathize much with such fetishes for inefficient stasis.<span id="more-32032"></span>

Under CHT, property owners would be tempted to declare values below their actual values, to lower their taxes. For example, they might try to estimate a distribution over the second highest value, and trade a gain from lower taxes against a risk of losing their property temporarily and having to buy it back at a higher price. However, as I’ve discussed, speculators would hunt for signs of owners lowballing declared values, and buy such properties in the hope of selling them back to owners at a higher price. In practice, this would ensure declared values didn’t fall much below actual values.

An owner seeking to leave an existing property soon would typically have to offer a substantial discount over typical nearby declared values, in order to entice buyers to take their property in particular. However, an owner seeking to switch properties, but who is flexible on their exact new property, has a comparative advantage in the above described speculation game. That is, they could gain a substantial discount on their purchase via having a sequence of owners pay them to get their properties back, and then finally switching to the property whose owner refuses to buy it back. For example, if 90% of owners will pay 1% of value to get their property back, a flexible buyer can on average gain a 10% price discount.

Now consider some concrete examples of using CHT to change rules limiting property use:

<ol>
<li><strong>Min Yard Variance</strong> – A property owner asks to build a structure closer to a lot line than is usually allowed.</li>
<li><strong>Less Parking Variance</strong> – A store currently required to have X parking spots asks to cut this by Y spots.</li>
<li><strong>New Park</strong> – Make a new park out of what is currently an empty lot. Specifies park quality, entrance fees.</li>
</ol>
Today such decisions tend to be made in part bureaucratically, and in part politically. The bureaucratic part tends to make decisions crude, failing to take into account many relevant details. In contrast, while the political part can consider more detail, it tends to be more random and expensive, an expense that makes politics mostly irrelevant for small changes. Both of these parts consistently create both winners and losers, inducing efforts to “fight” in order to win and not lose. In contrast, while CHT also makes winners and losers, and thus induces fights, it could be a lot less random and expensive, and take into account more local detail.

A land use rule change would usually be proposed by an owner who expects it to increase the value of their property. They would pay an auction fee to put their proposal on the official agenda, and once their proposal became official they’d declare a higher conditional value for their property. Other property owners would learn of this proposal, in part via prediction markets on the chance that this change will happen, and on which properties are likely to be substantially influenced. These other owners might then also declare new conditional property values, either higher or lower. Some property owners would personally track change proposals and change conditional declared values; others would delegate such tasks to an app or a human agent.

In any case, all those who’d gain or lose from the proposal would be tempted to exaggerate their conditional declared value differences from the status quo. This is because such exaggerations might influence the conditional tax revenue asset prices that would be used to decide if the proposal is approved. They’ want to exaggerate a win to help make the win happen, or exaggerate a loss to prevent it from happening.

Owners might also be tempted to trade directly in the speculative markets on conditional tax revenue assets, to distort such prices in order to achieve the same result. These efforts can be seen as attempts to “manipulate” such markets, via trades intended to change prices in ways that favorably influence decisions, even if those trades lose on average financially. We have a substantial literature on such manipulation; we have papers on game theory <a href="http://hanson.gmu.edu/biashelp.pdf">proofs</a>, <a href="http://hanson.gmu.edu/biastest.pdf">lab</a> experiments, and field experiments. And the bottom line is that when market speculators are free to counteract suspected manipulation attempts, such attempts tend on average to <em>increase</em> the accuracy of market prices.

Yes, noise in tax revenue asset prices can introduce noise into this decision process. But the two facts that 1) tax revenue on large property aggregates must exactly equal the sum of the tax revenue on each contained property, and 2) change-conditional values are by default equal to status-quo-conditional values, should greatly reduce the noise on differences in change versus status quo sums. CHT allow rather precise estimates of the effect of land use rule changes on net property values.

But there’s another game that owners could play: social shaming and other coordinated mistreatment of new property owners. For example, existing owners in an area might try to coordinate to mistreat any new owner if the old owner complained that they actually didn’t want to sell. For example, nearby owners might act coldly to a new resident, or refuse to shop at a new store. Nearby owners might similarly coordinate to mistreat new owners who bought a property conditional on a disliked property use rule change. As such mistreatment could actually change the value that potential property owners put on a property, it could be stably reflected in tax revenue asset prices. For example, by generally mistreating new owners who are complained about by old owners, new owners might be forced to pay a premium to gain old owner approval, and in that case local owners could safely reduce their declared property values by the amount of that premium.

Consider the example of a store asking to cut its parking requirement, to allow for more store space. Nearby residents may fear that this will result in the streets in front of their homes being used for overflow parking during peak shopping periods, such as at Christmas. While this may reduce the conditional value of their home property, given this parking change, honest estimates of those reductions might not be enough to compensate for the gains to the store. So the change could still happen. In this case, residents might try to exaggerate the harm they suffer, by reducing their conditional property values even more, and trying to coordinate to punish any new residents who buy such homes via paying such extra low conditional declared values.

However, a new owner is less likely to be mistreated by nearby owners when many new owners coordinate to buy new properties near each other all at once. And one easy way to coordinate is to buy properties conditional on a rule change. So new owners may seek exaggerated conditional value reductions, in order to both buy new property cheap, and also to coordinate with other new owners to buy in the same area at the same time.

More generally, anyone proposing a rule change to improve their property is probably well advised to package that change with changes that would be attractive to outsiders seeking to buy many properties at once in an area. This would make more credible the threat of outsiders taking many local properties at their conditional declared values. Which should reduce exaggerated expressions of harm in conditional declared property values. And if many change proposals did this, rules would tend to favor making it easier for big outsider buyers.

Now consider the example of a proposal for a new park. As managing the park is probably a money-losing proposition, the park owner must be paid to do the job, via a negative declared property value. So a simple proposal to turn an empty lot into a park could cause a great loss for the owner of that lot. If the CHT system limits the size of such losses, such proposals might typically include cash transfers from nearby owners to the owner of the park property. Even so, since the CHT system will likely allow some losses, one may still not want to be the property chosen for a new park. This fact should induce owners to think about whether a park might be a good idea in their area, and if so to proactively propose to turn someone <em>else’s</em> property into a park.

The fact that this CHT system induces these sort of games does not seem ideal, but even so it still looks promising compared to our status quo systems, which also induce many harsh conflicts and games.

Note that two key factors are likely to increase the spread between declared and actual values, and thus the scope for such strategic games. One factor is transaction costs; higher costs of changing property usage would reduce the value of other potential owners, relative to the current owner. The other factors is the default terms of the transaction specified via a CHT system. If you accept the official declared value as a sale offer, but if you and the property owner don’t like the default terms of that transaction, you’ll be tempted to renegotiate to get better terms. But this will introduce frictions and thus transaction costs. If, however, the law supports good transaction defaults and low transaction costs, declared values usually won’t deviate as far from actual values.

<strong>Added 22Feb</strong>: In the comments, Kieran Latty [points out](conditional-harberger-tax-games) that people may prefer a reframing of Harberger taxes, wherein owners are supposed to state their “honest value” of the property as D, and then the legally enforced sale prices is 20% above D, and we raise the tax rate by 20% as well. As this is mathematically equivalent, I have no objection.<span class="Apple-converted-space"> </span>

## [Reliable Private-Enough Physical Identity](#table-of-contents)
_Posted on 2020-12-14_

> “Your papers, please” (or “Papers, please”) is an expression or trope associated with police state functionaries, allegedly popularized in Hollywood movies featuring Nazi Party officials demanding identification from citizens during random stops or at checkpoints. It is a cultural metaphor for life in a police state. (<a href="https://en.wikipedia.org/wiki/Your_papers,_please">More</a>)

When we share public spaces with mobile active things like cars, planes, boats, pets, drones, guns, and soon robots, we are vulnerable to being hurt by such things. So most everywhere on Earth, we require most such things to show visible registered identifiers. (Land also requires such registration, and most smart-phones contain them.) This system has some obvious advantages:

<ol>
<li>If such a thing is actually used to harm us, and if we can remember or record its identifier, then we can look it up to find a human we might hold responsible.</li>
<li>If such a thing does not show a visible identifier, we can immediately suspect it is up to no good, and pull away to limit harms.</li>
<li>If you ever lose a registered thing, its finder can find you via the registration system, and return it.</li>
<li>Registration discourages theft, as fully effective theft then requires also changing a registration entry.</li>
<li>The identifier offers a clear shared unique index or “name” to facilitate records and discussion about the item.</li>
</ol>
Oddly, humans are the mobile active things to which we are usually the most vulnerable, and yet we don’t require humans to show visible registered identities in shared public spaces. As a result, it is harder to tell if a human is allowed to be where we see them, and if they hurt us and run away, then we face larger risks of not finding someone we can usefully hold responsible. Humans can also as a result be more easily lost or stolen.

Because of this problem, a great many organizations require humans who try to enter their spaces to, at their entrances, show a registered identity. And many of these orgs require that visible ID tags be continually shown within their spaces. Even more orgs (such as stores) require such identifiers on their responsible representatives, even if not on visitors. In fact, most orgs would probably require everyone in their spaces to have IDs this if were cheap; they relent mostly out of fear of extra costs and discouraging visitors.<span id="more-32657"></span>

Now in addition to direct costs to create, maintain, show, read, and record identities, identifiers do seem prone to other disadvantages:<br/>
<strong>A)</strong> If your car’s license plate is visible, then someone could copy it, put it on a similar-looking car, and do bad things while pretending to be you. Similarly, someone might destroy or change your car license plate to make it look like you were trying to hide your identity.<br/>
<strong>B)</strong> If info about your car were posted publicly in association with its identifier, others might learn things about it and therefore you that you’d rather they didn’t know.<br/>
<strong>C)</strong> Visible identifiers make it easier to discretely follow the identified thing around, and collect and share records of where it was when. For example, “tag reader” cameras [now](every-move-you-make) regularly record the license plates of cars that pass them, making it easier for police to collect and share records of which cars were where when.<br/>
<strong>D)</strong> Even if only government were allowed to see your registration info, and even if only government could collect and share records of where your identifier had been seen, a corrupt government might use this info against you, and an incompetent government might let others see it.<br/>
<strong>E)</strong> If identifiers could not be easily removed or hidden on special occasions, this might make it harder to mount protests or revolts against a government.
Now for cars, planes, etc. most of the world seems to think that the benefits of identifiers substantial outweigh their costs and disadvantages; this isn’t debated as if it were a close call. The close calls are when the value of the item, or the harm it might do, seem too small to justify identifier costs. Such as with model airplanes or children’s bicycles. And regarding humans, many organizations clearly feel that the benefits of identifiers far outweigh their costs.

Yet in most public spaces in the world, we are reluctant to require visible registered identifiers for humans. Even though the relative sizes of the costs and benefits of identifiers seem similar for humans to those in the other cases we require identifiers.

Now, yes, part of this is that visible ID badges can just look ugly. But surely a bigger reason is a huge negative cultural association with required visible identifiers on humans. We think of <a href="https://encyclopedia.ushmm.org/content/en/article/tattoos-and-numbers-the-system-of-identifying-prisoners-at-auschwitz">tattoos</a> put on Auschwitz inmates, of movies where Nazi officers say “papers, please”, or of the Bible’s <a href="https://www.biblegateway.com/passage/?search=Revelation+13%3A16-18&amp;version=KJV">forecast</a> of an anti-christ “beast” who requires everyone to display his number to buy or sell. And many action stories feature <a href="https://tvtropes.org/pmwiki/pmwiki.php/Main/FugitiveArc">heroes</a> <a href="https://tvtropes.org/pmwiki/pmwiki.php/Main/NobleFugitive">who</a> are fugitives on the run from authorities, heroes often thwarted by identity systems.

Lately we’ve seen a lot of progress on biometric techs that try to identify people via their faces, gaits, voices, etc. While still expensive and unreliable (error rates of 1-30%), such tech seems likely to get cheaper and more reliable. And laws, such as those against wearing masks, mostly try to support biometrics, instead of getting in their way. So if we don’t official adopt some other human identity system soon, we <a href="https://twitter.com/robinhanson/status/1337585354233290752">seem</a> likely to stumble into one built on such biometrics.

Such accidental systems seem likely to be substantially less fair and less reliable than designed system. Even today, facial recognition systems <a href="https://www.washingtonpost.com/technology/2019/12/19/federal-study-confirms-racial-bias-many-facial-recognition-systems-casts-doubt-their-expanding-use/">seem</a> less reliable for women, people of color, children, and the elderly. And complex accidental systems seem more likely to let people make them fail just when they are trying to get away with bad things. Such as doing bad things in darker places where facial recognition gets harder. Designed systems have a better chance of remaining reliable in especially important unusual situations.

Given this looming threat, rational policy analysts should try to look past the negative symbolism of human identifiers to ask: would it be better to deliberately create an organized identity system, rather than waiting for the indirect effects of cheaper biometrics? Might a deliberate system reduce the pressures that are driving the development of an accidental biometric system?

Over the years I’ve noticed several policy options ([private law](against-dwim-meta-law), [law vouchers](who-vouches-for-you)) that are made easier via required registered human identifiers. I went looking for deeper analysis of the tradeoffs here, but couldn’t find much in a quick search, so I’ve tried to do a quick analysis myself, which I now present here. I invite those with relevant expertise to correct or refine my efforts.
My tentative conclusion is that there seem to be cheap ways to greatly limit most of the potential disadvantages of human identifiers, if only we can get past their negative cultural associations. So let me first quickly outline what I see as a reasonable proposal, and then go through how it can limit disadvantages.

<strong>Proposal</strong>

Imagine that a new law required each person to sign up with an identity org. This org issues them <a href="https://en.wikipedia.org/wiki/Radio-frequency_identification">RFID</a> [tags](why-not-rfid-tag-humans), and they must have at least one such tag on their person whenever they are outside of their home. Such tags do <em>not</em> have to be put inside their body, though that is allowed. Each tag encodes at least one, and perhaps a great many, <em>N</em> bit strings that identify this person. There is a simple free “public option” identity org, which issues each client many cheap tags per year (via verified in-person meetings, perhaps passive UHF tags, now ~$0.15 ea.). Or you can pay more for fancier tags and orgs. (Tags and tag readers may be included within smart phones. There is a RFID design tradeoff between cheap tags and cheap readers; I’m not clear which is best.)
When a tag reader issues a standard <em>WhoIsHere?</em> signal, all tags that hear this query must respond promptly by broadcasting an <em>HereIAm</em> signal containing one of these <em>N</em> bit strings. This <em>HereIAm</em> signal is audible to all close enough tag readers. Tags need <em>not</em> respond with the same <em>N</em> bit string that they used in response to recent <em>WhoIsHere?</em> queries. (And they need not respond if they have already responded to such a request in the last <em>X</em> seconds. (Unless they have moved more than Y meters?) Some TBD mechanism limits response collisions, such as responding on different frequencies or at random response delays. There are standards on the signal strengths that tags produce and can hear. Currently unused tags are to be kept at home or in a signal-sealed container.)

If cameras or other detectors suggest that there is a human in the local space, and yet no tag at their location responded to a <em>WhoIsHere?</em> query, that is a legal violation and an immediate warning sign. After perhaps a quick check about this sign’s reliability, those who manage that local space may issue a warning to others nearby. And individuals with direct access to this indicator may take immediate precautions. The person without a responding tag may be subject to immediate restraint and serious legal consequences if they did this on purpose or negligently. To encourage detection of such violations, a bounty might be paid to those who discover and announce them.

If a tag reader does get a string from a tag, then it can submit a <em>HowSafe?</em> query to a standard identity name-server. This query includes this string, the time and place of reading, and an identifier of that tag reader. This <em>HowSafe?</em> query is then forwarded to the registration organization who issued that string, who then promptly responds (back through the identity server) with a few bits describing the legal status of that person at that place and time.

Another warning sign is if it seems that two tags on the same person respond with incompatible responses. That also suggests something isn’t right.

What exactly these few bits encode is a key design choice, discussed more below. They might encode if that person has outstanding warrants, or if they are an ex-con. They might encode what legal jurisdiction would cover disputes with that person. For example, is this person a foreigner, or a foreign diplomat immune to many local laws? (In a [private law](against-dwim-meta-law) world, what law are they signed up with?) They might also encode how insured is that person to pay damages should a court judge against them. (Do they have a [law voucher](who-vouches-for-you)?) It should probably encode if <em>HowSafe?</em> queries about that person have recently been received form apparently reliable readers at incompatible space-time locations, calling into doubt all of that person’s recent tags.
Identity orgs are required to privately record all their interactions, and to promptly update the info that they use to respond to such queries. They face large penalties if they are ever caught giving false responses. Large bounties to those who prove such violations could encourage detection of such violations.

Other queries besides <em>HowSafe?</em>, asking for more info or for agreement to some offer, might be sent with the <em>N</em> bit string to the identity org, who could then forward them to the person for approval. For example, a query may ask for a facial recognition <a href="https://ieeexplore.ieee.org/document/7217004">code</a> to check against a just-obtained facial image of the person there. Or a query may ask for payment to allow their entry into some space.

Finally, in response to a lawsuit or crime, a court might order the identity org to reveal more info, which may allow the questioning, investigation, or even arrest of that person. The fact of this future possibility can be key to making people feel safer around tagged strangers nearby.

<strong>Disadvantages</strong>

Now let’s go through the above listed possible disadvantages of identity systems, to see how well they can be limited within this proposal.

<strong>A)</strong> If someone tries to reuse one of the identifiers that your tag has previously given out, but uses it in a time or place incompatible with the plan that your identifier has arranged, or your org can be immediately flagged as probably false. Your identity org can also flag it as at substantial risk of being false if the time and place of a resulting <em>HowSafe?</em> query disagrees with other recent <em>HowSafe?</em> queries, or with where you have been reporting yourself to be. (Tag thefts must be reported promptly.) All of which makes it hard for someone to fake being you.

Simple passive RFID tags cannot be hacked without great difficulty and direct physical contact. But more complex tags might be easier to hack. Someone might also steal or break your RFID tag, or plant a bad RFID tag on your person, putting you at risk of being temporarily flagged as suspicious. You might perhaps put a spare RFID tag on your person that is usually off but turns on if it has been too long since it heard from your main one.

<strong>B)</strong> If you only ever use each of your <em>N</em> bit strings once, and if observers can’t discern the code that your identity org uses to generate them, then observers can’t match them against each other or other data. And even if each string is used a few times, that still may say little about their owner; they don’t directly leak much info about you.

Each identity org will need to have pool of N bit strings from which they can issue, and we’d like it to be hard to infer much about the org or person from just looking at a string. The identity name server will have to know at least an identity org to which each string maps, and any who compromise its security would gain that clue. But identity orgs might privately exchange strings, and forward query requests to each other. And getting further clues would require compromising the security of your identity org, and such orgs could compete on making that seem hard and unlikely.

<strong>C)</strong> Observers may try to coordinate to collect and share sequential <em>HereIAm</em> tag responses with camera footage and other data, in an attempt to identify your path in space-time. If <em>HowSafe?</em> queries contained very few bits on average, if the <em>X</em> second duration required between consecutive responses were long, and if people were closely spaced and moving around substantially, then tracking via <em>HereIAm</em> responses which each gave a new unique <em>N</em> bit string would by itself typically quickly fail, and lose the trail. However, if these conditions did not hold, or if this data could be combined with other surveillance data, longer trails might be more reliably identified.

Such trails could probably be identified anyway with sufficient shared other surveillance data, even in the absense of any <em>HereIAm</em> responses. So a key design consideration for this whole system is how much it is worth extending the X duration, and reducing the average bits per <em>HowSafe?</em> responses, in order to increase the chances that identified trails are broken into more smaller chunks. Such efforts only make sense of they are sufficiently pivotal; they are less worth the bother if trials are either very likely, or very unlikely, to be identified without such efforts.

<strong>D)</strong> In this system design, unless you choose a government managed public-option identity org, the government does not have access to your key identity info, and cannot get that info without a court order. Yes the government might pressure them in private to give up your info in private. But that will long remain true for all organizations that hold info on you.

<strong>E)</strong> If people want to mount a protest, or attempt a coordinated revolt, they can just choose to stop using tags during crucial periods. Even implanted tags might be made inoperable if covered when they sit in coverable parts of the body. Then the government might see that a crowd of people is protesting or revolting, but could not use tags to identify them.

So in sum, if we don’t want the expensive unreliable unfair identity system that will result accidentally from falling costs of biometrics, we should consider creating a deliberate system of visible registered identifiers on humans. Such a system might still protect a lot of privacy, when that’s otherwise possible, while achieving most of the gains that identity systems have long provided for cars, planes, pets, etc.

## [Freedom Isn’t Free](#table-of-contents)
_Posted on 2019-05-05_

> The concept of a right to health has been enumerated in international agreements which include the Universal Declaration of Human Rights, International Covenant on Economic, Social and Cultural Rights, and the Convention on the Rights of Persons with Disabilities. … equitable dissemination of medical knowledge and its benefits; and government-provided social measures to ensure adequate health. …
> Everyone has the right to … food, clothing, housing and medical care and necessary social services. … “responsibility [that] extends beyond the provision of essential health services to tackling the determinants of health such as, provision of adequate education, housing, food, and favourable working conditions” … right of everyone to the enjoyment of the highest attainable standard of physical and mental health … each individual holds an inherent right to the best feasible standard of health. (<a href="https://en.wikipedia.org/wiki/Right_to_health">more</a>)

We might want to say that people have a right to food. And in a place like the Soviet Union, where food was centrality produced and distributed, a right to food might be defined in terms of fixed numbers of particular items. So many loaves of bread, kilos of meat, and bottles of milk per week, for example. Such “rights” would be complex, vary by time and place, and result mostly from complex and changing tradeoffs, as well as politics.

While basic ethical principles might influence such rights, that influence may be hard to discern among so many other influences. If a right to food were enshrined in the text of a constitution, it would be hard for courts to have that text and a few abstract principles strongly determine if any given action is taken to violate that right or not. They might accumulate case law on how to make such decisions, but that would mostly be the court defining the right, not the constitution or abstract principles. The court might delegate many details to government agencies, in which case it would be those agencies deciding most details, not the constitution or abstract principles.

In contrast, in a market economy like ours, where individuals can more easily choose the particular foods that they want, it makes less sense to talk having about having rights to particular baskets of bread, meat, milk, etc. One could instead talk about a right to so many calories or grams of protein, but that might be hard to enforce. It could make more sense to talk about a right to a minimum food budget, and to having foods available to purchase at their real costs. (Such a budget might be set by market prices to get min calories, etc.) And it might be work even better to just focus on general redistribution systems expressed in terms of money, allowing each person to choose their own food priorities.

In a market-based economy where rights are implemented via food budgets or overall redistribution policies, outcomes would be influenced more by the constitution text and abstract principles, and by many individual choices, and less by the courts or government agencies.

Similarly, in a centrally-administered medical system, one could make a long list of the particular medical treatments to which each patient is entitled, if they were diagnosed with particular conditions. This long list of medical rights would be context-dependent and change frequently, and it wouldn’t have any clear relation to basic ethical principles or a constitutional text about a right to medicine. Such a list would mostly reflect many practical tradeoffs as well as politics. It seems quite hard to formally define and enforce any simple general “right to medicine” given all this complex variation and context dependence.

When medicine is allocated more by a competitive market, it can make more sense to try to ensure that people are free to buy medicine, medical insurance, and info on medical quality, all at prices that reflect the real costs of such things. One might try to define medical rights in terms of a minimum budget that each person has to buy medicine or medical insurance or info. Or one might focus on a more general system of redistribution expressed in terms of money, and let each person choose their medical priorities. In either of these last two scenarios, abstract principles and a constitutional text, together with individual choices, could have more influence on outcomes, relative to decisions by courts and government agencies.

In this last scenario, if you saw a case where you felt bad that someone who knew about a particular medical treatment didn’t buy that treatment, you might consider pushing to increase the priority of similar people in your more general system of redistribution. So that they could have more money to buy such treatments. If you gave such people more money, but they chose instead to spend that money on other things, you might accept that they have differing medical priorities from you, or you might try to push them to share your priorities. Either way, that dispute doesn’t really seem to be about a right to medicine.

If you are with me so far regarding food and medicine, then in the rest of this post I want to convince you to think similarly about many formal civil rights and liberties. At least regarding rights and liberties whose limits are set mainly by criminal law enforcement considerations. Today our constitutions and courts <a href="https://waterprotectorlegal.org/rights-tips-interacting-law-enforcement/">try</a> <a href="https://www.law.cornell.edu/wex/prisoners%27_rights">to</a> <a href="https://civilrights.findlaw.com/other-constitutional-rights/rights-of-inmates.html">specify</a> <a href="https://en.wikipedia.org/wiki/Civil_liberties">many</a> complex related rights and liberties. I will argue that this complexity is to a large degree due to having a centralized government-run system of criminal law enforcement. This is analogous to the complexity we would have if the government ran the food system or the medical system, wherein rights to food or medicine would consist of long lists of the food you could get each week, or the medical treatments to which you were entitled.

I will suggest that we could instead switch to a much more private, open, and competitive system of criminal law enforcement. In such a system, individuals could buy the particular civil rights and liberties that they wanted. We could then work to ensure that people are free to buy these rights and liberties at prices that reflect their real costs, and that people have a minimum budget to purchase such things. Or we might just focus on a more general system of redistribution expressed in terms of money, and let each person use money to express their priorities for rights and liberties re criminal law enforcement. Let me explain.

Today, we have explicitly declared a great many rather specific rights and liberties on how we are to be treated by our systems of law enforcement. Of course your actual rights and liberties vary according to your exact legal jurisdiction, the legal text there, court interpretations in that jurisdiction, and how local law enforcement agencies actually implement court rulings in their actual policies.

You may have have rights to be silent, and to not talk to police, and exceptions to those rights, such as when you must identify yourself. You may have rights and obligations regarding when you may be detained or arrested, and who must give testimony regarding which kinds things about which kinds of associates, including themself. There are rules on when one must be allowed to consult a lawyer, and rules that require lawyers to be available free of charge. You may have have rights to keep some things private, to be safe from unreasonable searches and seizures, and there may be exceptions to these based on warrants and on which things are in “plain view” or result from “hot pursuit”. Other exceptions are based on extra powers given to police in certain situations.

You may have rights to assemble with others of your choice, and to travel freely, but these rights may have exceptions limiting where you can go where when, such as curfews and orders to stay away from some places or to stay close enough to other places. You may have a right to speedy trials. Regarding punishment, you may have rights to avoid disproportionate punishment, and cruel and unusual punishments. In prison, you may have rights to minimum qualities of food and medicine, to a lack of racial segregation, to accommodation of your disabilities, to a lack of crowding, and to some kinds of speech, contact with outsiders, and religious activities.

For all of these rights and liberties, you may have complex rights regarding who must monitor to check that they are actually being upheld, and who may sue whom claiming that they are not, and what they would win if they won. Many have claimed that in fact many important groups in our societies don’t actually have many of the rights and liberties that they are supposedly granted on paper. I’m inclined to believe many such claims, which is a big part of why I seek other approaches.

Much of this complexity results from the fact that, in order to enforce criminal law, officials sometimes need to detain, punish, and watch people, and they sometimes need to limit their travel, assemblies, and other activities. Officials sometimes need to collect info about some people from their things and from other people. These many complex rules about rights and liberties are often claimed to be designed to give everyone as many rights and liberties as feasible, while still allowing criminal law officials to do what needs doing to enforce criminal law in a reasonably cost-effective manner. Because the world is complex, these rules must be complex.

But imagine that we replaced our centralized government run system of criminal law enforcement with this:

> Consider a [fine-insured-bounty](privately-enforced-punished-crime) (FIB) crime law system. … All (but one) crime is punished officially by fines, everyone is fully insured to pay large fines, and bounty hunters detect and prosecute each crime. In a FIB system, we collectively decide the fine and bounty level for each crime, and manage a judicial system which decides individual cases.
> If we set the fine level for each crime to be our best estimate of the social harm produced by one more crime event of that type (divided by the chance that it will be caught, plus enforcement costs), then the insurer-client pair would internalize that social harm, in which case we could leave that pair free to choose punishment types, costs, and levels, as well as (many aspects of) police and prosecutor monitoring and investigative powers. ([more](bounty-hunter-blackmail))

Within a FIB system, insurer-client pairs choose most details of punishment, including type, size, duration, etc. So within such a system, there’s little need to give people rights to avoid disproportionate, cruel, or unusual punishment. Anyone can choose to avoid any type of punishment, if they are willing to pay associated insurance premiums.

Similarly for monitoring to prevent crime. Insurers will want to promote and enable such monitoring, to avoid having to pay on behalf of clients. So insurers will offer lower premiums to clients who allow more monitoring. No need to guarantee any minimum or maximum monitoring; such levels are chosen by contract. For rights re how one interacts with police, it is possible to not give bounty hunters any more rights than ordinary people have. In which case we’d need no extra rights relative to police interactions.

Now it does seem plausible that the more rights that bounty hunters have to collect evidence, such as by searching places and compelling testimony, the higher the chance that any given crime could be caught, with the criminal’s insurer forced to pay a fine. But what if lowering this chance were the main external cost that resulted from letting a potential criminal choose to make it harder to collect evidence about them? In this case we could correct for this effect via fine amounts. The fine for each crime should depend on an estimate of the chance that crime would have been detected and successfully prosecuted. With decent (and perhaps conservative) estimates of how the chance of catching a crime depends on how open a criminal is to evidence collection, we could adjust fines for this effect, and thus allow insurer-client pairs to choose how open to be to bounty hunters seeking evidence. In which case we don’t need a right against “unreasonable” police searches.

So far I’ve argued that, in a FIB system, we don’t need formal rights and liberties regarding issues where we can just let insurer-client pairs choose, because they internalize the social harm of such choices. I’m not claiming that all civil rights and liberties are of this type, but many are. Creating a more private, open, competitive criminal law system could allow us to greatly simplify civil rights and liberties, and have the results depend a long more on constitutional text and general principles, and on individual choices, and depend less on courts and government agencies. Just as when we have private, open, competitive systems for food or medicine.

What if you felt bad when you saw someone choose fewer civil rights and liberties than you thought right or wise? You might try to persuade them to change their priorities, or you might try to increase the priority that you give to such people in your redistribution system that ensures minimum budgets to buy rights and liberties, or within your more general redistribution system. So that they could more easily afford to buy more rights and liberties if they wanted them. I think this would work better than trying to centrally legislate who exactly should have which particular rights and liberties, as that wouldn’t well take into account individual tastes, costs, and context.

<strong>Added 1June:</strong> Here’s a way to estimate “how the chance of catching a crime depends on how open a criminal is to evidence collection”. Have the statute of limitations be no shorter than N (=10?) years, and require everyone to keep good private electronic records of their activities for at least that long. Allow L (=4?) different privacy levels that everyone can choose among. Divide the polity into M (=1000?) regions, and every N years force one random region to have the lowest privacy level regarding its last N years of crimes. For each region and privacy level combination, have a prediction market estimating its crime rate (number of crimes weighted by fine level, divided by average-over-period fraction of residents at privacy level) conditional both on being randomly picked, and on not being so picked. That’s 2*L*M/N prices per year. The fine increase factor for each region and privacy level combination is given by a smoothed ratio of the estimated crime rates between the two conditions. Smoothing can take the whole set of prices and find a simpler model that fits them.

<strong>Added 5Aug:</strong> Here is <a href="http://hanson.gmu.edu/crime.pdf">pdf</a> of slides for talk I gave.

## [Quality Regs Say ‘High Is Good’](#table-of-contents)
_Posted on 2019-09-19_

> 95% think doctors should be licensed. … 96% oppose legalizing crystal meth. (<a href="https://fivethirtyeight.com/features/how-to-win-an-election/">more</a>)

One of the main ways that our world is not libertarian is that it is full of government rules requiring minimum quality levels for many kinds of products and services. We see this for food, drugs, building codes, auto/plane rules, allowed investments, censorship, professional licensing, school accreditation, sports equipment, and much more. Once you look for them, you find such rules everywhere. So a key basic puzzle is: why do we have so many min quality rules?

Here are some clues to keep in mind:

<ol>
<li>Though these rules limit consumer choices, they have strong voter support.</li>
<li>Such rules were far less common in the ancient world.</li>
<li>Today these rules are extremely widespread, across many areas of life and types of societies and governments.</li>
<li>These rules are implemented via many channels: liability law, regulatory agencies, and legislation.</li>
<li>Poor nations tend to have lower standards, like rich nations did when they were poor, yet we see few exceptions for poor people or neighborhoods.</li>
<li>Product bans are far more common than are official quality evaluations.</li>
<li>Many such rules are retained even when they seem quite ineffective, such as laws against vaping (little health harm), recreational drugs, and prostitution.</li>
<li>We don’t make exceptions for customers who can show that they clearly understand that the product is considered low quality.</li>
</ol>
<span id="more-32202"></span>

One explanation is that producers lobby for such rules, to limit competition. However, this often doesn’t actually reduce competition much; there can be nearly as much competition among those who can satisfy these rules as there would be without them. And if such rules cut overall demand for their product, that hurts these firms overall. In addition, it seems unlikely that politicians would support such rules if voters hated them. So we need to understand why voters don’t dislike, and quite often support, such rules.

Another explanation is externalities in product use. Shoddy cars can cause more car accidents, while shoddy buildings might start more fires that spread to neighboring properties. Ineffective drugs might keep patients contagious for longer. But use externalities are pretty minimal for most of these products, and often such externalities are positive; others often benefit from your using a product. In addition, our legal system already has mechanisms to discourage use externalities; if those aren’t working well enough, let’s just dial them up a notch (such as via [FIB](freedom-isnt-free)).
In simple supply-and-demand models, product quality is always optimal. However, quality choice can go wrong when a monopolist faces fixed or marginal costs of creating product quality, and sells to customers who vary in how much they value higher product quality. In such models, the monopolist chooses too low a quality, relative to welfare maximization, because they focus on their marginal customer, who has the lowest value for quality among all of their customers.

However, if we instead model a sequence of firms selling products of differing quality to customers with different tastes for quality, then only the firm that sells to the most discriminating customers chooses too low a quality. All the other sellers make good quality choices, as they have two kinds of marginal customers, at the low and the high ends of their customer range. So this model only predicts low quality problems for the highest quality products. It is private jets, mansions, and lobster dinners that aren’t quite as fancy as they should be. This can’t justify our usual min quality rules focused on the lowest quality products.

Another possible explanation is that customers are ignorant of product quality, while legislators, regulators, or courts know more about quality. If customers want to match the quantity or style of their product use to the quality of that product, they might appreciate the info implicit in a min-quality-based ban. Given such a ban, all product quality is known to be above the min level, which is a smaller quality range than without the ban. However, such customers should even more appreciate just being told the product quality level. Relative to being given such quality info, customers expect to be worse off with bans.

Sloppy and error-prone customers, who do not always do what they intend to do, have an additional reason to appreciate product bans. Such bans prevent them from accidentally choosing a lower quality than they intend. However, bans on higher quality products, favored by almost no one, would serve a similar function, by preventing customers from accidentally choosing a higher quality than they intend. And if this were the key problem then a simple solution would be to create “[would have banned](paternalism_is_)” stores, i.e., stores which only offer products that would usually be banned.
Imagine that, to shop in such a store, you had to get a license by passing a test showing that you really understand that these products would otherwise be banned, and you had to schedule your visit two days in advance, to prevent impulse purchases. We might even add educational or IQ test requirements to get the license. Yet even with all these added precautions, it seems quite unlikely that voters would support allowing “would have banned” stores. This makes it harder to believe that sloppy actions are voters’ main concern when they support min quality regs.

Another possible explanation for quality regs is that the value of quality evaluations and certifications is reduced by the temptation of evaluators to lie about quality. For example, a drug regulator seeking to prevent auto accidents might exaggerate the harms of alcohol, and underplay the harms of caffeine, relative to their real health risks. A month ago I [reviewed](a-model-of-paternalism) my theory paper on this:
> My model … applies to any one-dimensional choice of an activity level, a choice influenced by an uncertain one-dimensional quality level. Thus my model can help us understand why people placed into a role where they can either advise or ban some activity would often ban. Even when both parties are fully rational, and even when their interests only differ by small amounts. The key is that even small differences can induce big lies and an expectation of frequent bans, which force the advisor to ban often because extreme advice will not be believed.

This can explain the existence of bans <em>given</em> the existence of an informed regulator who is allowed to choose bans. However, I also show that there are large sets of cases where both sides are better off when no regulator is given the power to ban, something like how in the U.S. 1st amendment rights prevent most banning of books and magazines. We could declare many more such rights if we wanted, but we don’t. In addition, this model predicts that officials will, nearly as often as banning, publish quality evaluations of allowed products. But they don’t publish evals remotely as often as they ban. Also, this theory doesn’t seem to predict the pattern that we see of standards varying with national but not neighborhood income.

The explanations that we’ve discussed so far all assume rational, if perhaps ignorant, consumers. But one might instead postulate that consumers are proud and stubborn, and so just won’t listen to advice. Perhaps most consumers consider regulators to be arrogant blowhards unworthy of deference, and so the only way to protect those consumers is to just ban bad products. However, if these proud folks don’t respect regulators enough to take advice from them, why would they as voters support creating such regulators and empowering them to ban or certify products? This doesn’t make sense.

Min quality regulations for housing, such as min lot sizes, max building heights, and max people or families per apartment, tend to have the effect of making it harder for poorer people to live in places with such regulations. Rules that keep out cheap grocery stores or restaurants have a similar effect. So a desire to push poor people away from one’s geographical area is thus a plausible explanation for at least some kinds of min quality regulations. However, this works poorly as an explanation of min quality regulations that are adopted by decision-makers whose jurisdiction is over wide geographic areas. The poor have to go somewhere, after all.

In most societies in history, people have been eager to (1) know their place in the local status hierarchy, (2) get people below them to publicly acknowledge their higher status, and (3) find ways to rise to higher levels and to avoid falling to lower levels. People have also tended to use different explanations for how people above and below them got there. For people above us, we tend to focus on luck of birth, and on their happening to have more money and better social connections. For people below us, we tend to focus on their poor abilities, behaviors, and choices.

Long ago, relative status was more often overtly acknowledged in more direct ways, such as via accent, dress, formal greetings, who steps aside to let the other pass, who gets priority when served, who is allowed to accuse who of lies, etc. Many of these customs gave the impression that the higher status person was the better person. With fewer such overt acknowledgements today, elites may now be especially hungry for ways to have society acknowledge and affirm not only that they are higher status, but also that higher status people like them are actually better people in most important ways, and not just people who happen to be higher via historical accidents.

Thus another possible [explanation](explaining-paternalism) for min-quality regulations is that, by officially declaring common lower class choices to be bad choices, regulators support upper class claims to be better people. And by forcing everyone to visibly accept this declaration via their not visibly defying the bans, everyone appears to support this  claim that elite choices are better choices.
As elites believe that lower classes get there in part by making bad choices, and intuitively feel that the usual elite choices are good choices, elites find it easy to conclude that the different odd-to-elites choices made by lower classes are in fact bad choices. Naive data correlations tend to support such claims of course, as higher classes tend to have better outcomes.

So elites can tell themselves that they are helping lower classes by banning bad choices, and not consciously notice that this also helps elites to get others to officially affirm that they are better people, deserving higher status, due to their better choices. And since a great many of the different choices made by lower classes can be framed as their choosing lower quality, regulations that ban such choices can often be framed as min-quality regulations.

For example, elites often tell themselves things like “it just doesn’t make sense to buy cheap goods, as quality goods last much longer” or “the extra trouble to make sure your meat hasn’t gone bad is well worth the cost.” Pushing for regulators to make sure that everyone is legally required to follow their example feels to elites emotionally like they are helping, and it also happens to assert their moral dominance. Elites are high because they are right, and they are good and helpful to force others to follow their lead.

Okay, quality regs might make elites feel good, but why would so many non-elites support these policies as voters? Plausibly because they aspire to elite status, and by publicly displaying their agreement with elite attitudes, they affirm that they are themselves good candidates for higher status. For example, if min quality regulations only actually take away the choices that would otherwise be made by the lowest one quarter of the population, the other three quarters can sincerely support those regulations, and feel good about showing that they are not low status. And those who actually are inconvenienced by bans may not want to publicly admit this fact if that would signal their low status. So most people can rarely feel inconvenienced by regulations which officially declare that they are good people because their choices are good choices.

While all of the explanations that I’ve considered above have some place in the total picture, I suggest this affirming-high-as-good explanation as the strongest social force in creating and maintaining min quality regulations. It substitutes today for the many lost ways that ancient elites got more direct and frequent affirmation of their goodness. And it better fits the many puzzling details we see in our min quality regulation patterns.

<strong>Added 21Sep:</strong> An obvious implication if this theory is that we’d be better off without min quality regs. While there are some products & services where low classes would do better to copy elite choices, in most areas the different choices are better explained as reasonable adaptation to different contexts.

## [Socialism: A Gift You’d Exchange?](#table-of-contents)
_Posted on 2020-10-14_

After reading and reviewing a book [by](socialism-via-futarchy) a socialism critic, I then [did](the-socialist-manifesto) a book by an advocate. Then some told me “No, here is the advocate book you should have read.” I tried one of them: Nathan Robinson’s <a href="https://us.macmillan.com/books/9781250200877"><em>Why You Should Be a Socialist</em></a>, said to be “A primer on Democratic Socialism for those who are extremely skeptical of it.”
Robinson won’t commit himself to what exactly is socialism’s proposal, other than pushing for big changes in light of some vague and widely-shared values (mostly equality and democracy). He says conservatives are mean and liberals are wimpy; liberals have similar goals, but are to be disdained for not calling for bigger changes. Yet the only specific changes he’ll clearly endorse are smaller changes widely endorsed by liberals. I’ll get to some of those below, but instead of writing a whole review, I’d rather make one big point, riffing off of these quotes:<span id="more-32565"></span>

> If I grumble when I see what New York City developers have done, it’s because I miss the accordion shop on Forty-Eighth Street, and if I criticize what Amazon has done to small book publishers, it’s because I see how empty the bookstores in the French Quarter have become. …
> Neoliberal principles [are] … you measure “value for money,” and if a public asset isn’t providing it, hand it over to the private sector. This is not the traditional leftist way of thinking about libraries. Leftists don’t say, “Is this asset performing?” They say, “People need … Caplan argues that public schools are not good investments because they do not actually produce a “return” for students in the form of useful job skills. Caplan believes that the school system should be privatized and, to a great extent, dismantled, because it is “inefficient” in producing good job market outcomes. … schools as little more than factories for efficiently manufacturing value for the job market. But here’s the part that shows you the bipartisan nature of neoliberalism: Caplan’s strongest critics accepted that premise. …
> We don’t often consider just how radical public libraries are. A library is a place where anyone can go and—for free!—explore a mountain of human knowledge. It has meeting spaces, computers, and research assistance. It’s there for everyone, regardless of their means. A library is a pretty socialist institution, honestly. It’s owned by everyone, and there’s no money involved. (Unless you keep your books too long!) There’s no profit, no private ownership, no ulterior motive. It’s a place we all pay for and can all visit. They’re spaces of absolute equality, where anyone can go to study, learn, and hang out.
> Imagine the library model in other spheres of life. Say, for example, free medical clinics where anyone could go and get treatment. Free colleges, where people could go and take whatever classes they wanted, without being bankrupted by debt. Free bikes to borrow, a free water park.
> People love libraries. Now, I know, I know, nothing is actually free. But there is something very liberating about not having to think about money when you use a service, about having everything prepaid and accessible to all. It makes life less stressful and transactional. It means we don’t have to constantly be conducting little mental calculations about value, and we can just go and enjoy our lives. Lefties are often mocked for wanting “free stuff.” Free college! My God, what’s next, free ponies? But libraries work. Everyone loves them. …
> Obama … encouraged the establishment of privatized schools, which are independent of school districts and could therefore fire “bad” teachers more easily. Instead of asking teachers what resources they needed for their schools to function well, the policy relied heavily on “performance” tests and punished schools whose students did poorly. This kind of approach to policy is not egalitarian or democratic.

Imagine that for your birthday, someone gave you a gift of classes to learn French cooking. Except that you had never suggested you wanted to learn French cooking. And in fact, even if courses were free, that’s not how you’d want to spend your time. For you, there is a large “opportunity cost” of taking such classes, such that you’d get a net <em>negative</em> (net) value from taking them. (At least if we ignore some signaling effects.) So if possible, you’d prefer to exchange this gift for cash.

In this case, I’d say that this gift of French cooking lessons fails a “cash gift test”. If it were not for the possibility that doing so might make someone look bad, such as you looking ungrateful or the giver looking thoughtless, you’d rather get the cash that were used to buy the gift, rather than keeping (or using) the gift as given. This idea of a “cash gift test” is the point I want to focus on in this post.

So far, I’ve noted that, relative to a cash gift, there can be real losses from giving someone French cooking classes that they don’t want. And there could be even larger losses if you forced them at gunpoint to take such classes. We all know this at some level, which is a part of why we don’t get most of what we need via gift exchange. We do like to give each other gifts once in a while to signal our bonds to each other. But we know that if we only ate food, wore clothes, and resided in housing that came from others’ gifts, we’d overall have food, clothes, and housing that is worse matched to our personal situation. Compared to what we get for ourself, we know that our gifts usually just aren’t as well informed about what others need or want.

Note that this analysis so far doesn’t depend at all on living in a capitalist society. In <em>any</em> society, you have an opportunity cost of your time, and there will be real opportunity costs paid by others to set up, run, and manage French cooking classes. Not only might it be a waste to force people to take such classes, but it could also be a waste for some philanthropist to build and staff a big system of French cooking classrooms, making a class available to everyone within walking distance on every night of the week.

Such a philanthropist gift of a system of classes might also be said to fail a “cash gift test” if we’d all prefer that whatever was spent on making this cooking class system be spent instead on a particular cash gift. (And of course we might prefer such a cash gift exactly because we plan to spent it on some other project, instead of just being handed out as cash to individuals.) We might hope to thank this philanthropist for their generosity, but note to them that if they switched to the cash gift variation, they could be even more generous to us at no added cost to themselves.

But doesn’t this test depend on what method “we” use to pick what cash gift “we” prefer? Well, we can actually use a <em>unanimity</em> standard: the system of classes gift fails our test if there is any per-person plan for a (same-cost) cash gift that each and <em>every</em> one of us prefers to the gifted system. Of course this cash gift might need to vary across people. Some who are really interested in cooking might get a larger cash gift, and some who have no interest in cooking might get nothing. But, and here is the key point, if we believe that there is some same-cost cash gift plan of who gets how much cash, such that everyone prefers this plan, then the non-cash version fails our cash gift test. And is a bad gift, relative to the cash gift alternative.

Let me further stipulate that we shouldn’t need to know exactly how to identify who are the strongest lovers of cooking to see that this philanthropist gift is a mistake. That is, we might not be able in practice to construct an actual cash gift plan that guarantees that everyone actually prefers this particular plan to the system of cooking classes. The point here isn’t to make realistic systems for replacing real gifts with cash gifts which everyone is guaranteed to prefer. The point here is instead to clarify what we mean by a policy change being a waste (ignoring signaling incentives). If we believe that this philanthropist gift fails our cash gift test, it seems that we should hope to recommend to this philanthropist that they instead make a similar cash gift, <em>even</em> if they can’t design a detailed cash gift plan guaranteed to make <em>everyone</em> prefer it to the original gift plan.

So far I’ve been talking about a philanthropist’s gift, but now let’s apply this idea to any government policy change. For any such change, we should consider its opportunity costs to see if it is a net gain or net cost, relative to a cash transfer. (Again, setting aside signaling issues.) The fact that libraries, universities, or hospitals are given away for “free”, after being paid for via taxes, doesn’t at all settle the matter. For example, it could well be that most university education is in fact a waste; the question isn’t settled by merely noticing that sometimes some people benefit from some school in some ways.

For every proposed change in government policy, we should ask if there plausibly exists a cash-transfer-only that everyone would prefer to that proposed change. If such a preferred cash policy exists, then we should prefer to consider adopting that policy instead of the one that was proposed. “Cash” here just refers to whatever commodities are easiest to use in trades and transfers, being relatively stable, uniform, divisible, and easy to move. Money, when it exists, is usually a fine type of cash. Again, capitalism isn’t implied.

And, again, we shouldn’t have to know how exactly to construct the cash transfer plan. For example, consider the “Everest prize” policy of handing a million dollars (once, paid for via taxes) to everyone who manages to climb to the top of Mt. Everest. If we think everyone would prefer to just get handed the million without having to climb Everest, then we can know that there is a cash transfer plan that beats this Everest prize policy, even if we don’t know who exactly would get to the top of Everest to get paid if we adopted this policy.

We economists usually evaluate policies [in](on-liberty-vs-efficiency) [terms](efficient-economists-pledge) [of](efficiency-disclaimers) “efficiency” (or “cost-benefit analysis”). Under our usual (weak) assumptions, the only way for a policy change to pass this cash gift test is for the policy to improve efficiency by counter-acting some market failures, such as due to externalities. In particular, “free” schools, libraries, and hospitals only pass this cash gift test if there is some positive externality with their use to counter the fact that making something “free” induces wasteful over-consumption of more than they would consume they had to pay for its real marginal cost.
For more examples, in his “Sensible Agendas” chapter where Robinson endorses specifics, he starts by mentioning some items from Bernie Sanders’ 2016 book Our Revolution. Lets go through them.

“End fossil fuel subsidies” seems a win, if in fact there are not net positive externalities with using fossil fuels. But all of these look like likely inefficient giveaways, analogous to “free” French cooking classes:

> free college tuition, cut student loan rates, universal childcare, loans to turn firms into cooperatives, banking at post offices

Now I’m open to the possibility that such policies may counter market failures. For example, maybe banks collude to raise prices, so post office banking would offer more cost-effective competition. But that’s the sort of argument I want to hear (and that Robinson doesn’t give).

These items look like inefficient paternalistic limits on the deals people can make:

> $15 minimum wage, limit for-profit schools, new financial regulations, 12 weeks paid leave to workers with new kids

Though again I’m open to efficiency arguments. To me, most of these “socialistic” policies seem likely to be “gifts” that I think most of us would prefer to exchange for cash. We appreciate the thought, but please, don’t bother yourself, we’d rather take the cash.

One last thought. I worry that many of you will get stuck on the issue of it being hard to figure out how to plan a cash gift that everyone prefers. So here’s a more symmetric approach, using <em>two</em> tests.

Let us say that a policy change passes the “negative cash gift test” if there is no cash gift plan that everyone prefers to it, and say that a policy passes the “positive cash gift test” if there is a cash gift plan such that everyone prefers the policy to this cash gift plan. In theory, all policies with either pass both tests or neither of them. (Only cash gift plans sit right on the border.) So regarding any one policy you can ask yourself: which looks harder, finding a cash gift plan to beat this policy or finding a cash gift plan that this policy beats? Knowing which of these two tasks looks harder tells you if this policy is efficient, and thus good.

## [Vouch For Pandemic Passports](#table-of-contents)
_Posted on 2020-04-24_

Car pollution is an externality. Via pollution, the behavior of some hurts others, an effect that injurers may not take into account unless encouraged to by norm, contract, liability, or regulation. However, as the pollution from one vehicle mixes with that from many others, liability is poorly suited to discourage this; it is too hard to identify which cars hurt you, and there are too many of them. It seems to work better use regulations regarding car design and maintenance to limit the pollution emitted per mile driven, and to tax those miles driven.

Assault is also an externality; you can hurt someone by punching them. But in contrast to pollution, regulation is poorly suited to assault. We could require everyone to wear boxing globes and headgear, and we might ban insults and alcohol consumption, or perhaps even all socializing. But such regulations would go too far in restricting useful behavior. It works better to just hold liable those who punch others, via tort or criminal law. Yes, to discourage assault in this way, we must hold (or at least threaten to hold) expensive trials for each assault. But that still seems <em>far</em> cheaper than regulatory solutions.

These two examples illustrate a well-known tradeoff in choosing between (strict) liability and regulation. On the one hand, making people pay damages when they hurt others encourages them to take such harms into account, while also letting their behavior flexibly adapt to other context. On the other hand, regulation lets us avoid expensive court trials that require victims to prove who hurt who when where and how much. Though regulation induces more uniform behavior that is less well adapted to circumstances, it works acceptably well in many cases, like auto pollution, even if less well in others, like assault. (The mixed solution of negligence liability is discussed below.)

In our current pandemic, the main externality is infection, whereby one person exposes another to the virus. Conventional public health wisdom says to discourage infection via regulation: tell everyone when to get tested and isolated, and make them tell you who they met when and where. Tell them who can leave home when and for what reasons, and what they must wear out there. However, as we are all now experiencing first hand, not only are such changes to our usual behaviors quite expensive, such rules can also induce far from optimal behavior.

Recent pandemic rules have banned bike riding, but not cars or long walks. You can take only one exercise trip per day, but there’s no limit on how long. Members of a big home can all meet in their yard together, but members of two small adjacent homes may not meet in one of their yards. You can’t go meet distant friends, even if they only ever meet you. All parks are closed regardless of how densely people would be in there. The same rules were set in dense cities and in sparse rural areas. Alcohol store workers are deemed critical, even though alcohol can be mailed, but not auto repair, which cannot be mailed. Six feet is declared the safe distance, regardless of how long we stay near, if we wear masks, if we are outdoors, or which way the air is moving. Workplaces are closed regardless of the number of workers, how closely they interact, or how many other contacts each of them have. The same rules apply to all regardless of age or other illness. You may have to wear a mask, but it doesn’t have to be a good one.

Imagine that we instead used legal (strict) liability to make as many of us as possible expect to suffer personally and directly from infecting others, and to suffer more-so the worse their symptoms. In this scenario, such people would try to take all these factors and more into account in choosing their actions. For actions that risk infecting others, they would consider not only on how important such acts are to them, but also on how likely they are personally to be infected now, how vulnerable each other person they come near might be to suffering from an infection, how vigorously their activity moves the air near them, where such air currents are likely to go, how well different kinds of masks hinder infected air, and so on. If allowed, they might even choose [variolation](variolation-may-cut-covid19-deaths-3-30x).
Of course, for the purpose of protecting <em>ourselves</em> from getting infected by others, we already have substantial incentives to attend to such factors. The problem is that simple regulations don’t give us good incentives to attend to these factors for the purpose of preventing us from infecting <em>others</em>. With regulations, we have incentives to follow the letter of the law, but not its spirit. So we don’t do enough in some ways, and yet do too much in others. But if liability could make us care about infecting others as well as ourselves, then it might <em>simultaneously</em> reduce both infections <em>and</em> the economic and social disruptions caused by lockdowns. With strong and clear enough liability incentives, we wouldn’t need regulations; we could just let people choose when and how to work, shop, travel, etc.

But is it feasible to use liability to discourage infections? Yes, if we can satisfy two conditions: (1) most people are actually able to pay for damages if they are successfully sued for infecting others, and (2) enough of those who infect others are actually and successfully sued, and so made to pay.

On the first condition, ensuring that people can pay damages if they are found guilty, it is sufficient to require people who mix with others to buy infection liability insurance, similar to how we now require car drivers to get accident liability insurance. That is, to get a “pandemic passport” to excuse you from a strong lockdown, you must get an insurance company to guarantee that you will pay damages if you are shown to have infected someone. In a sense they “vouch” for you, and so are your “voucher”. The more types of voucher-client contract [terms](who-vouches-for-you) we are willing to enforce, the more levers vouchers gain to reduce risks.
The premiums for such insurance will be low if you can convince a voucher that you have already recovered from the virus, and so are relatively immune, or that you will leave your lockdown only rarely, to safe destinations. Otherwise, a voucher may require you to install an app on your phone to track your movements, or they may spot check your claims that you have sufficient supply of good masks that you use reliably when you leave home.

Okay, but what about the second condition, that enough infectors are actually made to pay? For this we need enough data to be collected on both sides, the infector and the infected, so that one can frequently enough match the two, to conclude that this person likely infected that one at this location at this time.

Now, we don’t need to be always absolutely sure of who infected who. In ordinary civil trials, the standard is a “preponderance of the evidence”; courts need only be 51% or more sure to convict the defendant. And sometimes we add on extra “punitive” damages, up to four ties as large as basic damages, often to compensate for a lower chance of catching offenders. So if we can find evidence to convince a court at the 51% or better standard for only one fifth of offenders, but we can add four times punitive damages, then offenders who do not know if they will be caught still expect to on average to pay near the basic damage amount.

Okay, but we still need to collect enough info to see who infected who at least one fifth of the time. Is this feasible? Well it is clearly quite feasible early in a pandemic, when few have been infected. Early on, if the times and places, i.e., space-time path, consistent with you being infected then and there overlap with the space-time path when someone else was likely infectious, then it was most likely their fault. This is the “contact trace” process usually recommended by public health workers early in a pandemic.

The problem gets harder later in a pandemic, when your infected path may overlap with the infectious paths of many others. Here it might be possible to use info on which virus strain you and they had to narrow the field. But even so there may still be several consistent candidates. In this case it seems reasonable to divide the liability over all of them, perhaps in proportion to the size of the path overlap. For the purpose of creating incentives to avoid infecting others, it isn’t that important to know later who exactly infected who when.

But yes, we still need info on who was infected and infectious where and when, perhaps supplemented by data on who had what virus strains. How can we get this info? People who might get infected have incentives to collect info on their path, to help them sue if infected. But people who might infect others would seem to want to erase such info, to keep them from being sued. I’ve [recently](subpoena-futures) outlined a more general approach to induce the collection of info sufficiently likely to be useful in later lawsuits. But for this essay, I’ll just propose that collecting key info be another condition required to get a pandemic passport, with violations punished by fines also guaranteed by your voucher.
Let me also note that yes, legal liability doesn’t work to discourage harms if typical harms get so small that people wouldn’t bother to sue to recover damages. In this case we could use a random lottery [approach](double-or-nothi) to dramatically lower the average cost of suing.
So let’s put this all together. You must stay at home, locked down, unless you get a “pandemic passport”, in which case you can go where you want when, to meet anyone. To get such a passport, you must get someone to [vouch](vouchers-fight-pandemics) [for](who-vouches-for-you) you. They guarantee that you will pay should someone successfully sue you for infecting them, if you agree to their terms of premiums, behavior, monitoring, punishment, and co-liability. Defendants who pay damages may have to pay extra, to compensate for most infectors not getting caught in this way. When several infector candidates are consistent with the data, they can divide the damages. And for low damage levels, a random lottery approach can lower court costs.
To get and keep a passport, your voucher also guarantees that you will collect info that can help others to show that you infected them, but which can also help you to sue others if they infect you, and win you bounties via showing that others did not collect required info. For example, perhaps you must track your movements in space and time, regularly record some symptoms like body temperature, and also save regular spit samples. This info is available to be subpoenaed by those who can show sufficient reason to suspect that you infected them. Such info seems sufficient to catch enough infectors.

And by catching a sufficient fraction of infectors who then actually pay on average for the harms that they cause by infecting, (strict) legal liability can give sufficient incentives to individuals to avoid infecting others. If so, we don’t need crude lockdown regulations telling people what to do when and how; individuals can instead more flexibly adapt to details of their context in deciding when and where to work, shop, travel etc. Yes, voucher rules would not let them do such things as freely as they would in the absence of a pandemic. But behavior would be more free and impose lower economic costs than under crude regulations which similarly suppress the pandemic spread.

Note that today the most common form of legal liability is actually negligence, which we can see as a mixed form between simple regulation and simple strict liability. With negligence, the court judges if your behavior has been consistent with good behavior standards, which are essentially behavior regulations. But you are only punished for violating these regulations in situations where your behavior contributed to the harm of a particular person. Today courts tend to limit strict liability to cases where courts find it hard to define or observe good behavior details, such as using explosives, keeping a pet tiger, or making complex product design choices. As courts find it harder to define and observe good behavior in a new pandemic, strict liability seems better suited to this case.

Note also that none of this requires employers to be liable for their infected employees. Someone who is sued for infecting others may turn around and blame their employer for pushing them into situations that cause them to infect others. Employer-employee contract could usefully address such issues.

## [Can We Tame Political Minds?](#table-of-contents)
_Posted on 2022-02-22_

> Give me a firm spot on which to stand, and I shall move the earth. (<a href="https://en.wikiquote.org/wiki/Archimedes">Archimedes</a>)
> A democracy … can only exist until the voters discover that they can vote themselves largesse from the public treasury. (<a href="https://www.goodreads.com/author/quotes/5451872.Alexander_Fraser_Tytler">Tytler</a>)
> Politics is the mind killer. (<a href="https://www.lesswrong.com/posts/9weLK2AJ9JEt2Tt8f/politics-is-the-mind-killer">Yudkowsky</a>)

The world is a vast complex of interconnected subsystems. Yes, this suggests that you can influence most everything else via every little thing you do. So you might help the world by picking up some trash, saying a kind word, or rating a product on Yelp.

Even so, many are not satisfied to have <em>some</em> effect, they seek a <em>max</em> effect. For this reason, they say, they seek max personal popularity, wealth, or political power. Or they look for the most neglected people to help, like via African bed nets. Or they seek dramatic but plausibly neglected disaster scenarios to prevent, such as malicious foreigners, eco-apocalypse, or rampaging robots.

Our future is influenced by a great many things, including changes in tech, wealth, education, political power, military power, religion, art, culture, public opinion, and institutional structures. But which of these offers the strongest lever to influence that future? Note that if we propose to change one factor in order to induce changes in all the others, critics may reasonably question our ability to actually control that factor, since in the past such changes seem to have been greatly influenced by other factors.

Thus a longtime favorite topic in “serious” conversation is: where are the best social levers, i.e. factors which do sometimes change, which people like us (this varies with who is in the conversation) can somewhat influence, and where the effects of this factor on other factors seem lasting and stronger than reverse-direction effects.

When I was in tech, the consensus there saw tech as the strongest lever. I’ve heard artists make such claims about art. And I presume that priests, teachers, activists, and journalists are often told something similar about their factors.

We economists tend to see strong levers in the formal mechanisms of social institutions, which we happen to be well-placed to study. And in fact, we have seen big effects of such formal institutions in theory, the lab, and the field. Furthermore, we can imagine actually changing these mechanisms, because they tend to be stable, are sometimes changed, and can be clearly identified and concisely described. Even stronger levers are found in the higher level legal, regulatory, and political institutions that control all the other institutions.

My <a href="http://hanson.gmu.edu/dissertation.html">Ph.D.</a> in social science at Caltech focused on such controlling institutions, via making formal game theory models, and testing them in the lab and field. This research finds that institution mechanisms and rules can have big effects on outcomes. Furthermore, we seem to see [many](why-be-contrarian) big institutional failures in particular areas like telecom, transport, energy, education, housing, and medicine, wherein poor choices of institutions, laws, and regulations in such areas combine to induce large yet understandable waste and inefficiency. Yes institutions do matter, a lot.
However, an odd thing happens when we consider higher level models. When we model the effects of general legal and democratic institutions containing rational agents, we usually find that such institutions work out pretty well. Common fears of concentrated interests predating on diffuse interests, or of the poor taxing the rich to death, are not usually borne out. While the real world does seem full of big institutional problems at lower levels, our general models of political processes do <em>not</em> robustly predict these common problems. Even when such models include voters who are quite ignorant or error prone. What are such models missing?

Bryan Caplan’s book <a href="https://press.princeton.edu/books/paperback/9780691138732/the-myth-of-the-rational-voter"><em>Myth of the Rational Voter</em></a> gets a bit closer to the truth with his concept of “rational irrationality”. And I was heartened to see Alex Tabarrok [AT] and Ezra Klein [EK], who have quite different political inclinations, basically agree on the key problem in their <a href="https://www.nytimes.com/2022/02/18/podcasts/transcript-ezra-klein-interviews-alex-tabarrok.html">recent podcast</a>:

> [AT:] Mancur Olson thought he saw … more and more of these distributional coalitions, which are not just redistributing resources to themselves, but also slowing down… change. … used to be that we required three people to be on the hiring committee. This year, we have nine … Now, we need [more] rules. … we’ve created this more bureaucratic, kind of rule-bound, legalistic and costly structure. And that’s not a distributional coalition. That’s not lobbying. That’s sort of something we’ve imposed upon ourselves. …
> [EK:] it’s not that I want to go be part of slowing down society and an annoying bureaucrat. Everybody’s a hero of their own story. So how do you think the stories people tell themselves in our country have changed for this to be true? …
> [AT:] an HOA composed of kind of randos from the community telling you what your windows can look like, it’s not an obvious outcome of a successful society developing coalitions who all want to pursue their own self-interest. … naked self-interest is less important than some other things. And I’ll give you an example which supports what you’re saying. And that is, if you look at renters and the opinions of renters, and they are almost as NIMBY, Not In My Backyard, as owners, right, which is crazy.… farmers get massive redistribution in their favor. … But yet, if you go to the public … They’re, oh, no, we’ve got to protect the family farm. …
> [EK:] a lot of political science … traditionally thought redistribution would be more powerful than it has proven to be … as societies get richer, they begin emphasizing what he calls post-materialist values, these moral values, these identity values, values about fairness. (<a href="https://www.nytimes.com/2022/02/18/podcasts/transcript-ezra-klein-interviews-alex-tabarrok.html">More</a>)

That is, our larger political and legal systems induce, and do not fix, many more specific institutional failures. But not so much because of failures in the structure of our political or legal institutions. Instead, the key problem seems to lie in voters’ minds. In political contexts, minds that are usually quite capable of being reasonable and pragmatic, and attending to details, instead suffer from some strange problematic mix of confused, incoherent, and destructive pride, posturing, ideology, idealism, loyalty, and principles. For want of a better phrase, let’s just call these “political minds.”

Political minds are just not well described by the usual game theory or “rational” models. But they do seem to be a good candidate for a strong social level to move the future. Yes, political minds are probably somewhat influenced by political institutions, and by communications structures of who talks to and listens to whom. And by all the other systems in the world. Yet it seems much clearer how they influence other systems than how the other systems influence them. In particular, it is much clearer how political minds influence institution mechanisms than how those mechanisms influence political minds.

In our world today, political minds somehow induce and preserve our many more specific institutional failures. And also the accumulation of harmful veto players and added procedures discussed by [AT] and [EK]. Even so, as strong levers, these political minds remain gatekeepers of change. It seems hard to fix the problems they cause without somehow getting their buy-in. But can we tame politician minds?

This is surely one of the greatest questions to be pondered by those aware enough to see just how big a problem this is. I won’t pretend to answer it here, but I can at least review six possibilities.

<strong>War</strong> – One ancient solution was variation and selection of societies, such as via war and conquest. These can directly force societies to accept truths that they might not otherwise admit. But such processes are now far weaker, and political minds fiercely oppose strengthening them. Furthermore, the relevant political minds are in many ways now integrated at a global level.

<strong>Elitism</strong> – Another ancient solution was elitism: concentrate political influence into fewer higher quality hands. Today influence is not maximally distributed; we still don’t let kids or pets vote. But the trend has definitely been in that direction. We could today limit the franchise more, or give more political weight to those who past various quality tests. But gains there seem limited, and political minds today mostly decry such suggestions.

<strong>Train</strong> – A more modern approach is try to better train minds in general, in the hope that will also improve minds in political contexts. And perhaps universal education has helped somewhat there, though I have doubts. It would probably help to replace geometry with statistics in high school, and to teach more economics and evolutionary biology earlier. But remember that the key problem is reasonable minds turning unreasonable when politics shows up; none of these seem to do much there.

<strong>Teach</strong> – A more commonly “practiced” approach today is just to try to slowly persuade political minds person by person and topic by topic, to see and comprehend their many particular policy mistakes. And do this faster than new mistakes accumulate. That has long been a standard “educational” approach taken by economists and policy makers. It seems especially popular because one can pretend to do this while really just playing the usual political games. Yes, there are in fact people like Alex and Ezra who do see and call attention to real institutional failures. But overall this approach doesn’t seem to be going very well. Even so, it may still be our best hope.

<strong>Privatize</strong> – A long shot approach is to try to convince political minds to not trust their own judgements as political minds, and thus to try to reduce the scope for politics to influence human affairs. That is, push to privatize and take decisions away from large politicized units, and toward more local units who face stronger selection and market pressures, and induce less politicized minds. Of course many have been trying to do exactly this for centuries. Even so, this approach might still be our best hope.

<strong>Futarchy</strong> – My <a href="https://mason.gmu.edu/~rhanson/futarchy.html">proposed</a> solution is also to try to convince political minds to not trust their own judgements, but only regarding on matters of fact, and only <em>relative</em> to the judgements of speculative markets. Speculative market minds are in fact vastly more informed and rational than the usual political minds. And cheap small scale trials are feasible that could lead naturally to larger scale trials that could go a long way toward convincing many political minds of this key fact. It is quite possible to adopt political institutions that put speculative markets in charge of estimating matters of fact. At which point we’d only be subject to political mind failures regarding values. I have other ideas for this, but let’s tackle one problem at a time.

Politics is indeed the mind killer. But once we know that, what can we do? War could force truths, though at great expense. Elitism and training could improve minds, but only so far. Teaching and privatizing are being tried, but are progressing terribly slowly, if at all.

While it might never be possible to convince political minds to distrust themselves on facts, relative to speculative markets, this approach has hardly been tried, and seems cheap to try. So, world, why not try it?

## [Consider Reparations](#table-of-contents)
_Posted on 2019-03-04_

> First … ally of President Trump’s. “We are in a civil war,” he said. “The suggestion that there's ever going to be civil discourse in this country for the foreseeable future is over. ... It’s going to be total war.” The next day … Trump critic … agreed with him—although she placed the blame squarely on the president. Trump, she said, “greenlit a war in this country around race. (<a href="https://www.washingtonpost.com/politics/in-america-talk-turns-to-something-unspoken-for-150-years-civil-war/2019/02/28/b3733af8-3ae4-11e9-a2cd-307b06d0257b_story.html">more</a>)<span class="Apple-converted-space"> </span>

Frequently in human history, one party has complained about how they’ve been treated by another. Typically, the first party suggests that the issue be resolved in particular ways, and the second party tries to avoid giving in to such demands. To pressure the other party to give in, such parties often act less cooperatively toward one another, and try to enlist allies to assist in this stance. Such conflicting coalitions can grow large, and the resulting feuds can be quite destructive, sometimes escalating into full scale war.

The larger society has an interest in resolving such disputes fairly, as the expectation of fair future resolutions can encourage better behavior. But that larger society has an even stronger interest in just resolving disputes somehow, anyhow, to prevent the accumulation of destructive feuds. So for roughly a million years, humans have used informal group norm enforcement. If a forager had a complaint about someone else, they could tell their band, and that band would discuss it and come to a consensus on how to resolve the issue. The band would then apply increasing pressures to get the disputing parties to abide by their decision, and to stop any feud. <span class="Apple-converted-space"> </span>

During the farming era, we formalized this practice as law, which lowered costs of making and enforcing group decisions on how to resolve particular conflicts. But the key idea remains: prevent escalating feuds via having relatively independent judges declare resolutions, and pressuring parties to respect them. Hopefully fair resolutions, but more importantly clear and widely accepted ones. Pressure parties and their allies not only to do what resolutions say, but also to publicly accept such decisions as resolving their conflicts.<span class="Apple-converted-space"> </span>

That is, we want people who have been loudly declaring their dispute to publicly put it behind them. For example, by treating ex-cons as “having paid their debt to society”. We’d like these legal resolutions to be reliable and predictable, to give people incentives to behave well and not do things that cause disputes. And when disputable events happen, we want the involved parties to have incentives to quietly make a deal to resolve them, so as to avoid larger social conflict and the need for a formal legal resolution.<span class="Apple-converted-space"> </span>

For a very long time, most legal conflicts have been resolved via cash transfers. Not always, of course; crimes often need more punishment than fines can produce. (At least without selling people into slavery or requiring [crime](requirelegalliabilityinsurance) [insurance](privately-enforced-punished-crime).) But cash makes many things easier, including trade and charity. Yes, cash doesn’t always make the best symbolic statement. Even so, law usually uses cash because it is an admirably robust measure of value across a wide range of groups and social contexts. <span class="Apple-converted-space"> </span>
Which brings me to the current US political conflict, and the topic of reparations for slavery and racism. Our political climate seems today to be drifting toward a war-like lack of restraint. And “grievances” seem an important part of this conflict. One side at least claims to represent wronged parties, parties whose wrongs have not been adequately addressed. And one especially big and long-lasting grievance has been about our history of raced-based slavery, and related racism. Many say that we have not adequately addressed this complaint.<span class="Apple-converted-space"> </span>

My main point here is that cash reparations for past slavery and racism harms make a lot of sense in the context of the general history and purpose of law. We have been suffering from a costly long-standing political feud, a law-like resolution could help us resolve and get past that feud, and cash transfers are our standard go-to way to resolve law-like conflicts.

I’m not going to argue for any particular level of compensation, nor for any particular interpretation of particular cases of precedent. I can believe that precedent isn’t clear here, and that many issues and complexities are in play. But complexity needn’t prevent resolution; we rely on law all the time to resolve complex disputes. In fact, in terms of avoiding wider social conflict, law is probably more socially valuable in more complex cases.<span class="Apple-converted-space"> </span>

Yes, reparations today for wrongs from long ago does require some form of vicarious liability, wherein the people who lose and those who gain from a cash transfer aren’t the same as those who did wrongs and who were harmed. But we actually use many forms of vicarious liability in law today, and ancient societies used it a lot more. <span class="Apple-converted-space"> </span>

Some fear that even after paying reparations, racism-complaint-based conflict would persist unabated. Others fear the opposite, that many would feel that we could cut back on other responses to racism, such as affirmative action, and “put the issue behind us”, risking complacency on future problems. Here I must come down strongly in favor of risking complacency.<span class="Apple-converted-space"> </span>

One of the main goals of law, and of humanity’s more ancient norm enforcement, has been to try to get disputes resolved, to give them a better chance of fading away. Yes, it remains possible that past wrongs will be repeated in the future. But to always presume that is to never allow disputes to be resolved, and to instead accumulate escalating complaints and feuds until war becomes nearly inevitable.<span class="Apple-converted-space"> </span>

If our national legal system isn’t up to the task of resolving this conflict, or isn’t seen as neutral enough by important audiences, I have a simple proposal: randomly pick 13 adults from the whole world, let them each pick one legal advisor, then isolate them all in a room and have them work together as a jury to pick a resolution. When they must pick a number, let them just use a median vote (each submits a number, the median of which is the answer). Finally, let the whole world apply social pressure to get everyone to accept this as the most neutral and independent resolution likely to be available anytime soon. Accept it, implement it, and then let it go.<span class="Apple-converted-space"> (If you worry about one side betraying the resolution later, consider spreading cash payments out over a long time period.) </span>

When conflict appears in a marriage, the couple sometimes seeks a counselor, who often offers neutral independent advice on how to resolve their conflict. Which is helpful when partners actually do want to resolve a conflict. But sometimes they prefer war, and the marriage ends. Similarly an independent reparations recommendation can’t force us to resolve our conflict over racism and slavery, if what we really want is all out war. But as with a feuding couple, if we think there’s still a chance that we’ll want to stay together, we might still give the independent counselor thing a try. <span class="Apple-converted-space"> </span>

Yes, like you I hear of many who seem eager for all-out war, as they feel confident they will win. For example, some intend to crush all opposition within the elite professions that they expect to dominate, such as journalism, academia, government, social media tech, and even law. But while such people do exist, social media exaggerates their numbers. It is not yet too late to step back from the brink, and reconcile. Via something like law.<span class="Apple-converted-space"> </span>

In a <a href="https://twitter.com/robinhanson/status/1101507003850113024">recent</a> Twitter poll, I found that 800 respondents favored cash reparations (CR) 4-1 over affirmative action (AA) as a way to deal with past and present racism, including race-based slavery:


If you can only pick 1, which kind of policy do you prefer to deal with past & current racism (including race-based slavery): cash reparations, or preferential treatment at school, jobs, etc?

> — Robin Hanson (@robinhanson) <a href="https://twitter.com/robinhanson/status/1101113087963422720?ref_src=twsrc%5Etfw">February 28, 2019</a>

<script async="" charset="utf-8" src="https://platform.twitter.com/widgets.js"></script><br/>
My 73 facebook poll respondents favored CR over AA 87% to 13%. Yes, there are reasons to doubt a wider public shares this judgment, but <a href="https://news.gallup.com/opinion/polling-matters/247178/reparations-black-americans-attitudes-race.aspx">three</a> different polls find at least that majorities of blacks favor cash reparations. The idea isn’t crazy.

<strong>Added 3pm:</strong> Over the weekend, I paid for nationally representative surveys via Google Surveys. When I asked the above question except with “just show results” replaced by “I don’t know” (IDK), then out of 220, IDK got 77%, AA 14%, and CR 9%. I initially paid for a much bigger survey, but bailed when I saw so many IDK. I tried again without the IDK option, and out of 1154, AA got 53% and CR 47%. I agree that these stats aren’t very supportive of a majority favoring CR over AA.

I interpret these stats as Google Survey respondents trying to answer as fast as they can to get paid more faster, and so only giving accurate opinions when such can be generated very quickly. If the question looks at all complex, then they pick an IDK or “none of the above” if they see one, and otherwise pick randomly. I’d pay a lot more for surveys where the same person is asked the same question a week apart, and only gets paid if their answers match.

<strong>Added 6Mar</strong>: Almost all responses are critical, from folks who apparently don’t want any reparations. They mainly complain that this case would be difficult to judge from a legal precedent point of view. But we almost never refuse to have a legal proceeding on the basis of difficulty of judging. If it seems plausible that a judge might find for the plaintiff, the case goes forward. A judge might then rule for the defendant because it seems too hard to find a clear enough reason to rule otherwise. But that’s after a proceeding, not before. I’m okay if the jury of 13 that I suggested picks, after much deliberation, a median compensation of zero; no reparations.

<strong>Added 8Mar</strong>: David Brooks <a href="https://www.nytimes.com/2019/03/07/opinion/case-for-reparations.html">comes out</a> in favor of reparations:

> Reparations are a drastic policy and hard to execute, but the very act of talking about and designing them heals a wound and opens a new story.

George Will <a href="https://www.kansascity.com/opinion/opn-columns-blogs/syndicated-columnists/article227324389.html">opposes</a> reparations because they’d be complicated.

<strong>Added 10Mar</strong>: A <a href="https://twitter.com/lukefreeman/status/1104618236778291200">Postily poll</a> of 283 finds 28% prefer AA, 36% prefer CR, 36% say IDK. Non-whites like CR more across the board, but even whites favor it 33% to 27%. Masters degrees & higher prefer AA. Democrats prefer AA over CR 45% to 26%. Oddly, all regions but the South preferred AA over CR.

## [Regulating Infinity](#table-of-contents)
_Posted on 2014-08-17_

As a professor of economics in the GMU Center for the Study of Public Choice, I and my colleagues are well aware of the many long detailed disputes on the proper scope of regulation.

One the one hand, the last few centuries has seen increasing demands for and expectations of government regulation. A wider range of things that might happen without regulation are seen as intolerable, and our increasing ability to manage large organizations and systems of surveillance is seen as making us increasingly capable of discerning relevant problems and managing regulatory solutions.

On the other hand, some don’t see many of the “problems” regulations are set up to address as legitimate ones for governments to tackle. And others see and fear regulatory overreach, wherein perhaps well-intentioned regulatory systems actually make most of us worse off, via capture, corruption, added costs, and slowed innovation.

The poster-children of regulatory overreach are 20th century totalitarian nations. Around 1900, many were told that the efficient scale of organization, coordination, and control was rapidly increasing, and nations who did not follow suit would be left behind. Many were also told that regulatory solutions were finally available for key problems of inequality and inefficient resource allocation. So many accepted and even encouraged their nations to create vast intrusive organizations and regulatory systems. These are now largely seen to have gone too far.

Or course there have no doubt been other cases of regulatory under-reach; I don’t presume to settle this debate here. In this post I instead want to introduce jaded students of regulatory debates to something a bit new under the sun, namely a newly-prominent rationale and goal for regulation that has recently arisen in a part of the futurist community: stopping preference change.

In history we have seen change not only in technology and environments, but also in habits, cultures, attitudes, and preferences. New generations often act not just like the same people thrust into new situations, but like new kinds of people with new attitudes and preferences. This has often intensified intergenerational conflicts; generations have argued not only about who should consume and control what, but also about which generational values should dominate.

So far, this sort of intergenerational value conflict has been limited due to the relatively mild value changes that have so far appeared within individual lifetimes. But at least two robust trends suggest the future will have more value change, and thus more conflict:

<ol>
<li><strong>Longer lifespans</strong> – Holding other things constant, the longer people live the more generations will overlap at any one time, and the more different will be their values.</li>
<li><strong>Faster change</strong> – Holding other things constant, a faster rate of economic and social change will likely induce values to change faster as people adapt to these social changes.</li>
<li><strong>Value plasticity</strong> – It may become easier for our descendants to change their values, all else equal. This might be via stronger ads and schools, or direct brain rewiring. (This trend seems less robust.)</li>
</ol>
These trends robustly suggest that toward the end of their lives future folk will more often look with disapproval at the attitudes and behaviors of younger generations, even as these older generations have a smaller proportional influence on the world. There will be more “Get off my lawn! Damn kids got no respect.”

The futurists who most worry about this problem tend to assume a worst possible case. (Supporting quotes below.) That is, without a regulatory solution we face the prospect of quickly sharing the world with daemon spawn of titanic power who share almost none of our values. Not only might they not like our kind of music, they might not like music. They might not even be conscious. One standard example is that they might want only to fill the universe with paperclips, and rip us apart to make more paperclip materials. Futurists’ key argument: the space of possible values is vast, with most points far from us.

This increased intergenerational conflict is the new problem that tempts some futurists today to consider a new regulatory solution. And their preferred solution: a complete totalitarian takeover of the world, and maybe the universe, by a new super-intelligent computer.

You heard that right. Now to most of my social scientist colleagues, this will sound bonkers. But like totalitarian advocates of a century ago, these new futurists have a two-pronged argument. In addition to suggesting we’d be better off ruled by a super-intelligence, they say that a sudden takeover by such a computer will probably happen no matter what. So as long as we have to figure out how to control it, we might as well use it to solve the intergenerational conflict problem.

Now I’ve already discussed at some length why I don’t think a sudden (“foom”) takeover by a super intelligent computer is likely (see [here](debate-is-now-book), [here](30855), [here](limits-on-generality)). Nor do I think it obvious that value change will generically put us face-to-face with worst case daemon spawn. But I do grant that increasing lifespans and faster change are likely to result in more intergenerational conflict. And I can also believe that as we continue to learn just how strange the future could be, many will be disturbed enough to seek regulation to prevent value change.
Thus I accept that our literatures on regulation should be expanded to add one more entry, on the problem of intergenerational value conflict and related regulatory solutions. Some will want to regulate infinity, to prevent the values of our descendants from eventually drifting away from our values to parts unknown.

I’m much more interested here in identifying this issue than in solving it. But if you want my current opinion [it is](humanity-cant-steer-its-future-much) that today we are just not up to the level of coordination required to usefully control value changes across generations. And even if we were up to the task I’m not at all sure gains would be worth the quite substantial costs.
<strong>Added 8a:</strong> Some think I’m unfair to the fear-AI position to call AIs our descendants and to describe them in terms of lifespan, growth rates and value plasticity. But surely AIs being made of metal or made in factories aren’t directly what causes concern. I’ve tried to identify the relevant factors but if you think I’ve missed the key factors do tell me what I’ve missed.

<strong>Added 4p</strong>: To try to be even clearer, the standard worrisome foom scenario has a single AI that grows in power very rapidly and whose effective values drift rapidly away from ones that initially seemed friendly to humans. I see this as a combination of such AI descendants having faster growth rates and more value plasticity, which are two of the three key features I listed.

<strong>Added 15Sep</strong>: A version of this post <a href="http://www.globalgovernmentventuring.com/magazines">appeared</a> as:

Robin Hanson, Regulating Infinity, <em>Global Government Venturing</em>, pp.30-31, September 2014.

Those promised supporting quotes:<span id="more-30875"></span>

First, [David Chalmers](seek-peace-not-values):
If humans survive, the rapid replacement of existing human traditions and practices would be regarded as subjectively bad by some but not by others. … The very fact of an ongoing intelligence explosion all around one could be subjectively bad, perhaps due to constant competition and instability, or because certain intellectual endeavours would come to seem pointless. On the other hand, if superintelligent systems share our values, they will presumably have the capacity to ensure that the resulting situation accords with those values. …

<p style="padding-left: 30px;">If at any point there is a powerful AI+ or AI++ with the wrong value system, we can expect disaster (relative to our values) to ensue. The wrong value system need not be anything as obviously bad as, say, valuing the destruction of humans. If the AI+ value system is merely neutral with respect to some of our values, then in the long run we cannot expect the world to conform to those values. ([more](seek-peace-not-values), [see](chalmers-reply-2) [also](when-is-soon))
Second, <a href="http://slatestarcodex.com/2014/07/30/meditations-on-moloch/">Scott Alexander</a>:

The current rulers of the universe – call them what you want, Moloch, Gnon, Azathoth, whatever – want us dead, and with us everything we value. Art, science, love, philosophy, consciousness itself, the entire bundle. And since I’m not down with that plan, I think defeating them and taking their place is a pretty high priority.

The opposite of a trap is a garden. The only way to avoid having all human values gradually ground down by optimization-competition is to install a Gardener over the entire universe who optimizes for human values.

And the whole point of Bostrom’s Superintelligence is that this is within our reach. Once humans can design machines that are smarter than we are, by definition they’ll be able to design machines which are smarter than they are, which can design machines smarter than they are, and so on in a feedback loop so tiny that it will smash up against the physical limitations for intelligence in a comparatively lightning-short amount of time. If multiple competing entities were likely to do that at once, we would be super-doomed. But the sheer speed of the cycle makes it possible that we will end up with one entity light-years ahead of the rest of civilization, so much so that it can suppress any competition – including competition for its title of most powerful entity – permanently. In the very near future, we are going to lift something to Heaven. It might be Moloch. But it might be something on our side. If it is on our side, it can kill Moloch dead.

And so if that entity shares human values, it can allow human values to flourish unconstrained by natural law.

Third, Nick Bostrom in <em>Superintelligence</em>:

We suggested earlier that machine intelligence workers selected for maximum productivity would be working extremely hard and that it is unknown how happy such workers would be. We also raised the possibility that the fittest life forms within a competitive future digital life soup might not even be conscious. Short of a complete loss of pleasure, or of consciousness, there could be a wasting away of other qualities that many would regard as indispensible for a good life. Humans value music, humor, romance, art, play, dance, conversation, philosophy, literature, adventure, discovery, food and drink, friendship, parenting, sport, nature, tradition, and spirituality, among many other things. There is no guarantee that any of these would remain adaptive. …

We have seen that multipolarity, even if it could be achieved in a stable form, would not guarantee an attractive outcome. The original principal– agent problem remains unsolved, and burying it under a new set of problems related to post-transition global coordination failures may only make the situation worse. Let us therefore return to the question of how we could safely keep a single superintelligent AI.

## [Privately Enforced & Punished Crime](#table-of-contents)
_Posted on 2018-01-15_

I’ve been teaching law & economics for many years now, and have slowly settled on the package legal reforms for which I most strongly want to argue. I have chosen a package that seems big enough to inspire excitement and encompass synergies, and yet small enough to allow a compelling analysis of its net benefits.

My proposal is regarding how to detect, prosecute, and punish criminal law. It is not about non-criminal law, and it is <em>not</em> a proposal to change how we decide what acts are crimes, when to be persuaded by a particular crime accusation, how hard to work to discourage each criminal act, nor how hard to work to catch each criminal act. To start, I hold constant how we do these other things.<span id="more-31715"></span>

Now you might happen to think that we are very bad today at labeling acts as crimes, evaluating accusations, and choosing detection and discouragement priorities. So bad in fact that you think it a bad idea to make law more effective at detecting, prosecuting, and punishing crime. You might then prefer to reduce our criminal law enforcement abilities, such as via slashing police budgets or arbitrarily hindering police. For example, by requiring police to wear big bricks on their ankles. And you might not want my proposal adopted until you first figure out how to fix the rest of criminal law. But you might still want my solution available for later. I’d be happy to work with you to design fixes for those other parts of the system. Just not today, not in this post. Today I focus on detection, prosecution, and punishment of crime.

Non-crime law deals mostly with accidents and mild sloppy selfishness among parties who are close to each other in a network of productive relations. In such cases, law can usually require losers to pay winners cash, and rely on those who were harmed to detect and prosecute violations. This approach, however, can fail when “criminals” make elaborate plans to grab gains from others in ways that make they, their assets, and evidence of their guilt hard to find.

Ancient societies dealt with crime via torture, slavery, and clan-based liability and reputation. Today, however, we have less stomach for such things, and also weaker clans and stronger governments. So a modern society instead assigns government employees to investigate and prosecute crimes, and gives them special legal powers. But as we don’t entirely trust these employees, we limit them in many ways, including via juries, rules of evidence, standards of proof, and anti-profiling rules. We also prefer to punish via prison, as we fear government agencies eager to collect fines. Yet we still suffer from a great deal of police corruption and mistreatment, because government employees can coordinate well to create a <a href="https://en.wikipedia.org/wiki/Blue_wall_of_silence">blue wall of silence</a>.

I propose to instead privatize the detection, prosecution, and punishment of crime. My proposal isn’t especially original; many similar changes have been proposed before, and I can’t be bothered to do sufficient historical research to support public claims on who deserves how much credit for which suggestions. So I’ll instead just argue for a certain package of changes, and let historians work out the credit.

The key idea is to use competition to break the blue wall of silence, via allowing many parties to participate as bounty hunter enforcers and offering them all large cash bounties to show us violations by anyone, including by other enforcers. With sufficient competition and rewards, few could feel confident of getting away with criminal violations; only court judges could retain substantial discretionary powers.

That is, all laws are enforced, and enforced equally on everyone everywhere. Police and prosectors no longer have discretion to quietly ignore particular kinds of crimes like jaywalking that they think unworthy of their efforts. Or to prioritize crime in rich neighborhoods over poor ones. Or refuse to prosecute crimes by police or friends of the mayor. Enforcers are fully responsible for any harms they cause in attempting to pursue criminals. The legal system could still prioritize some people, acts, or neighborhoods, if it did so openly and explicitly, via officially declared legal parameters.

Okay, let’s get to the details. There are three big related changes:

<strong>Full Legal Liability Insurance</strong> – To get and keep a driver’s license today, one must usually show insurance able to pay others a certain minimum amount if found liable in a car accident. I propose to [require](requirelegalliabilityinsurance) that everyone get insurance to cover a high maximum liability for legal violations. At least a few million dollars. The main requirement to be an insurance firm here is to have deep enough pockets to pay client liabilities. Ideally clients and their insurers would retain a lot of freedom to make deals of their choosing, including over premiums, co-payments, etc., as long as it remains clear which insurers are responsible for paying for which clients. With full legal liability insurance, parties with deep pockets would sit on both sides of all criminal cases. No longer could a deep pocket side gain by threatening to force the other side to spend more than they have.
<strong>Cash Fines For Most Everything</strong> – With deep pocket insurers available to pay, we can use fines to punish most all legal violations. This makes it possible to avoid prisons for punishment, which are terribly inefficient. And if the rest of us feel that these fines sufficiently compensate us for crimes, we could make clients and insurers free to choose many details of how best to prevent and minimize the costs from crime, choices we now make awkwardly and centrally, taking too little context into account. Clients and insurers could make trades between preventing crime via monitoring and limiting client actions before a crime, or preventing crime via fear of punishment and loss after a crime. They might agree on (and arrange to pay for) specific amounts and types of punishment, such as fines, prison, [torture](torture-kids-instead), [exile](consider-exile), or even death, in the event the client were found guilty of particular crimes. Only the crime of not obtaining full legal liability insurance might need an official non-fine punishment. Strong monitoring and punishments make sense here; it should be possible to make this a very rare situation.
<strong>Cash-Paid Bounty Hunters</strong> – A [bounty hunter](expand-bounty-hunting) is a party who pays to collect evidence on a crime, who pays to convince a court that a particular party is guilty, and who gets paid a bounty upon obtaining such a conviction. For each possible crime the legal system must set both a bounty and a fine, or a process for setting such quantities, which could depend on context (such as criminal wealth). The fine tells the insurer and client how hard to work to prevent violations, and the bounty tell hunters how hard to work to catch violations. A bounty that rises with the duration since a crime could pay small bounties for easy to catch fines, and large bounties for hard to catch ones. Large differences between bounties and fines, however, can induce the creation of crime, either by hunters to get bounties or governments to get fines.
Okay, that’s the key package of big changes. Now lets discuss some details.

With bounty hunters, I’m not sure how to handle privacy and property rights about evidence regarding a possible crime. Hunters could be given extra abilities to penetrate privacy, and might temporarily acquire exclusive property rights over evidence that they have collected. I’m pretty sure that if people like the rest of this legal reform package, we could work out reasonable solutions on these issues. And I’m not sure its worth the bother to work out such solutions in the absence of seeing sufficient interest.

As per <a href="http://elephantinthebrain.com">my new book</a>, citizens may prefer hypocritical legal systems, which pretend to do one thing, but really do another. Hypocrisy can sometimes be achieved by agents with discretion, who can say they are doing one thing but are given opaque incentives to do another. Today police, prosectors, and judges all have discretion. In my alternative proposal, only judges can retain much discretion, to decide if particular people are guilty, and perhaps also case to decide case-specific fines and bounties. If citizens want hypocrisy, but judge discretion is insufficient to achieve it, this becomes a disadvantage of my proposal. Even so, we might study reactions to a proposal like this to help identify common hypocrisies regarding legal policy.

It is in general very important for legal systems set legal liability carefully. The power to make others more liable to you, just because you want to get paid more, is a huge and dangerous power. For example, if we make big firms pay little people like us huge amounts for being harmed by product defects, just because we sympathize more with them relative to big firms, well that can cause great harm, such as overly cautious product safety. This issue becomes somewhat more important in my proposal, as all laws are fully enforced and insurers’ deeper pockets allow for larger liability payments. If we are handling this problem reasonable well now, however, we can probably continue to manage it well enough. But if this problem gets bad enough, it might be better not to have any legal system.

This system could include redistribution to help poor people with high legal liability premiums. For example, someone with a long conviction history might only be able to afford premiums if they agreed to stay on a gated and isolated work farm. The rest of us would have to decide whether to subsidize this person to gain more lifestyle options. Note that today such a person might be just left in prison. Note also that we don’t today offer special cheap insurance to low income folks who have caused many auto accidents; we instead want them off the road.  As premiums depend on the deals that clients accept on punishments, monitoring, and action limitations, redistribution polices would implicitly make judgements about which such deals are seen as reasonable. Even so, this might still give individuals more flexibility than in our world of criminal law. A related option is to create standard public insurance options open to all.

Relative to non-criminal law, we today make it more difficult to obtain convictions in criminal law, such as via juries and higher standards of proof. If this is because we less trust government police and prosecutors not to collude with judges, then we might see this as less of a problem in a more open system of competitive bounty hunters. If so, we could cut juries and lower standards of proof, and then catch more crimes more cheaply. There are also [other](double-or-nothi) [ways](blackmail-enforces-law) we might plausibly lower court costs without much lowering the accuracy of decisions.

## [Fine Grain Futarchy Zoning Via Harberger Taxes](#table-of-contents)
_Posted on 2019-01-25_

“<a href="http://hanson.gmu.edu/futarchy.html">Futarchy</a>” is my proposed system of governance which approves a policy change when conditional prediction markets give a higher expected outcome, conditional on that change. In a city setting, one might be tempted to use a futarchy where the promoted outcome is the total property value of all land in and near that city. After all, if people don’t like being in this city, and are free to move elsewhere, city land won’t be worth much; the more attractive a city is as a place to be, the more its property will be worth.

Yes, we have problems measuring property values. Property is only traded infrequently, sale prices show a marginal not a total value, much land is never offered for sale, sales prices are often obscured by non-cash terms of trade, and regulations and taxes change sales and use. (E.g., rent control.) In addition, we expect at least some trading noise in the prices of any financial market. As a result, simple futarchy isn’t much help for decisions whose expected consequences for outcomes are smaller than its price noise level. And yes, there are other things one might care about beside property values. But given how badly city governance often actually goes, we could do a lot worse than to just consistently choose policies that maximize a reasonable estimate of city property value. The more precise such property estimates can be, the more effective such a futarchy could be.

Zoning (and other policy that limits land use) is an area of city policy that seems especially well suited to a futarchy based on total property value. After all, the main reason people <a href="https://www.collectorsweekly.com/articles/demolishing-the-california-dream/?fbclid=IwAR2ZXkzOUeVhapITr00RR-Ka0tMAnnhiU0--7bLl-itz3BVehUDpUG0ghCY">say</a> that we need zoning is because using some land in some ways decreases how much people are willing to pay to use other land. For example, people might not want to live next to a bar, liquor store, or sex toy store, are so are willing to pay less to buy (or rent) next to such a place. So choosing zoning rules to maximize total property value seems especially promising.

I’ve also [written](for-stability-rents) [before](between-property-and-liability) [favorably](radical-markets) on Harberger taxes (which I once called “stability rents”). In this system, owners of land (and property tied to that land) must set and may continuously adjust a declared property “value”; they are taxed per unit time as a percentage of momentary value, and must always agree to sell their property at their currently declared value. This system has great advantages in inducing property to be held by those who can gain the most value from it, including via greatly lowering the transaction costs of putting together big property packages. With this system, there’s no more need for eminent domain.
I’ve just noticed a big synergy between futarchy for zoning and Harberger taxes. The reason is that such taxes allow the creation of prices which support a much finer grain accounting of the net value of specific zoning changes. Let me explain.

First, Harberger taxes create a continuous declared value on each property all the time, not just a few infrequent sales prices. This creates a lot more useful data. Second, these declared values better approximate the value that people place on property; the higher an actual value, the higher an owner will declare his or her taxable value to be, to avoid the risk of someone taking it away. Third, these declared values are all relative to a standard terms of trade, not the varying terms of actual sales today. Thus the sum total of all declared property values can be a decent estimate of total city property value. Fourth, it is possible to generalize the Harberger tax system to create zoning-conditional property ownership and prices.

That is, relative to current zoning rules, one can define a particular alternative zoning scenario, wherein the zoning (or other property use limit) policies have changed. Such as changing the zoning of a particular area from residential to commercial on a particular date. Given such a defined scenario, one can create conditional ownership; I own this property <em>if</em> (and when) this zoning change is made, but not otherwise. The usual ownership then becomes conditional on no zoning changes soon.

With conditional ownership, conditional owners can make conditional offers to sell. That is, you can buy my property under this condition if you pay this declared amount of conditional cash. For example, I might offer to make a conditional sale of my property for $100,000, and you might agree to that sale, but this sale only happens if a particular zoning change is approved.

The whole Harberger tax system can be generalized to support such conditional trading and prices. In the simple system, each property has a declared value set by its owner, and anyone can pay that amount at any time to become the new owner. In the generalized system, each property has a declared value for each (combination of) approved alternative zoning scenario. By default, alternative declared values are equal to the ordinary no-zoning-change declared value, but property owners can set them differently if they want, to be either higher or lower. Anyone can make a scenario-conditional purchase of a property from its current (conditional) owner at its scenario-conditional declared value. To buy a property for sure, buy it conditional on all scenarios.

(For concreteness, assume that only one zoning change proposal is allowed per day per city region, that a decision is made on that proposal in that day, and that the proposal for each day is chosen via open public auction a month before. The auction fee can subsidize markets in bets on if this proposal will be approved and markets in tax-revenue asset conditional differences (explained below). A week before the decision day of a proposal, each right in a property is split into two conditional rights, one conditional on this change and one on not-this-change. At that point, owner declared values conditional on this change (or not) become active sale prices. Taxes are paid in conditional cash. Physical control of a property only transfers to conditional owners if and when a zoning scenario is actually approved.)

Having declared values for all properties under all scenarios gives us even more data with which to estimate total city property value, and in particular helps with estimating the <em>difference</em> in total city property value due to a zoning change. To a first approximation, we can just add up all the zoning-change-conditional declared values, and compare that sum to the sum from the no-change declared values. If the former sum is consistently and clearly higher than the latter sum over the proposal’s decision day, that seems a good argument for adopting this zoning proposal. (It seems safer to choose the higher value option with a chance increasing in value difference, and this all works even when other factors influence a decision.) At least if the news that this zoning proposal seems likely be approved gets spread wide and fast enough for owners to express their conditional declared values. (The bet markets on which properties will be effected helps to notify owners.)

Actually, to calculate the net property value difference that a zoning change makes, we need only sum over the properties that actually have a conditional declared value different from its no-change declared value. For small local zoning changes, this might only be a small number of properties within a short distance of the main changes. As a result, this system seems capable of giving useful advice on very small and local zoning changes, in dramatic contrast to a futarchy based on prices estimating total city property values. For example, it might even be able to say if a particular liquor store should be allowed at a particular location, or if the number of required parking spots at a particular shopping mall can be reduced. As promised, this new system offers much finer grain accounting of the net value of specific zoning changes.

Note that in this system as described, losers are not compensated by winners for zoning rule changes, even though we can roughly identify winners and losers. I’ve thought a bit about ways to add a extra process by which winners compensate losers, but haven’t been able to make that work. So the best I can think of is to have the system look at the distribution of wins and losses, and reject proposed changes where there are too many big losers relative to winners. That would force a search for variations which spread out the pain more evenly.

We are close to a workable proposal, but not quite there yet. This is because we face the problem of owners temporarily inflating their declared values conditional on a zoning change that they seek to promote. This might tip the balance to get a change approved, and then after approval such owners could cut their declared values back down to something reasonable, and only pay a small extra tax for that small decision period. Harberger taxes impose a stronger penalty for declaring overly-low values than overly-high values.

A solution to this problem is to use, instead of declared values, prices for the purely financial assets that represent claims on all future tax revenue from the Harberger tax on a particular property. That is, each property will pay a tax over time, we could divert that revenue into a particular account, and an asset holder could own the right to spend a fraction of the funds in that account. Such tax-revenue assets could be bought and sold in financial markets, and could also be made conditional on particular zoning scenarios. As such assets are easy to create and duplicate, the usual speculation pressures should make it hard to manipulate these prices much in any direction.

A plan to temporarily inflate the declared value of a property shouldn’t do much to the market price for a claim to part of all future tax revenue from that property. So instead of summing over conditional differences in declared-values to see if a zoning change is good, it is probably better to sum over conditional differences in tax revenue assets. Subsidized continuous market makers can give exact if noisy prices for all such differences, and for most property-scenario pairs this difference will be exactly zero.

So that’s the plan for using futarchy and Harberger taxes to pick zoning (and other land use limit policy) changes. Instead of just one declared value per property, we allow owners to specify declared values conditional on each approved zoning change (or not) scenario, and allow conditional purchases as well. By default, conditional values equal no-change values. We should tend more to adopt zoning proposals when, during its decision day, when the sum of its (tax-revenue-asset) conditional differences clearly and consistently exceeds zero.

Thanks to Alex Tabarrok & Keller Scholl for their feedback.

<strong>Added 11pm:</strong> One complaint people have about a Harberger tax is that owners would feel stressed to know that their property could be taken at any time. Here’s a simple fix. When someone takes your property at your declared value, you can pay 1% of their new declared value to get it back, if you do so quickly. But then you’d better raise your declared value or someone else could do the same thing the next day or week. You pay 1% for a fair warning that your value is too low. Under this system, people only lose their property when someone else actually values it more highly, even after considering the transaction costs of switching property.

<strong>Added 2Feb:</strong> I edited this post a bit. Note that with severe enough property limits, <em>negative</em> declared property values can make sense. For example, if a property must be maintained so as to serve as a public park, the only people willing to become owners are those who get paid when they take the property, and then get paid per unit time for remanning owners. In this way, city services can be defined and provided via this same decision mechanism.

<strong>Added 11July:</strong> On reflection, there’s not much need for the special 1% grab-back rule I proposed above. While it might be good rhetoric to allay fears, it isn’t actually needed. In principle it could reduce your loss from setting too low a price, but in practice I don’t think it will be possible to underprice that much; speculators will buy underpriced assets intending to sell them back.

Assuming that there’s a standard delay in transferring property, the moment someone grabs your declared value price, they must declare a new value. So you are either willing to grab it back at that price, and then set a new higher value, or you accept that they have a higher value for the property and can keep it. If you grab it back and set a higher value, they can of course take it at that new value; you can in effect go back and forth in an auction to see who values it more. Each time they grab from you will regret not having set a higher value; so this won’t go many rounds and will be settled quickly.

<strong>Added 21Oct2021:</strong> It has been pointed out to me that allowing owners to pay 1% of property value to get it back is at odds with allowing people to buy up big blocks all at once. Right after a big block is purchased, parts of that block could dramatically raise their prices, as a way to threaten to destroy the block. So I think this ability to pay 1% to get your property back should <em>not</em> apply when many properties are purchased at once as part of a block.

## [Fear Made Farmers](#table-of-contents)
_Posted on 2010-10-06_

Farming required huge behavior changes, mostly unnatural to foragers.  A key enabler seems to have been increased self-control to follow social norms.  But what allowed this increased self-control?

One source was moving from vague spirituality to religions with powerful and morally-outraged gods who punish norm violators.  In addition (as I’ll explain tomorrow), high densities and larger social networks made stronger credible threats to ostracize folks for specific deviant acts.  Yes both these mechanisms require the fear that norm violations could lead to great harm, even death.  But for poor farmers living on edge, such threats were easy to come by.

Interestingly, this death-threat pressure could work even without farmers being conscious of the relevant threats or fears.  In fact, farming society probably worked <em>better</em> with [homo hypocritus](homo-hipocritus) farmers, consciously denying that strong social pressures pushed them to do what would otherwise feel unnatural.
A large robust literature makes it clear that inducing people to unconsciously think about death pushes them to more strongly obey and defend cultural norms, especially norms framed as disgust at animal-like behavior.  Today, fear of death encourages folks to obey authorities, and be more loyal to their communities and spouses, all strong farmer norms:

Empirical support for [Terror management theory] has originated from more than 175 published experiments which have been conducted cross-culturally both nationally and internationally. … People, when reminded of their own inevitable death, will cling more strongly to their cultural worldviews. …. Nations or persons who have experienced traumas are more attracted to strong leaders who express traditional, pro-establishment, authoritarian viewpoints. … Many terror management studies have examined elicited affect as a covariate to mortality salience, and only one reviewed study has found elicited affect (fear) in the terror management process. Why? Terror management is a non-conscious process. …

Research corroborates the link between love and the fear of death. Studies reveal an association between close relationship seeking and mortality salience. Moreover, further studies demonstrate that the desire for close relationships under conditions of mortality salience trumps other needs including self-esteem and maintenance (pride) or avoidance (shame/guilt) … [Researchers] find the rejection of animality or creatureliness to function as the central tendency driving disgust … Studies demonstrate that mortality salience is associated with the rejection of animal traits. (<a href="http://en.wikipedia.org/wiki/Terror_management_theory">more</a>)

Subtle reminders of death on a subconscious level motivates a statistically significant number of subjects to exhibit biased and xenophobic type behaviors, such as gravitating toward those who they perceive as culturally similar to themselves and holding higher negative feelings and judgments toward those they perceive as culturally dissimilar to themselves. (<a href="http://en.wikipedia.org/wiki/Flight_from_Death">more</a>)

Note that fear-of-death based norm-enforcement mechanisms should work better on poor folk for whom death is a more immediate threat. Farming culture took advantage of a prior natural fear of death to push farming ways, but as farmers got richer, such pressures weakened, inclining folks to revert to more natural-feeling forager ways.

I suspect that social scientists, even those favoring “behavioral” explanations, consistently neglect fear of ([thinking about](fear-of-near-death-thoughts)) death as an explanation of social phenomena. Social scientists also don’t like to think about death, and thinking about explanations involving fear of death makes social scientists think too much about death.
<strong>Added</strong>: tijmz <a href="http://tijmz.wordpress.com/2010/10/06/calling-conservatives-primitive/">points out</a> an ’08 <em>Science</em> <a href="http://www.sciencemag.org/cgi/content/abstract/321/5896/1667">study</a> showing more fear-sensitive folks are more conservative:

Individuals with measurably lower physical sensitivities to sudden noises and threatening visual images were more likely to support foreign aid, liberal immigration policies, pacifism,and gun control, whereas individuals displaying measurably higher physiological reactions to those same stimuli were more likely to favor defense spending, capital punishment, patriotism, and the Iraq War. Thus, the degree to which individuals are physiologically responsive to threat appears to indicate the degree to which they advocate policies that protect the existing social structure from both external (outgroup) and internal (norm-violator) threats.

Bryan reminded he that he <a href="http://econlog.econlib.org/archives/2007/12/the_oldfashione.html">pointed out</a> <a href="http://jonjayray.fortunecity.com/oldfas.html">this essay</a> arguing that “authoritarian personalities” looks more like “old-fashioned personalities”, a fact which emphasizes just how much opinion has moved in a less conservative direction over time.

## [Forage vs Farm Future](#table-of-contents)
_Posted on 2010-10-07_

The two biggest events of last million years, by far, are the transition from foraging to farming and then from farming to industry. Since industry began, humans have changed in many ways, some of which are puzzling, since there hasn’t been time for much genetic selection, and only limited time for cultural selection. Especially puzzling are big changes in our basic attitudes, and big variations in such attitudes between people and nations.

The ten thousand years since the farming transition, however, offers more time for genetic and cultural adaptation.  Yet ten thousand years is also short enough that we should expect much less than full adaptation. Some people and places should retain vestiges of forager ways, and variations in these vestiges should be important.

So it seems natural to try to explain key variations and changes in attitudes today as vestiges of the transition from foragers to farmers colliding with the vast increase in individual wealth that is the main effect of industry.  On Monday I [described](two-types-of-people) how foragers vs. farmers seems to do a decent job of capturing the rich-poor axis in the World Values Survey, which is related to today’s liberal/modern vs. conservative/traditional political axis. I suggested that the social pressures which encouraged farming behaviors were naturally stronger for the poor, predicting that people retreat to forager ways with increasing industry wealth.  The [rest](my-political-hypothesis) [of](fear-made-farmers) [the](towns-norm-best) week I explored two theories of why such social pressures reduce with wealth.
Today I want to consider what this theory implies about our future. First, it implies that if we continue to get richer, we should continue to see attitude changes in roughly the same directions. We should expect continued movement toward accepting school and workplace domination and ranking, and whatever other attitudes greatly enable industry to create wealth.  And regarding how we spend our increased wealth, we should expect a continued shift from farmer to forager style attitudes for a while. For example, we should expect less war and physical cruelty to humans and animals, and more forager-like sexual promiscuity and respect for the environment. This should make us feel more happy, relaxed, and natural. In the extreme, we might even end up (for a time) as foragers in bands wandering virtual robot-supported forests, absent predators, famines, or pandemics.

Yet in the long run, if our interactions remain competitive, we shouldn’t expect forager behavior to be anything like the most adaptive for our descendants’ future worlds. Neither should farming of course, but one might still wonder which offers the best basis for generating adaptation to those future worlds. And on that criteria, the farming style seem more promising.  Its not so much that farming ways adapted to a larger social world, more like the large social worlds we expect for our descendants. Its more that farming adapted at all – farming found ways to push foragers, whose ways had been changing very slowly by farming standards, rather quickly into doing quite unnatural things.  So farming meta-innovations, like religion, honor, politeness, etc., might well be usefully repurposed to get our descendants to adapt to even stranger future environments.

For example, ems, or whole brain emulations, are my best guess for the next big transition on the order of the farming and industry transitions.  Farmer-style stoicism, self-sacrifice, and self-control, detached as needed from farmer specifics like love of land or sexual monogamy, might well be more effective at creating acceptance of em-efficient lifestyles. Religious ems might, for example, better accept being deleted when new more efficient versions of themselves are introduced. “Onward Christian robots” might be the new sensibility. And em’s low incomes might help farmer-style [fear-based](fear-made-farmers) norm-enforcement to gain traction.
Perhaps you hope that an industry-refashioned forager style might adapt just as well to these new requirements.  But wishing won’t make it so.

## [Two Types of People](#table-of-contents)
_Posted on 2010-10-04_

I’m about to describe two types of people, A vs. B.  While reading their descriptions I want you to think about which people around you are more like type A or B.  Also ask yourself:  which type do you respect more?  Which would you rather be?

<strong>TYPE *A*</strong> folks eat a healthier more varied diet, and get better exercise.  They more love nature, travel, and exploration, and they move more often to new communities. They work fewer hours, and have more complex mentally-challenging jobs.  They talk more openly about sex, are more sexually promiscuous, and more accepting of divorce, abortion, homosexuality, and pre-marital and extra-marital sex.  They have fewer kids, who they are more reluctant to discipline or constrain. They more emphasize their love for kids, and teach kids to more value generosity, trust, and honesty.

Type A<strong> </strong>folks care less for land or material posessions, relative to people.  They spend more time on leisure, music, dance, story-telling and the arts. They are less comfortable with war, domination, bragging, or money and material inequalities, and they push more for sharing and redistribution. They more want lots of discussion of group decisions, with everyone having an equal voice and free to speak their mind. They deal with conflicts more personally and informally, and more prefer unhappy folk to be free to leave. Their leaders lead more by consensus.

<strong>TYPE *B*</strong> folks travel less, and move less often from where they grew up. They are more polite and care more for cleanliness and order. They have more self-sacrifice and self-control, which makes them more stressed and suicidal. They work harder and longer at more tedious and less healthy jobs, and are more faithful to their spouses and their communities. They make better warriors, and expect and prepare more for disasters like war, famine, and disease.  They have a stronger sense of honor and shame, and enforce more social rules, which let them depend more on folks they know less.  When considering rule violators, they look more at specific rules, and less at the entire person and what feels right.  Fewer topics are open for discussion or negotiation.

Type B folks believe more in good and evil, and in powerful gods who enforce social norms.  They envy less, and better accept human authorities and hierarchy, including hereditary elites at the top (who act more type A), women and kids lower down, and human and animal slaves at the bottom. They identify more with strangers who share their ethnicity or culture, and more fear others. They are less bothered by violence in war, and toward foreigners, kids, slaves, and animals. They more think people should learn their place and stay there. Nature’s place is to be ruled and changed by humans.

Types A and B map reasonably well onto today’s culture wars, with A the modern/liberal and B the traditional/conservative. It maps well to the [rich-poor axis](key-political-data) from the World Value Survey.  But in fact, type A vs. B are actually foragers vs. farmers. [The above summarizes many books and articles I’ve read over the last year.]  Which is my point: I think a lot of today’s political disputes come down to a conflict between farmer and forager ways, with forager ways slowly and steadily winning out since the industrial revolution. It seems we acted like farmers when farming required that, but when richer we feel we can afford to revert to more natural-feeling forager ways. The main exceptions, like school and workplace domination and ranking, are required to generate industry-level wealth. We live a farmer lifestyle when poor, but prefer to buy a forager lifestyle when rich. Why this should be will be the subject of my [next](my-political-hypothesis) few posts.

## [Forager v Farmer, Elaborated](#table-of-contents)
_Posted on 2017-08-31_

Seven years ago, after a year of reading up on forager lives, I first [started](two-types-of-people) [to](divide-forager-v-farmer) [explore](fear-made-farmers) a forager vs. farmer axis:
<p style="padding-left: 30px;">A lot of today’s political disputes come down to a conflict between farmer and forager ways, with forager ways slowly and steadily winning out since the industrial revolution. It seems we acted like farmers when farming required that, but when richer we feel we can afford to revert to more natural-feeling forager ways. The main exceptions, like school and workplace domination and ranking, are required to generate industry-level wealth. ([more](two-types-of-people))
Recently I decided to revisit the idea, to see if I could find a clearer story that accounts better for many related patterns. Here is what I’ve come up with.

Our primate ancestors lived in a complex Machiavellian social world, with many nested levels of allies each coordinating to oppose outside rival groups of allies, often via violence. Humans, however, managed to collapse most of those levels into one: what [Boehm](hail-christopher-boehm) has called a “<a href="https://en.wikipedia.org/wiki/Dominance_hierarchy#Reverse">reverse dominance hierarchy</a>.” Human bands were mostly on good terms with neighboring bands, who they met infrequently. Inside each band, the whole group used weapons and language to coordinate to enforce shared social norms, to create a peaceful egalitarian safe space.
Individuals who saw a norm violation could tell others, and then the whole band could discuss what to do about it. Once a consensus formed, the band could use weapons to enforce their collective decision. As needed, punishments could escalate from scolding to shunning to exile to death. Common norms included requirements to share food and protection, and bans on violence, giving orders, bragging, and creating subgroup factions.

This worked often, but not always. People retained general Machiavellian social abilities, and usually used them covertly, just out of view of group norm enforcement. But sometimes the power of the collective waned, and then many would switch to acting more overtly Machiavellian. For example, an individual or a pair of allies might become so powerful that they could openly defy the group’s disapproval. Or such a pair might violate norms semi-privately, and use a threat of strong retaliation to dissuade others from openly decrying their violations. Or a nearby rival group might threaten to attack. Or a famine or flood might threaten mass mortality.

In the absence of such threats, the talky collective was the main arena that mattered. Everyone worked hard to look good by the far-view idealistic and empathy-based norms usually favored in collective views. They behaved well when observed, learned to talk persuasively to the group, and made sure to have friends to watch and talk for them. They expressed their emotions, and acted like they cared about others.

When they felt on good terms with the group, people could relax and [feel](specific-vs-general-foragers-farmers) <a href="http://slatestarcodex.com/2013/03/04/a-thrivesurvive-theory-of-the-political-spectrum/">safe</a>. They then become more playful, and [acted](play-will-persist) [like](play-blindness) animals generally do when playful. Within a bounded safe space, behavior becomes more varied, stylized, artistic, humorous, teasing, self-indulgent, and emotionally expressive. For example, there is more, and more varied, music and dance. New possibilities are explored.
A feeling of safety includes feeling safe to form more distinct subgroups, without others seeing such subgroups as threatening factions. And that includes feeling safe to form groups that tend to argue together for similar positions within talky collective discussions, and to disagree with the larger group. After all, it is hard for a talky collective to function well unless members are allowed to openly disagree with one another.

But when the group was stressed and threatened by dominators, outsiders, or famine, the collective view mattered less, and people reverted to more general Machiavellian social strategies. Then it mattered more who had what physical resources and strength, and what personal allies. People leaned toward projecting toughness instead of empathy. And they demanded stronger signals of loyalty, such as conformity, and were more willing to suspect people of disloyalty. Subgroups and non-conformity became more suspect, including subgroups that consistently argued together for unpopular positions.

And here is the key idea: individuals vary in the thresholds they use to switch between focusing on dealing with issues via an all-encompassing norm-enforcing talky collective, and or via general Machiavellian social skills, mediated by personal resources and allies. Everyone tends to switch together to a collective focus as the environment becomes richer and safer. (This is one of the [many](key-disputed-values) [ways](more-2d-values) that behaviors and values consistently change with wealth.) But some switch sooner: those better at working the collective, such as being better at talking and empathy, and those who gain more from collective choices, such as physically weaker folks who can’t hunt or gather as well. And also people just generally less prone to feeling afraid as a result of ambiguous cues.
People who feel less safe are more afraid of changing whatever has worked in the past, and so hold on more tightly to typical past behaviors and practices. They are more worried about the group damaging the talky collective, via tolerating free riders, allowing more distinct subgroups, and by demanding too much from members who might just up and leave. Also, those who feel less able to influence communal discussions prefer groups norms to be enforced more simply and mechanically, without as many exceptions that will be more influenced by those who are good at talking.

I argue that this key “left vs. right” inclination to focus more vs less on a talky collective is <strong>the</strong> main parameter that consistently determines who people tend to ally with in large scale political coalitions. Other parameters can matter a lot in different times and places, but this is the one that consistently matters. This parameter doesn’t matter much for how individuals relate to each other personally, and at smaller social scales like clubs or firms, coalitions form more via our general Machiavellian abilities, based on parameters than matter directly in those contexts. But everyone has an intuitive sense for how much we all expect and want big issues to be handled by a talky collective of “everyone” with any power. The first and primary political question is how much to try to resolve issues via a big talky collective, or to let smaller groups decide for themselves.

This account that I’ve just outline does reasonably well at accounting for many known left-right patterns. For example, the right is more conscientious, while the left is <a href="http://www.nature.com/neuro/journal/v10/n10/full/nn1979.html">more</a> open to experience. The left prefers more varied niche types of <a href="http://www.thepostgame.com/blog/dish/201303/how-politics-correlate-sports-interests">sports</a>, movies, and music, while the right [prefers](media-genre-more-basic-than-politics-or-personality) fewer standardized types. Artists, musicians, and comedians tend to be on the left. Right sports focus more on physical strength and combat, <a href="https://en.wikipedia.org/wiki/Biology_and_political_orientation">stronger</a> men have stronger political opinions, and when low status they favor more redistribution. People on the right are less reflective, prefer simpler arguments, are more sensitive to disgust, and startle more easily.
Education elites are <a href="http://www.business.rutgers.edu/business-insights/business-insight-red-versus-blue-understanding-socio-economic-correlates-liberalism">more</a> left than business elites. In romance and spirituality, the left tends to favor authentic feelings while the right cares more about standards of behavior. The left is more <a href="http://www.huffingtonpost.com/matthew-hutson/conservatives-more-religious-liberals-more-spiritual_b_1553460.html">spiritual</a> while right is more religious. Left [jobs](conservative-vs-liberal-jobs) focused more on talking and on a high tail of great outcomes, while right jobs focus more on avoiding a low tail of bad outcomes.
The left is more <a href="http://www.people-press.org/2015/04/07/a-deep-dive-into-party-affiliation/">okay</a> with people forming distinct subgroups, even as it thinks more in terms of treating everyone equally, even across very wide scopes, and including wide scopes in more divisive debates. The right wants to make redistribution more conditional, more <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.500.6248&amp;rep=rep1&amp;type=pdf">wants</a> to punish free riders, and wants norm violators to be more consistently punished. The left <a href="http://gizmodo.com/exclusive-heres-the-full-10-page-anti-diversity-screed-1797564320">tends</a> to presume large scale cooperation is feasible, while right tends to presume competition more. The left hopes for big gains from change while the right worries about change damaging things that now work.

Views tend to drift leftward as nations and the world get richer. Left versus right isn’t very useful for predicting individual behavior outside of politics, even as it is the main parameter that robustly determines large scale political inclinations. People tend to think differently about politics on what they see as the largest scales; for example, there are whole separate fields of political science and political philosophy, which don’t overlap much with fields dealing with smaller scale politics, such as in clubs and firms.

I shouldn’t need to say it but I will anyway: it is obvious that a safe playful talky collective is sometimes but not always the best way to deal with things. Its value varies with context. So sometimes those who are more reluctant to invoke it are right to be wary, while at other times those who are eager to apply it are right to push for it. It is <strong>not</strong> obvious, at least to me, whether on average the instincts of the left or the right are more helpful.

I’ve [noted before](specific-vs-general-foragers-farmers) that if one frames left attitudes as better when the world is safe, while right attitudes as better when world is harsh, the longer is the timescale on which you evaluate outcomes, the harsher is the world.
<strong>Added 9Sept:</strong> This post didn’t say much directly about farmers. In the much larger farmer social groups, simple one layer talky collectives were much less feasible. Farmer lives had new dangers of war and disease, and neighboring groups were more threatening. The farmer world more supported property in spouses and material goods and had more social hierarchies, farmer law less relied on a general discussion of each accused, and more reliable food meant there was less call for redistribution. Farmers worked more and had less time for play.  Together, these tended to reduce the scope of safe playful talky collectives, moving society in a rightward direction relative to foragers.

## [Rome As Semi-Foragers](#table-of-contents)
_Posted on 2010-12-28_

It seems that an “almost” industrial revolution happened around 500BC. For example, <a href="http://en.wikipedia.org/wiki/File:World_population_growth_(lin-log_scale).png">this graph</a> of estimated world population shows a population jump then similar to the start of the ~1800 jump.  Also, consider this brief history of the Roman Empire:

~5 century BC: Roman civilization is a strong patriarchy, fathers … have absolute authority over the family.<br/>
~1 century BC: … Material wealth is astounding, … Romans enjoy the arts … democracy, commerce, science, human rights, animal rights, children rights and women become emancipated. No-fault divorce is enacted, and quickly becomes popular by the end of the century.<br/>
~1-2 century AD: … Men refuse to marry and the government tries to revive marriage with a “bachelor tax”, to no avail. … Roman women show little interest in raising their own children and frequently use nannies. The wealth and power of women grows very fast, while men become increasingly demotivated and engage in prostitution and vice. Prostitution and homosexuality become widespread.<br/>
~3-4 century AD: … Roman population declines due to below-replacement birth-rate. Vice and massive corruption are rampant. (<a href="http://www.reddit.com/r/Equality/comments/cp35h/feminism_of_the_future_relies_on_men_nytimescom/c0u6hw8">more</a>; HT <a href="http://roissy.wordpress.com/2010/12/27/feminism-responsible-for-the-fall-of-rome/">Roissy</a>)

Yes this exaggerates, but the key point remains: a sudden burst in productivity and wealth lead to big cultural changes that made the Greek-Roman world and its cultural descendants more forager-like than the rest of the farmer world. These changes helped clear the way for big cultural changes of the industrial revolution.

These cultural changes included not more political egalitarianism, but also more forager like attitudes toward alchohol and mating:

Historically, we find a correlation between the shift from polygyny to monogamy and the growth of alcohol consumption. Cross-culturally we also find that monogamous societies consume more alcohol than polygynous societies in the preindustrial world. … Studies find a positive relationship between alcohol use on the one hand and a more promiscuous and high-risk sexual behavior on the other hand. … The Greek and Roman empires … were the only (and first) to introduce formal monogamy. … Hunting tribes drink more than agricultural and settled tribes. … Hunting tribes … have more monogamous marriage arrangements than agricultural tribes. …

The emergence of socially imposed formal monogamy in Greece coincides with (a) the growth of “chattel slavery” (where men can have sex with female slaves) and (b) the extension of political rights. … The industrial revolution played a key role in the shift from formal to effective monogamy and in the sharp increase of alcohol consumption (<a href="http://wine-economics.org/workingpapers/AAWE_WP75.pdf">more</a>; HT <a href="http://www.marginalrevolution.com/marginalrevolution/2010/12/women-and-alcohol.html">Tyler</a>)

This roughly fits my simple story: forager to farmer and back to forager with industry. The key is to see monogomous marraige as an intermediate form between low-commitment feeling-based forager mating, and wives-as-property-for-live farmer polygamy. Let me explain.

Forager work and mating is more intuitive, less institutional. Mates stay together mainly because they feel like it; there is more an open compeition to seduce mates, and there’s a lot of sneaking around.  Foragers drink alchohol when they can, and spontaneous feelings count for more relative to formal commitments. The attitude is more that if you can’t hold her interest, you don’t deserve to keep her. Men show off abilities to obtain resources mainly to signal attractive qualities; most resources acquired must be shared with the rest of the band.

Farmers, in contrast, don’t share much, and are far more unequal in the resources they control, by which they can more directly “buy” wives. Farmer wives so bought are supposed to be committed to their husbands even when they don’t feel like it. Marriage was less about mutal attraction and more about building households and clans. Husbands worry about cheating wives, and so try to limit access and temptations, which includes alchohol. Musicians and artists are also suspect if they excite wives’ passions, which might lead to cheating.

When empires like Greece and Rome achieved sustained periods of prosperity, their elites reverted to more forager-like ways. They had more drinking and art, more egalitarian politics, fertility fell, and [non-slave] mating became more egalitarian and about feelings. If a bit of alchohol was enough to get your wife cheat to on you, well maybe you didn’t deserve her. The Greek-Roman move from polygamy to monogamy was a move in the direction of more forager-like feeling-based mating, though it retained farmer-like lifelong commitment.

The Greeks and Romans became models for Europe when industry made it rich again. In our era, fertility has fallen far, divorce and out-of-wedlock births are common, and alchohol, drugs, and sneaking about are more tolerated. Women need men less for their resources, and choose them more on other grounds. Dropping the lifelong commitment element of marriage, and often the expectation of any sort of marriage commitment, we have moved even further away from farmer wives-as-lifelong-property and toward forager “promiscuity.”

<strong>Added</strong>: <a href="http://blogs.discovermagazine.com/gnxp/2010/12/the-axial-age-world-population/">Razib Khan</a> and <a href="http://www.jasoncollins.org/2011/01/world-population-500bc/">Jason</a> elaborate.

<strong>Added 1Feb:</strong> A <a href="http://dmmsclick.wiley.com/view.asp?m=48f40ro8u75v2ntxm0dk&amp;u=9131279">new study</a> says that in places where marriages are more arranged by parents, there is more mate-guarding. Discouraging alcohol seems a reasonable mate-guarding strategy.

## [Self-Control Is Slavery](#table-of-contents)
_Posted on 2010-06-05_

I’ve been pondering 3 related points.  1) [Self-Control Is Culture-Control](self-control-is-culture-control):
It seems to me that … <em>the</em> key change after farming [was] an increased sensitivity to culture, so that social sanctions became better able to push behavior contrary to other inclinations. … This increased sensitivity to the carrots and sticks of culture generally appears to us as greater “self-control”, i.e., as our better resisting immediate inclinations for other purposes. And since we have more self-control in far mode, I suspect an important component of change since farming has been greater inclinations toward and abilities in far mode.

2) Fogel & Engerman’s economic classic <a href="http://www.sjsu.edu/faculty/watkins/fogel.htm">analysis of US slavery</a>:

Plantation agriculture based upon slave labor … may have been significantly more efficient than family farming. … The typical slave field-hand may have been more productive than a free, white field-hand. … Slavery was not incompatible with industrial production. … Slave-labor farms were 28 percent more productive than southern free-labor farms and 40 percent more productive than northern free-labor farms. …

Plantation operators strove for a disciplined, specialized and coordinated labor force. Labor was organized into something like the assembly line operations in industry. This involved “driving” the slaves’ efforts to maintain a pace of production. The “drivers” or foremen were slaves themselves. …

Plantations had a much higher rate of labor force participation, two thirds, as compared with a free population, one third. This was achieved by finding productive pursuits for the young and the elderly and maintaining nurseries so that slave women could work.

3) The latest <em>AER</em> on designing work to aid <a href="http://www.aeaweb.org/articles.php?doi=10.1257/aer.100.2.624">self-control</a>:

The Industrial Revolution involved workers moving from agriculture to manufacturing; from working on their own to working with others in factories; and from flexible work-hours to rigid work-days. … Some work-place arrangements may make self-control problems more severe, while others may ameliorate them. … The firm … can use regular compensation to … make the returns to effort more immediate.  Firms can also create disproportionate penalties for certain types of low efforts … so as to create sharp self-control incentives. … Conforming to an externally set pace, however, can decrease these self-control costs. … Workers planting rice-fields often find it helpful to synchronize movements to music or to beats. In industrial production, the assembly line may serve a similar purpose. … An intrinsic competitive drive may make the momentary self exert more effort when surrounded by hard-working coworkers. Young boys run races faster when running alongside another boy than when running alone. …

[Farming] creates difficult self-control problems. First, it involves long time horizons — farmers must tend their land constantly for months before reaping benefits at harvest. These lags can generate suboptimal effort in early stages of production. Financially, farmers may also fail to save enough money out of lumpy harvest payments to make efficient investments during the production cycle, further affecting labor supply returns and output. Second, agriculture often involves self-employment or very small firms. As a result, there are rarely firms or large employers to mitigate the self-control problem. Tasks cannot be structured, compensation altered, or work  intensity regulated. Finally, agrarian production by nature is also geographically dispersed, which makes colocation of workers difficult. … This can help explain the observation that work hours appear to be low in modern-day subsistence agriculture. …

In the workshop system, workers rented floor space or machinery in factories, received pure piece rates for output … Clark presents evidence that workers under the workshop system had very unsteady attendance and hours, spent a lot of time socializing at work, and concentrated effort in the latter half of the week leading up to paydays. Clark argues that this led firms to transition to the factory discipline system to solve self-control problems.

OK, now let’s put it all together.  Apparently, factory-like methods that greatly increase farming productivity have long been feasible.  (First known factory: <a href="http://en.wikipedia.org/wiki/Factory">Venice Arsenal</a>, 1104.)  Yet it took slaves to actually implemented such methods in farming. Even after ten thousand years of Malthusian competition, a farming method that could support a much larger population per land area did <em>not</em> displace other methods.  (And if factory-fortified foraging was possible, the timescale problem gets much worse.)

The introduction of farming was associated with important new elements, like religion, that encouraged more “self-control,” i.e. sensitivity to social norms.  However, those additions were <em>not</em> sufficient to achieve factory-like farming — most humans had too little self-control to make themselves behave that way, and too strong an anti-dominance norm to let rulers enforce such behavior.

This dramatically illustrates the huge self-control innovations that came with industry. [School](school-is-far), propaganda, mass media, and who knows what else have greatly changed human nature, enabling a system of industrial submission and control that proud farmers and foragers simply would not tolerate – they would (and did) starve first.  In contrast, industry workers had enough self/culture-control to act as only slaves would before – working long hours in harsh alien environments, and showing up on time and doing what they were told.
So what made industry workers so much more willing to increase their self-control, relative to farmers?  One guess: the productivity gains from worker self-control were far larger in industry than in farming. Instead of a 50% gain, it might have been a factor of two or more. Self-controlled workers and societies gained a big enough productivity advantage to compensate for lost pride.

Humans are an increasingly self-domesticated species. Foragers could cooperate in non-kin groups of unprecedented size, farmers could enforce norms to induce many behaviors unnatural for foragers, and the schooled humans of industry would willingly obey like enslaved farmers. Our descendants may evolve even stronger self/culture-control of behavior.

## [School Is To Submit](#table-of-contents)
_Posted on 2016-04-06_

Most animals in the world can’t be usefully domesticated. This isn’t because we can’t eat their meat, or feed them the food they need. It is because all animals naturally resist being dominated. Only rare social species can let a human sit in the role of dominant pack animal whom they will obey, and only if humans do it just right.

Most nations today would be richer if they had long ago just submitted wholesale to a rich nation, allowing that rich nation to change their laws, customs, etc., and just do everything their way. But this idea greatly offends national and cultural pride. So nations stay poor.

When firms and managers from rich places try to transplant rich practices to poor places, giving poor place workers exactly the same equipment, materials, procedures, etc., one of the main things that goes wrong is that poor place workers just refuse to do what they are told. They won’t show up for work reliably on time, have many problematic superstitions, hate direct orders, won’t accept tasks and roles that that deviate from their non-work relative status with co-workers, and <a href="http://faculty.georgetown.edu/mh5/class/econ489/Clark-Why-Isn't-the%20-Whole-World-Developed.pdf">won’t accept</a> being told to do tasks differently than they had done them before, especially when new ways seem harder. Related complaints are often made about the poorest workers in rich societies; they just won’t consistently do what they are told. It seems pride is a big barrier to material wealth.

The farming mode required humans to swallow many changes that didn’t feel nice or natural to foragers. While foragers are fiercely egalitarian, farmers are dominated by kings and generals, and have unequal property and classes. Farmers work more hours at less mentally challenging tasks, and get less variety via travel. Huge new cultural pressures, such as religions with moralizing gods, were needed to turn foragers into farmers.

But at work farmers are mostly autonomous and treated as the equal of workers around them. They may resent having to work, but adults are mostly trusted to do their job as they choose, since job practices are standardized and don’t change much over time. In contrast, productive industrial era workers must accept more local domination and inequality than would most farmers. Industry workers have bosses more in their face giving them specific instructions, telling them what they did wrong, and ranking them explicitly relative to their previous performance and to other nearby workers. They face more ambiguity and uncertainty about what they are supposed to do and how.

How did the industrial era get at least some workers to accept more domination, inequality, and ambiguity, and why hasn’t that worked equally well everywhere? A simple answer I want to explore in this post is: prestigious schools.

While human foragers are especially averse to even a hint of domination, they are also especially eager to take “orders” via copying the practices of [prestigious](two-kinds-of-status) folks. Humans have a uniquely powerful capacity for [cultural evolution](how-plastic-are-values) exactly because we are especially eager and able to copy what prestigious people do. So if humans hate industrial workplace practices when they see them as bosses dominating, but love to copy the practices of prestigious folks, an obvious solution is to habituate kids into modern workplace practices in contexts that look more like the latter than the former.
In his upcoming book, <em><a href="http://econlog.econlib.org/archives/2016/03/the_case_agains_11.html">The Case Against Education</a></em>, my colleague Bryan Caplan argues that school today, especially at the upper levels, functions mostly to help students signal intelligence, conscientiousness, and conformity to modern workplace practices. He says we’d be better off if kids did this via early jobs, but sees us as having fallen into an unfortunate equilibrium wherein individuals who try that seem non-conformist. I agree with Bryan that, compared with the theory that older students mostly go to school to learn useful skills, signaling better explains the low usefulness of school subjects, low transfer to other tasks, low retention of what is taught, low interest in learning relative to credentials, big last-year-of-school gains, and student preferences for cancelled classes.

My main problem with Caplan’s story so far (he still has time to change his book) is the fact that centuries ago most young people did signal their abilities via jobs, and the school signaling system has slowly displaced that job signaling system. Pressures to conform to existing practices can’t explain this displacement of a previous practice by a new practice. So why did signaling via school did win out over signaling via early jobs?

Like early jobs, school can have people practice habits that will be useful in jobs, such as showing up on time, doing what you are told even when that is different from what you did before, figuring out ambiguous instructions, and accepting being frequently and publicly ranked relative to similar people. But while early jobs threaten to trip the triggers than make most animals run from domination, schools try to frame a similar habit practice in more acceptable terms, as more like copying prestigious people.

Forager children aren’t told what to do; they just wander around and do what they like. But they get bored and want to be respected like adults, so eventually they follow some adults around and ask to be shown how to do things. In this process they sometimes have to take orders, but only until they are no longer novices. They don’t have a single random boss they don’t respect, but can instead be trained by many adults, can select them to be the most prestigious adults around, and can stop training with each when they like.

Schools work best when they set up an apparently similar process wherein students practice modern workplaces habits. Start with prestigious teachers, like the researchers who also teach at leading universities. Have students take several classes at at a time, so they have no single “boss” who personally benefits from their following his or her orders. Make class attendance optional, and let students pick their classes. Have teachers continually give students complex assignments with new ambiguous instructions, using the excuse of helping students to learn new things. Have lots of students per teacher, to lower costs, to create excuses for having students arrive and turn in assignments on time, and to create social proof that other students accept all of this. Frequently and publicly rank student performance, using the excuse of helping students to learn and decide which classes and jobs to take later. And continue the whole process well into adulthood, so that these habits become deeply ingrained.

When students finally switch from school to work, most will find work to be similar enough to transition smoothly. This is especially true for desk professional jobs, and when bosses avoid giving direct explicit orders. Yes, workers now have one main boss, and can’t as often pick new classes/jobs. But they won’t be publicly ranked and corrected nearly as often as in school, even though such things will happen far more often than their ancestors would have tolerated. And if their job ends up giving them prestige, their prior “submission” to prestigious teachers will seem more appropriate.

This point of view can help explain how schools could help workers to accept habits of modern workplaces, and thus how there could have been selection for societies that substituted schools for early jobs or other child activities. It can also help explain unequal gains from school; some kinds of schools should be less effective than others. For example, teachers might not be prestigious, teachers may fail to show up on time to teach, teacher evaluations might correlate poorly with student performance, students might not have much choice of classes, school tasks might diverge too far from work tasks, students may not get prestigious jobs, or the whole process might continue too long into adulthood, long after the key habituation has been achieved.

In sum, while students today may mostly use schools to signal smarts, drive, and conformity, we need something else to explain how school displaced early work in this signaling role. One plausible story is that schools habituate students in modern workplace habits while on the surface looking more like prestigious forager teachers than like the dominating bosses that all animals are primed to resist. But this hardly implies that everything today that calls itself a school is equally effective at producing this benefit.

## [Why Grievances Grow](#table-of-contents)
_Posted on 2019-03-09_

> We have come to call these fields “grievance studies” in shorthand because of their common goal of problematizing aspects of culture in minute detail in order to attempt diagnoses of power imbalances and oppression rooted in identity. (<a href="https://areomagazine.com/2018/10/02/academic-grievance-studies-and-the-corruption-of-scholarship/">more</a>)
> A full 80% [of US] believe that “political correctness is a problem in our country.” … The woke are in a clear minority across all ages. … Progressive activists are the only group that strongly backs political correctness: Only 30% see it as a problem. … Compared with the rest of the [nation], progressive activists are much more likely to be rich, highly educated—and white. … What people mean by “political correctness.” … [is] their day-to-day ability to express themselves: They worry that a lack of familiarity with a topic, or an unthinking word choice, could lead to serious social sanctions for them. (<a href="https://www.theatlantic.com/ideas/archive/2018/10/large-majorities-dislike-political-correctness/572581/">more</a>)
> While the American legal system favors the state over the individual in property takings, for example in contrast with the Japanese system, the political system favors NIMBYs and really anyone who complains. Infrastructure construction takes a long time and the politician who gets credit for it is rarely the one who started it, whereas complaints happen early. This can lead to many of the above-named problems [with transit construction], especially overbuilding, such as tunneling where elevated segments would be fine or letting agency turf battles and irrelevant demands dictate project scope. (<a href="https://pedestrianobservations.com/2019/03/03/why-american-costs-are-so-high-work-in-progress/">more</a>)
> Chronic Complainers: These folks live in a constant state of complaint. If they’re not voicing about their “woe is me” attitude, they’re probably thinking about it. Psychologists term this compulsory behavior rumination, defined as “repetitively going over a thought or a problem without completion.” Rumination is, unfortunately, directly relayed to the depressed and anxious brain. (<a href="https://www.powerofpositivity.com/complaining-changes-brain-anxious-depressed-research/">more</a>)
> Customers with high status tended to register more service failures and to complain more frequently than customers of lower social status. All three social status distinctions explored in this study (gender, education, and age) correlated negatively with formal complaint, but only age correlated negatively with informal complaint. … Two cultural dimensions [power distance and uncertainty avoidance] had the expected negative effect on intention to complain, and moderated the relationship between social status and intention to complain. (<a href="https://link.springer.com/article/10.1007/s11205-015-0884-y">more</a>)
<blockquote class="twitter-tweet" data-lang="en">
Learning someone is prone to complain more often that others can change your opinion of them. And this effect may be different for low vs. high status (S) people. Do you think more or less of complainers who are high vs. low status?

> — Robin Hanson (@robinhanson) <a href="https://twitter.com/robinhanson/status/1104470464137711617?ref_src=twsrc%5Etfw">March 9, 2019</a></blockquote>
> <script async="" charset="utf-8" src="https://platform.twitter.com/widgets.js"></script>
> <script async="" charset="utf-8" src="https://platform.twitter.com/widgets.js"></script>

My [favorite](two-types-of-people) one-factor [theory](forager-v-farmer-elaborated) of social attitude (and value) change over the last few centuries is that increasing wealth has induced a drift from farmer back to forager attitude (and values). (A theory I also outline in <a href="http://ageofem.com"><em>Age of Em</em></a>.) Which plausibly helps explains changing attitudes toward fertility, gender, slavery, crime, democracy, war, leisure, art, and travel. In this post I want to suggest a (to me) new hypothesis about forager attitudes, which could help explain some recent attitude trends.
Foragers are fiercely egalitarian. They share many kinds of food and other resources, and enforce a norm of quickly and aggressively squashing any signs of attempts to use or threaten to use force, or any inclinations to do so. In fact, this is [probably](hail-christopher-boehm) the uber-norm that drove the evolution of norms in the first place. Bragging about your physical strength is a no-no, as that can be interpreted as an implicit threat to use that strength. Even bragging about your intelligence or other resources is discouraged, as those might also be seen as threats, or as attempts to form coalitions that might threaten. Forager group decisions are to be made by consensus, after everyone has had a chance to weigh in.
Now consider foragers attitudes about complaining. When someone more dominant makes a complaint to someone less dominant, that can often be interpreted as a threat to use power if the complaint isn’t fixed. Which is a big forager no-no. But when a less dominant person complains to a more dominant person, it is harder to see that as a threat to use power. So complaints down are discouraged more than complaints up, just as punching down is more of a no-no than punching up. And we’ll tend to interpret complaints as a pro-down positions.

A complaint that is made to third parties fits the standard norm-enforcement pattern, a pattern of which foragers greatly approve. Thus having A complain to B about how a more dominant person C is treating a less dominant person D badly should generally meet with approval. This is A helping out with norm enforcement, and can be seen as “speaking truth to power.” If A is a high [prestige](dominance-hides-in-prestige-clothing) person, and B is a wise and moral audience, this pattern should be especially approved. After all, we naturally believe prestigious people more than others. And if a complaint leads to action of which we later approve, that can increase the prestige of the complainers.
Yes, people who complain a lot tend to seem unhealthy, and we tend to think less of frequent complainers. Even so, foragers likely a big soft spot in their hearts for prestigious people who complain to the whole group that some low dominance people are being treated badly by high dominance people. Those complaints, foragers respected.

In our society today, we tend to frame big firms, governments, rich folks, and larger demographic groups as more dominant actors. So when a local neighborhood group complains about a government plan for a transit construction project, we tend to see that as a low dominance actor complaining about a high dominance actor, and habitually sympathize. And to the extent that we have forager-like attitudes about such situations, this increases the political negotiating power of such complainers, inducing governments to give in to them, and raising the costs of transit construction projects. Similar processes likely increase the power of neighborhood groups who demand rent, zoning, and private construction restrictions, resulting in less new buildings and housing.

Forager-like attitudes similarly prime us to favor ordinary consumers or employees who complain about big firms, and this encourages regulations focused mostly on consumer and employee welfare, relative to the welfare of investors, who are framed as rich and thus dominators. Even rich high status people feel comfortable complaining about how big firms treat them, and in fact they feel more comfortable than low status folks. Their higher prestige can make them feel like respected moral crusaders for all.

As larger race/ethnicities are framed as dominators relative to smaller ones, forager-like attitudes prime us to sympathize with complaints that the former mistreat the latter. Similarly for complaints on how the larger groups who have more standard gender and sexual preferences treat the smaller groups who have more deviant genders and sexual preferences. Men’s higher physical strength and participation in war, and higher percentage among top positions at most organizations, has long induced us to frame men as more dominant relative to women.

Thus when we have more forager-like attitudes, we naturally sympathize when high prestige people complain that these more dominant groups are mistreating the less dominant groups. And in fact people with the potential for high prestige can seek to cement and increase their prestige via such complaints. Which is plausibly why it is high prestige folks who participate most in “grievance studies” type complaining.

Forager-like attitudes should make us sympathize with most any complaint about how rich people treat less rich people. Including how they conspire to mess up markets, political systems, or legal systems. Also, when criminals are committing crimes, they can seem like illicit dominators relative to ordinary citizens. But police, courts, and prisons can seem like dominators relative to criminals, thus inducing us to sympathize with complaints that criminals are being treated too harshly by the legal system. Perhaps explaining why prestigious folks [seem to](why-weakly-enforced-rules) consistently push for weaker criminal punishments.
My wealth-induces-farmer-to-forager-attitudes story says that this complaint-sympathizing effect has been slowly getting stronger as we’ve been getting richer and more forager-like. It is strongest in the richest nation, which is currently the US, and it will continue to get stronger world-wide as the world gets richer. And these grievances accumulate when we do not [use law](consider-reparations) to try and settle them.
And that’s my story. Hyper-egalitarian foragers were especially sympathetic to complaints by prestigious folks that high-dominance folks were mistreating less-dominant others, and with increasing wealth we’ve been slowly increasing our embrace of this forager attitude. And so we’ve been listening more to such complainers, and giving them more political and social power, which has encouraged more high prestige folks to present themselves as such crusading complainers. Which results in a growing accumulation of such grievances.

What to do about this will have to wait for another post.

<strong>Added 10Mar:</strong> The conceptual power here is that this theory is more specific than the general idea that we dislike inequality and dominance, and so work consistently to reduce them. A habit of favoring specific complaints against more dominant parties can actually increase inequality and dominance in many cases.

<strong>Added 11Mar:</strong> Martin Gurri’s book <a href="https://www.amazon.com/dp/B07J2V3PG4/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1"><em>Revolt of the Public</em></a> can be seen as describing a switch to a focus on popular complaints. He describes many new social movements around 2011 that focused on complaining loudly to an enthusiastic public, but which due to egalitarian ideals weren’t interested in or capable of negotiating concrete demands or working within the usual political systems.

## [The World Forager Elite](#table-of-contents)
_Posted on 2020-09-22_

My [last post](elois-ate-your-flying-car) was on <em>Where’s My Flying Car?</em>, which argues that changing US attitudes created a tsunami of reluctance and regulation that killed nuclear power, planes, and ate the future that could have been. This explanation, however, has a problem: if there are many dozens of nations, how can regulation in one nation kill a tech? Why would regulatory choices be so strongly correlated across nations? If nations compete, won’t one nation forgoing a tech advantage make others all the more eager to try it?
Now as nuclear power tech is close to nuclear weapon tech, maybe major powers exerted strong pressures re how others pursued nuclear power. Also, those techs are high and require large scales, limiting how many nations could feasibly do them differently.

But we also see high global correlation for many other kinds of regulation. For example, as Hazlett [explains](hazletts-political-spectrum), the US started out with a reasonable property approach to spectrum, but then Hoover broke that on purpose, to create a problem he could solve via nationalization, thereby gaining political power that helped him become U.S. president. Pretty much all other nations then copied this bad US approach, instead of the better prior property approach, and kept doing so for many decades.
The world has mostly copied bad US approaches to over-regulating planes as well. We also see regulatory convergence in topics like human cloning; many had speculated that China would be defy the consensus elsewhere against it, but that turned out not to be true. Public prediction markets on interesting topics seems to be blocked by regulations almost everywhere, and insider trading laws are most everywhere an obstacle to internal corporate markets.

Back in February we saw a dramatic example of world regulatory coordination. Around the world public health authorities were talking about treating this virus like they had treated all the others in the last few decades. But then world elites talked a lot, and suddenly they all agreed that this virus must be treated differently, such as with lockdowns and masks. Most public health authorities quickly caved, and then most of the world adopted the same policies. Contrarian alternatives like [variolation](variolation-may-cut-covid19-deaths-3-30x), challenge trials, and cheap fast lower-reliability tests have also been rejected everywhere; small experiments have not even been allowed.
One possible explanation for all this convergence is that regulators are just following what is obviously the best policy. But if you dig into the details you will quickly see that the usual policies are not at all obviously right. Often, they seem obviously wrong. And having all the regulatory bodies suddenly change at once, even when no new strong evidence appeared, seems especially telling.

It seems to me that we instead have a strong world culture of regulators, driven by a stronger world culture of elites. Elites all over the world talk, and then form a consensus, and then authorities everywhere are pressured into following that consensus. Regulators most everywhere are quite reluctant to deviate from what most other regulators are doing; they’ll be blamed far more for failures if they deviate. If elites talk some more, and change their consensus, then authorities must then change their polices. On topic X, the usual experts on X are part of that conversation, but often elites overrule them, or choose contrarians from among them, and insist on something other than what most X experts recommend.

This looks a lot like the ancient forager system of conflict resolution within bands. Forager bands would gossip about a problem, come to a consensus about what to do, and then everyone would just do that. Because each one would lose status if they didn’t. In this system, there were no formal rules, and on the surface everyone had an equal say, though in fact some people had a lot more prestige and thus a lot more influence.

This world system also looks new – I doubt this description applied as well to the world centuries or millennia ago, even within smaller regions. So this looks like [another](forager-v-farmer-elaborated) way in which our world has become more forager-like over the last few centuries, as we’ve felt more rich and safe. Big world wars probably cut into this feeling, so there was probably a big jump in the few decades after WWII, helping to explain the big change in attitudes ~1970.
Elites like to talk about this system as if it were “democratic”, so that any faction that opposes it “undermines democracy”. And it is true that this system isn’t run by a central command structure. But it is also far from egalitarian. It embodies a huge inequality of influence, even if individuals within it claim that they are mainly driven by trying to help the world, or “the little guy”.

This system seems a big obstacle for my hopes to create better policy institutions driven by expert understanding of institutions, and to get trials to test and develop such things. Because as soon as any policy choice seems important, such by triggering moral feelings, world elite culture feels free to gossip and then pressure authorities to adopt whatever solution their gossip prefers. Experts can only influence policy via their prestige. Very prestigious types of experts, such as in physics, can win, especially on topics about which world elites care little. But otherwise, elite gossip wins, whenever it bothers to generate an opinion.

That is, the global Overton window isn’t much wider than are local Overton windows, and often excludes a lot of valuable options.

Notice that in this kind of world, policy has varied far more across time than across space. Context and fashion change with time, and then elites sometimes change their minds. So perhaps my hopes for policy experiments must wait for the long run. Or for a fall of forager values, such as seems likely in an <a href="http://ageofem.com"><em>Age of Em</em></a>. Alas neither I nor my allies have sufficient prestige to push elites to favor our proposals.

<strong>Added 11p</strong>: It seems to me that the actual degree of experimentation and variance in policy is far below optimum in this conformist sort of policy world. We are greatly failing to try out as many alternatives as fast as we should to find out what works best. And we are failing to listen enough to our best experts, and instead too often going with the opinions of well-educated but amateur world elites.

<strong>Added4p:</strong> As John Nye reminds me, in the early years of a new tech, only a few nations in the world may be able to pursue it. They then set the initial standards of regulation. Later, more nations may be able to participate, but risk-averse regulators may feel shy about defying widely adopted initial standards.

## [The Great Cycle Rule](#table-of-contents)
_Posted on 2017-03-08_

History contains a <em>lot</em> of data, but when it comes to the largest scale patterns, our data is very limited. Even so, I think we’d be crazy not to notice whatever patterns we can find at those largest scales, and ponder them. Yes we can’t be very sure of them, but we surely should not ignore them.

I’ve <a href="http://mason.gmu.edu/~rhanson/longgrow.pdf">said</a> that history can be summarized as a sequence of roughly exponential growth modes. The three most recent modes were the growth of human foragers, then of farmers, then of industry. Roughly, foragers doubled every quarter million years, farmers every thousand years, and industry every fifteen years. (Before humans, animal brains doubled roughly every 35 million years.)

I’ve previously noted that this sequence shows some striking patterns. Each transition between modes took much less than a previous doubling time. Modes have gone through a similar number of doublings before the next mode appeared, and the factors by which growth rates increased have also been similar.  In addition, the group size that typified each mode was roughly the square of that of the previous mode, from thirty for foragers to a thousand for farmers to a million for industry.

In this post I report a new pattern, about cycles. Some cycles, such as days, months, and years, are common to most animals days, months, years. Other cycles, such as heartbeats lasting about a second and lifetimes taking threescore and ten, are common to humans. But there are other cycles that are distinctive of each growth mode, and are most often mentioned when discussing the history of that mode.

For example, the 100K year cycle of ice ages seems the most discussed cycle regarding forager history. And the two to three century cycle of empires, such as [documented](cycles-of-war-empire) by Turchin, seems most discussed regarding the history of farmers. And during our industry era, it seems we most discuss the roughly five year business cycle.
The new pattern I recently noticed is that each of these cycles lasts roughly a quarter to a third of its mode’s doubling time. So a mode typically grows 20-30% during one period of its main cycle. I have no idea why, but it still seems a pattern worth noting, and pondering.

If a new mode were to follow these patterns, it would appear in the next century, after a transition of ten years or less, and have a doubling time of about a month, a main cycle of about a week, and a typical group size of a trillion. Yes, these are only very rough guesses. But they still seem worth pondering.

## [The Labor-From-Factories Explosion](#table-of-contents)
_Posted on 2016-05-04_

As I’ve discussed before, including in <a href="http://ageofem.com">my book</a>, the history of humanity so far can be roughly <a href="http://hanson.gmu.edu/longgrow.pdf">summarized</a> as a sequence of three exponential growth modes: foragers with culture started a few million years ago, farming started about ten thousand years ago, and industry starting a few hundred years ago. Doubling times got progressively shorter: a quarter million years, then a millennia, and now fifteen years. Each time the transition lasted less than a previously doubling time, and roughly similar numbers of humans have lived during each era.

Before humans, animal brains brains grew exponentially, but even more slowly, doubling about every thirty million years, starting about a half billion years ago. And before that, genomes [seem to](life-before-earth) have doubled exponentially about every half billion years, starting about ten billion years ago.
What if the number of doublings in the current mode, and in the mode that follows it, are comparable to the number of doublings in the last few modes? What if the sharpness of the next transition is comparable to the sharpness if the last few transitions, and what if the factor by which the doubling time changes next time is comparable to the last few factors. Given these assumptions, the next transition will happen sometime in roughly the next century. Within a period of five years, the economy will be doubling every month or faster. And that new mode will only last a year or so before something else changes.

To summarize, usually in history we see relatively steady exponential growth. But five times so far, steady growth has been disturbed by a rapid transition to a much faster rate of growth. It isn’t crazy to think that this might happen again.

Plausibly, new faster exponential modes appear when a feedback loop that was previously limited and blocked becomes is unlocked and strong. And so one way to think about what might cause the next faster mode after ours is to look for plausible feedback loops. However, if there thousands of possible factors that matter for growth and progress, then there are literally millions of possible feedback loops.

For example, denser cities should innovate more, and more innovation can find better ways to make buildings taller, and thus increase city density. More better tutorial videos make it easier to learn varied skills, and some of those skills help to make more better tutorial videos. We can go all day making up stories like these.

But as we have only ever seen maybe five of these transitions in all of history, powerful feedback loops whose unlocking causes a huge growth rate jump must be extremely rare. The vast majority of feedback loops do not create such a huge jump when unlocked. So just because you can imagine a currently locked feedback loop does <em>not</em> make unlocking it likely to cause the next great change.

Many people lately have fixated on one particular possible feedback loop: an “intelligence explosion.”  The more intelligence a creature is, the more it is able to change creatures like itself to become more intelligent. But if you mean something more specific than “[mental goodness](the-betterness-explosion)” by “intelligence”, then this remains only one of thousands of possibilities. So you need strong additional arguments to see this feedback loop as more likely than all the others. And the mere fact that you can imagine this feedback being positive is not remotely enough.
It turns out that we already know of an upcoming transition of a magnitude similar to the previous transitions, scheduled to arrive roughly when prior trends led us to expect a new transition. This explosion is due to labor-from-factories.

Today we can grow physical capital very fast in factories, usually doubling capital on a scale ranging from a few weeks to a few months, but we grow human workers much more slowly. Since capital isn’t useful without more workers, we are forced to grow today mainly via innovation. But if in the future we find a way to make substitutes for almost all human workers in factories, the economy can grow <em>much</em> faster. This is called an <a href="http://www.econ.umn.edu/~guvenen/Lecture8.pdf">AK model</a>, and standard growth theory says it is plausible that this could let the economy double every month or so.

So if it is plausible that artificial intelligence as capable as humans will appear in the next century or so, then we already know what will cause the next great jump to a faster growth mode. <em>Unless</em> of course some other rare powerful feedback loop is unlocked before then. But if an intelligence explosion isn’t  possible until you have machines at least as smart as humans, then that scenario won’t happen until <em>after</em> labor-from-factories. And even then it is far from obvious that feedback can cause one of the few rare big growth rate jumps.

## [Lost Advanced Civilizations](#table-of-contents)
_Posted on 2020-08-18_

Did life on Earth start on Earth, or did it start on Mars and move to Earth? If you frame such panspermia as an “extraordinary claim” for which you demand “extraordinary evidence”, you will of course conclude that this should be treated “skeptically” as unlikely and sloppy unscientific “speculation”. To be disdained and not treated as serious by respectable academics and science journalists. But that’s not really fair.

You see the early Mars environment is, a priori, about as likely a place for life to start as the Earth environment. So if the rate at which life is transferred between the planets were high enough, then equal chances of life starting first in both places would result in equal chances for Earth life to have started in either place. We should take the expected time difference between life starting in the two places, and ask how high is the chance that life would move from one planet to the next during that period. The more often rocks are thrown from one place to the other, and the more easily life could survive for the travel period within those rocks, then the more likely it is that Earth life started on Mars.

In addition, Mars, being further from the Sun, would have cooled first, and had a head start in its window for life. Making it <em>more</em> likely that life would start there and spread to Earth than vice versa. Of course life starting first on Mars would have implications for what we might see when we look at Mars. If we had expected Mars life to continue strong until today, then the fact that we see no life on Mars now would be a big strike against this hypothesis. But if we expected Mars life to have died out or at least gone dormant by now, then the issue is what we will see when we dig on Mars. With enough data on such digs, we may come to reject to Mars first hypothesis even given its initial plausibility.

A similar analysis applies to panspermia from other stars. You might think it obvious that the rate at which life-filled rocks from a star make it to seed other stars is very low, but most stars are born in large groups close together in stellar nurseries. So if life arose early enough within our star’s nursery, there might have been high rates of moving that life between stars in that nursery. In which [case](pondering-panspermia) the chance that Earth life came from another star could also be high, and the best place to look for life outside our star would be the other stars from our stellar nursery.
Now consider the possibility of lost advanced civilizations. Not just civilizations at a similar level of development to those around them in space and time; that’s quite likely given that we keep finding new previously-unknown settlements and developed places. No, the more interesting claims are about substantial (but not crazy extreme) decreases in the peak or median level of civilizations across wide areas. Such as what happened late in the late Mediterranean <a href="https://en.wikipedia.org/wiki/Late_Bronze_Age_collapse">Bronze</a> Age, or at the fall of the Roman Empire. Could there have been “higher” civilizations before the “first” ones that we now know about in each region, such as the Sumerians, Egyptians, and Chinese Shang dynasty? (I’m talking human civs, [not](dinos-on-the-mo) <a href="https://www.scientificamerican.com/article/could-an-industrial-prehuman-civilization-have-existed-on-earth-before-ours/">others</a>.)<span id="more-32503"></span>
Yes, you might think of these as “extraordinary” claims for which we lack extraordinary evidence, and declare them unlikely and sloppy unscientific speculation, to be disdained by the respectable. But again, that’s not fair. A priori it is nearly as likely that overall advancement in a region would have taken a big (but not crazy huge) temporary dip, as that it would have had a recently-typical rise. No, that isn’t much of a reason for skepticism.

Substantial, if hardly overwhelming, supporting evidence comes in the form of writings from the earliest authors we can find, who explicitly claim that they descended from more advanced prior civilizations, who fell due to big cataclysms. This story is actually quite common. Further supporting evidence exists when the earliest versions of the first civilizations we see had surprisingly advanced abilities for their time in key areas, abilities which then declined over time. That is what you’d expect to see after a prior peak. And that does seem to be what we <a href="http://www.unchartedx.com/2020/01/16/evidence-for-ancient-high-technology-part-1-machining/">see</a> in Egypt and Peru, as far as I can tell, regarding stone masonry abilities. Of course that might also just reflect local fluctuations in particular abilities; the big question is how much correlation to expect to see across different kinds of civilization abilities.

The most common contrary evidence offered is the absence of expected supporting evidence. For example:

> No matter how devastating an extraterrestrial impact might be, are we to believe that after centuries of flourishing every last tool, potsherd, article of clothing, and, presumably from an advanced civilization, writing, metallurgy and other technologies—not to mention trash—was erased? Inconceivable. (<a href="https://www.scientificamerican.com/article/no-there-wasnt-an-advanced-civilization-12-000-years-ago/">More</a>)
> He claims that glacial runoff from the comet’s incineration of the ice sheets covering North America could have destroyed every trace of civilization, though how animal bones survived but not a single stone or metal tool, or a single indisputably human-carved block of stone is beyond me … Clovis people left behind tens of thousands of stone tools and fluted points, while Atlantis is represented by exactly nothing. Even if their bones turned to dust, where are their stones and their metals? Where is the pollen from their crops…? (<a href="https://www.skeptic.com/reading_room/american-atlantis-colavito-review-america-before-key-to-earths-lost-civilization/">More</a>)

Here the key question here is: what sort of historical evidence should you expect to have already seen, if it were really there? On the one hand, we clearly have seen enough to safely conclude that there aren’t large dinosaurs roaming the streets in our major city centers. On the other hand, we <a href="https://www.nationalgeographic.com/history/2020/08/mysterious-carvings-evidence-human-sacrifice-uncovered-ancient-city-china/">often</a> <a href="https://en.wikipedia.org/wiki/Göbekli_Tepe">hear</a> <a href="https://www.nature.com/articles/d41586-018-01713-y">reports</a> of people uncovering old things that others had pretty confidently predicted would never be found. Which makes many suspect widespread overconfidence in claims about what we know can’t be there, because if so we would have seen them already.

Yes, the bigger and more techy a lost civilization one postulates, the more likely it is that we’d have seen evidence of it. For example, the bigger a civ, the earlier they adopted pottery, and the more widely they used it, the more we should expect to find pottery shards. Similar for widespread use of metal. But if there are plausible civ hypotheses that don’t require them to be as big, or as much into stuff that creates long-lasting evidence like pottery shards, then the more trouble we’ll have rejecting such hypotheses.

One complication re lost advanced civilizations is that the last 7K years have seen especially calm weather worldwide. Before that, sea levels <a href="https://en.wikipedia.org/wiki/Past_sea_level#/media/File:Post-Glacial_Sea_Level.png">changed</a> a lot more, and before 10Kya temperatures <a href="https://wattsupwiththat.com/2016/09/29/earths-obliquity-and-temperature-over-the-last-20000-years/">changed</a> a lot more, and much of the Earth was covered with glaciers. There may even have been some huge worldwide cataclysms <a href="https://www.sciencemag.org/news/2018/11/massive-crater-under-greenland-s-ice-points-climate-altering-impact-time-humans">around</a> 12Kya. All this made it harder to sustain complex civilizations back then, but also made it harder to preserve evidence of them for us to see now, if they has been there.

Seems to me we want something like prediction markets here, to give better incentives and aggregation re predictions of what stuff will be found where re when. So let me suggest: markets in archeology prize obligations.

First, let’s set up some archeology prizes, each of which pays $P to the first group who can show an X found in region R from before date D. For example, show a homo sapien skull found in the Americans dated before 200Kya. Define $P in units of some standard investment asset, like the S&P 5000 or MSCI All Country World Index. Then create markets where people can be paid in those same units to take on fractions of prize obligations. For example, someone might be paid 10 units to take on an obligation to pay 100 units of the pre-200Kya America skeleton prize.

The asset ratio price in these markets, such as the 10% ratio of 10 to 100 in the example above, could be interpreted as a probability that the prize will ever be won. With enough kinds of prizes for enough findings X, regions R, and dates D, we could get a pretty good picture of what we are likely to find. Such as lost advanced civilizations. These prize payments would encourage more archeology effort to discover things.

Skeptics who see little chance of dramatic findings might eagerly be paid to take on such obligations, while enthusiasts who see such discoveries as more likely could take the opposite sides of such transactions. Each side can expect to profit by reversing their trade when the world comes to its senses and agrees with them, which might happen long before any actual discoveries are paid prizes.

Investigators who expect to be able to show particular findings soon might offer to pay now for others to take on obligations to pay bigger related prizes later. And these relative prices might give investigators hints about what to look for where.

As it is quite legal to pay out prizes and to transfer obligations to pay prizes, all of this looks pretty legal to me. (But I’m not a lawyer, so of course not legally allowed to state opinions on such things.) Yes, you’d need to set things up to ensure that people will make good on obligations to pay prizes, but that seems feasible. We could get even more trading if anyone were allowed to pay to become an auxiliary prize recipient in case a prize was one by someone, but I’m less sure that would be considered legal.

So, who wants to help set this up?

<strong>Added 9a:</strong> During the classic Egypt era, many monuments were built over and near apparently much older sites with much older monuments built using apparently advanced tech. Many of these older sites have very large tunnel systems, many of which are from being fully explored. That is my best bet re where to look for evidence of lost advanced civs.

## [Try-Try or Try-Once Great Filter?](#table-of-contents)
_Posted on 2020-12-03_

[Here’s](new-hard-steps-results) <a href="https://www.liebertpub.com/doi/10.1089/ast.2019.2149">a</a> <a href="http://mason.gmu.edu/~rhanson/greatfilter.html">simple</a> <a href="http://mason.gmu.edu/~rhanson/hardstep.pdf">and</a> pretty standard theory of the origin and history of life and intelligence. Life can exist in a supporting oasis (e.g., Earth’s surface) that has a volume <em>V</em> and metabolism <em>M</em> per unit volume, and which lasts for a time window <em>W</em> between forming and then later ending. This oasis makes discrete “advances” between levels over time, and at any one time the entire oasis is at the same level. For example, an oasis may start at the level of simple dead chemical activity, may later rise to a level that counts as “life”, then rise to a level that includes “intelligence”, and finally to a level where civilization makes a big loud noises that are visible as clearly artificial from far away in the universe.
There can be different kinds of levels, each with a different process for stepping to the next level. For example, at a “delay” level, the oasis takes a fixed time delay <em>D</em> to move to the next level. At a “[try once](two-types-of-future-filters)” level, the oasis has a particular probability of immediately stepping to the next level, and if it fails at that it stays forever “stuck”, which is equivalent to a level with an infinite delay. And at a “try try” level, the oasis stays at a level while it searches for an “innovation” to allow it to step to the next level. This search produces a constant rate per unit time of jumping. As an oasis exists for only a limited window <em>W</em>, it may never reach high levels, and in fact may never get beyond its first try-try level.
If we consider a high level above many hard try-try levels, and with small enough values of <em>V,M,W</em>, then any one oasis may have a very small chance of “succeeding” at reaching that high level before its window ends. In this case, there is a “great filter” that stands between the initial state of the oasis and a final success state. Such a success would then only tend to happen somewhere if there are enough similar oases going through this process, to overcome these small odds at each oasis. And if we know that very few of many similar such oases actually succeed, then we know that each must face a great filter. For example, knowing that we humans now can see no big loud artificial activity for a very long distance from us tells us that planets out there face a great filter between their starting level and that big loud level.

Each try-try type level has an expected time <em>E</em> to step to the next level, a time that goes inversely as <em>V*M</em>. After all, the more volume there is of stuff that tries, and faster its local activity, the more chances it has to find an innovation. A key division between such random levels is between ones in which this expected time <em>E</em> is much less than, or much greater than, the oasis window <em>W</em>. When <em>E &lt;&lt; W</em>, these jumps are fast and “easy”, and so levels change relatively steadily over time, at a rate proportional to <em>V*M</em>. And when <em>E &gt;&gt; W</em>, then these jumps are so “hard” that most oases never succeed at them.

Let us focus for now on oases that face a great filter, have no try-once steps, and yet succeed against the odds. There are some useful patterns to note here. First, let’s set aside <em>S</em>, the sum of the delays <em>D</em> for delay steps, and of the expected times <em>E</em> for easy try-try steps, for all such steps between the initial level and the success level. Such an oasis then really only has a time duration of about <em>W-S</em> to do all its required hard try-try steps.

The first pattern to note is that the chance that an oasis does all these hard steps within its window <em>W</em> is proportional to <em>(V*M*(W-S))<sup>N</sup></em>, where <em>N</em> is the number of these hard steps needed to reach its success level. So if we are trying to predict which of many differing oases is mostly likely to succeed, this is the formula to use.

The second pattern to note is that if an oasis succeeds in doing all its required hard steps within its <em>W-S</em> duration, then the time durations required to do each of the hard steps are all drawn from the <em>same</em> (roughly exponential) distribution, regardless of the value of <em>E</em> for those steps! Also, the time remaining in the oasis after the success level has been reached is <em>also</em> drawn from this same distribution. This makes concrete predictions about the pattern of times in the historical record of a successful oasis.

Now let’s try to compare this theory to the history of life on Earth. The first known fossils of cells seems to be from 0.1-0.5 Ga (billion years) after life would be possible on Earth, which happened about 4.2 Gya (billion years ago), which was about 9.6 Ga after the universe formed. The window remaining for (eukaryotic) life to remain on Earth seems 0.8-1.5 Ga. The relatively [steady](brain-size-is-not-filter) growth in max brain sizes since multi-cellular life arose 0.5 Gya suggests that during this period there were many easy, but no hard, try-try steps. Multi-celluar life seems to require sufficient oxygen in the atmosphere, but the process of collecting enough oxygen seems to have started about 2.4 Gya, implying a long 1.9 Ga delay step. Prokaryotes started exchanging genes <a href="https://en.wikipedia.org/wiki/Evolution_of_sexual_reproduction">about</a> 2.0 Gya, eukaryotes appeared about 1.7 Gya, and modern sex appeared about 1.2 Gya. These events may or may not have been the result of successful try-try steps.
Can we test this history against the predictions that try-try hard step durations, and the window time remaining, should all be drawn from the same roughly exponential distribution? Prokaryote sex, eukaryotes, and modern sex all appeared within 0.8 Ga, which seems rather close together, and leaving a long uneventful period of almost ~2 Ga before them. The clearest hard step duration candidates are before the first life, which took 0.0-0.5 Ga, and the window remaining of 0.8-1.5 Ga, which could be pretty different durations. Overall I’d say that while this data isn’t a clear refutation of the same hard step distribution hypothesis, it also isn’t that much of a confirmation.

What about the prediction that the chance of oasis success is proportional to <em>(V*M*(W-S))<sup>N</sup></em>? The prediction about Earth is that it will tend to score high on this metric, as Earth is the only example of success that we know.

Let’s consider some predictions in turn, starting with metabolism <em>M</em>. Life of the sort that we know seems to allow only a limited range of temperatures, and near a star that requires a limited range of distances from the star, which then implies a limited range of metabolisms <em>M</em>. As a result of this limited range of possible <em>M</em>, our prediction that oases with larger <em>M</em> will have higher chances of success doesn’t have much room to show itself. But for what its worth, Earth <a href="https://en.wikipedia.org/wiki/Circumstellar_habitable_zone">seems</a> to be nearer to the inner than outer edge of the Sun’s allowable zone, giving it a higher value of <em>M</em>. So that’s a weak confirmation of the theory, though it would be stronger if the allowed zone range were larger than most authors now estimate.

What about volume <em>V</em>? The radii of non-gas-giant planets <a href="https://www.researchgate.net/publication/325816854_Planet_Size_Distribution_from_the_Kepler_Mission_and_its_Implications_for_Planet_Formation/figures?lo=1">seems</a> to be lognormally distributed, with Earth at the low end of the distribution (at a value of 1 on this axis):

[](Planet-Size-Distribiution)
So there are many planets out there (at r=4) with 16 times Earth’s surface area, and with 64 times the volume, ratios that must be raised to the power of<em> N</em> to give their advantage over Earth. And these larger planets are made much more of water than is Earth. This seems to be a substantial, if perhaps not overwhelming, disconfirmation of the prediction that Earth would score high on <em>V<sup>N</sup></em>. The higher is the number of hard steps <em>N</em>, the stronger is this disconfirmation.

Regarding the time window <em>W</em>, I see three relevant parameters: when a planet’s star formed, how long that star lasts, and how often there are supernova nearby that destroy all life on the planet. Regarding star lifetimes, main sequence star luminosity <a href="http://www.ucolick.org/~bolte/AY4_04/class4_04bwd.pdf">goes</a> as mass to the ~3.5-4.0 power, which implies that star lifetimes go inversely as mass to the ~2.5-3.0 power. And as the smallest viable stars have 0.08 of our sun’s mass, that implies that there are stars with ~500-2000 times the Sun’s lifetime, an advantage that must again be raised to the power <em>N</em>. And there are actually a <a href="https://en.wikipedia.org/wiki/Initial_mass_function#/media/File:Plot_of_various_initial_mass_functions.svg">lot</a> more such stars, 10-100 times more than of the Sun’s size:

[](StarMassDistribution)
However, the higher metabolism of larger mass stars gives them a spatially wider habitable zone for planets nearby, and planets near small stars are said to face <a href="https://en.wikipedia.org/wiki/Habitability_of_red_dwarf_systems#:~:text=Intense%20tidal%20heating%20caused%20by,life%20developing%20in%20these%20systems.&amp;text=There%20are%20expected%20to%20be,stars%20in%20the%20Milky%20Way">other</a> problems; how much does that compensate? And double stars should also offer wider habitable zones; so why is our Sun single?

Now what if life that appears near small long-lived stars would appear too late, as life that appeared earlier would spread and take over? In this case, we are talking about a race to see which oases can achieve intelligence or big loud civilizations before others. In which case, the prediction is that winning oases are the ones that appeared first in time, as well has having good metrics of <em>V,M,W</em>.

Regarding that, here are <a href="https://arxiv.org/pdf/1509.02832.pdf">estimates</a> of [where](galaxy-calc-shows-aliens) the habitable stars appear in time and galactic radii, taking into account both star formation rates and local supernovae rates (with the Sun’s position shown via a yellow star):
[](GalacticHabitableZone)
As you can see, our Sun is far from the earliest, and its quite a bit closer to galactic center than is ideal for its time. And if the game isn’t a race to be first, our Sun seems much earlier than is ideal (these estimates are arbitrarily stopped at 10Ga).

Taken together, all this seems to me to give a substantial disconfirmation of the theory that chance of oasis success is proportional to <em>(V*M*(W-S))<sup>N</sup></em>, a disconfirmation that gets stronger the larger is <em>N</em>. So depending on <em>N</em>, maybe not an overwhelming disconfirmation, but at least substantial and worrisome. Yes, we might yet discover more constraints on habitability to explain all these, but until we find them, we must worry about the implications of our analysis of the situation as we best understand it.

So what alternative theories do we have to consider? In this post, I’d like to suggest replacing try-try steps with try-once steps in the great filter. These might, for example, be due to evolution’s choices of key standards, such as the genetic code, choices that tend to lock in and get entrenched, preventing competing standards from being tried. The overall chance of success with try-once steps goes as the number of oases, and is independent of oasis lifetime, volume, or metabolism, favoring many small oases relative to a few big ones. With more try-once steps, we need fewer try-try steps in the great filter, and thus <em>N</em> gets slower, weakening our prediction conflicts. In addition, many try-once steps could unproblematically happen close to each other in time.

This seems attractive to me because I estimate there to be in fact a <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1526419/">great</a> many rather hard steps. Say at least ten. This is because the design of even “simple” single cell organisms seems to me amazingly complex and well-integrated. (Just <a href="http://www.digizyme.com/cst_landscapes.html">look</a> at it.) “Recent” life innovations like eukaryotes, different kinds of sex, and multicellular organisms do involved substantial complexity, but the total complexity of life seems to me <em>far</em> larger than these. And while incremental evolution is capable of generating a lot of complexity and integration, I expect that what we see in even the simplest cells must have involved a lot of hard steps, of either the try-once or the try-try type. And if they are all try-try steps, that makes for a huge <em>N</em>, which makes the prediction conflicts above very difficult to overcome.

Well that’s enough for this post, but I expect to have more to say on the subject soon.

<strong>Added 19Jan:</strong> Turns out we <a href="https://arxiv.org/abs/1507.04346">also</a> seem to be in the wrong kind of galaxy; each giant elliptical with a low star formation rate hosts 100-10K times more habitable Earth-like planets, and a million times as many habitable gas giants, than does our Milky Way.

## [Great Filter with Set-Backs, Dead-Ends](#table-of-contents)
_Posted on 2022-04-01_

A biological cell becomes cancerous if a certain set of rare mutations all happen in that same cell before its organism dies. This is quite unlikely to happen in any one cell, but a large organism has enough cells to create a substantial chance of cancer appearing somewhere in it before it dies. If the chances of mutations are independent across time, then the durations between the timing of mutations should be roughly equal, and the chance of cancer in an organism rises as a power law in time, with the power equal to the number of required mutations, usually around six.

A similar process may describe how an advanced civilization like ours arises from a once lifeless planet. Life may need to advance through a number of “hard step” transitions, each of which has a very low chance per unit time of happening. Like evolving photosynthesis or sexual reproduction. But even if the chance of advanced life appearing on any one planet before it becomes inhabitable is quite low, there can be enough planets in the universe to make the chance of life appearing somewhere high.

As with cancer, we can predict that on a planet lucky enough to birth advanced life, the time durations between its step transitions should be roughly equal, and the overall chance of success should rise with time as the power of the number of steps. Looking at the history of life on Earth, many observers have estimated that we went through roughly six (range ~3-12) hard steps.

In our [grabby aliens](http://grabbyaliens.com/) analysis, we say that a power of this magnitude suggests that Earth life has arrived very early in the history of the universe, compared to when it would arrive if the universe would wait empty for it to arrive. Which suggests that grabby aliens are out there, have now filled roughly half the universe, and will soon fill all of it, creating a deadline soon that explains why we are so early. And this power lets us estimate how soon we would meet them: in roughly a billion years.

According to this simple model, the short durations of the periods associated with the first appearance of life, and with the last half billion years of complex life, suggest that at most one hard step was associated with each of these periods. (The steady progress over the last half billion years also suggests this, though our paper describes a “multi-step” process by which the equivalent of many hard steps might be associated with somewhat steady progress.)

In an excellent new [paper](https://royalsocietypublishing.org/doi/10.1098/rspb.2021.2711) in the _Proceedings of the Royal Society_, “Catastrophe risk can accelerate unlikely evolutionary transitions”, Andrew Snyder-Beattie and Michael Bonsall extend this standard model to include set-backs and dead-ends.

> Here, we generalize the [standard] model and explore this hypothesis by including catastrophes that can ‘undo’ an evolutionary transition. Introducing catastrophes or evolutionary dead ends can create situations in which critical steps occur rapidly or in clusters, suggesting that past estimates of the number of critical steps could be underestimated. ([more](https://royalsocietypublishing.org/doi/10.1098/rspb.2021.2711))

Their analysis looks solid to me. They consider scenarios where, relative to the transition rate at which a hard step would be achieved, there is a higher rate of a planet “undoing” its last hard step, or of that planet instead switching to a stable “stuck” state from which no further transitions are possible. In this case, advanced life is achieved mainly in scenarios where the hard steps that are vulnerable to these problems are achieved in a shorter time than it takes to undo or stuck them.

As a result, the hard steps which are vulnerable to these set-back or dead-end problems tend to happen together much faster than would other sorts of hard steps. So if life on early Earth was especially fragile amid especially frequent large asteroid impacts, many hard steps might have been achieved then in a short period. And if in the last half billion years advanced life has been especially fragile and vulnerable to astronomical disasters, there might have been more hard steps within that period as well.

Their paper only looks at the durations between steps, and doesn’t ask if these model modifications change the overall power law formula for the chance of success as a function of time. But my math intuition is telling me it feels pretty sure that the power law dependence will remain, where the power now goes as the number of all these steps, including the ones that happen fast. Thus as these scenarios introduce more hard steps into Earth history, the overall power law dependence of our grabby aliens model should remain but become associated with a higher power. Maybe more like twelve instead of six.

With a higher power, we will meet grabby aliens sooner, and each such civilization will control fewer (but still many) galaxies. Many graphs showing how our predictions vary with this power parameter can be found in our [grabby aliens](http://grabbyaliens.com/) paper.

## [Seeing ANYTHING Other Than Huge-Civ Is Bad News](#table-of-contents)
_Posted on 2021-07-04_

The <a href="http://hanson.gmu.edu/greatfilter.html">great filter</a> is whatever obstacles prevent simple dead matter from evolving into a civilization big and visible on astronomical scales. The fact that we see nothing big and visible in a huge universe says this filter must be large, and a key question is the size of the future filter: how much have we passed and how much remains ahead of us?

I’ve suggested that evidence of life elsewhere below our level makes the past filter look smaller, and thus our future filter larger. From which you might conclude that evidence of a civilization above our level is good news. That seems to be what <span class="c-byline__item"><span class="c-byline__author-name"><a data-analytics-link="author-name" href="https://www.vox.com/authors/dylan">Dylan Matthews</a></span></span> says here at Vox:

> If (and I must stress that this is a quite unlikely “if”) UFO sightings on earth are actually evidence that an advanced alien civilization has developed a system of long-distance probes that it is using to monitor or contact humanity, then that would be an immensely hopeful sign in Great Filter terms. It would mean that at least one civilization has far surpassed humanity without encountering any insurmountable hurdles preventing its survival. (<a href="https://www.vox.com/future-perfect/22556083/ufo-uap-report-fermi-paradox-aliens">more</a>)

But I don’t think that’s right. This would move the filter more to above their level, but below the level of becoming big and visible, without changing the size of the total filter. Which implies a larger future filter for us. In addition, any UFO aliens are [likely](ufos-what-the-hell) here to actively impose a filter on us, i.e., to stop us from getting big and visible (or “<a href="http://grabbyaliens.com">grabby</a>“).
So if UFOs as aliens is not good news, what <em>would</em> be good news re our future filter? Aside from detailed engineering and social calculations showing that we are in fact very close to becoming irreversibly grabby, the only good news I can imagine is actual concrete evidence of big visible aliens civilizations out there. Maybe we’ve misread their signatures somehow.

Looking out further and in more detail at the universe and still finding it dead suggests the total filter is larger, which is bad news. And finding any evidence of anything other than death suggests the filter is smaller up to the level of that finding, but doesn’t revise our estimate of the total filter. Which is bad news re our future. Thus a perhaps surprising conclusion: finding <em>anything</em> other than a big visible civilization out there is bad news re our future prospects for becoming big and visible.

Remember also: the SIA indexical prior (IMHO the reasonable choice) <a href="https://meteuphoric.com/2010/03/23/sia-doomsday-the-filter-is-ahead/">favors</a> <a href="https://arxiv.org/abs/2106.13348">larger</a> future filters. Beware the future filter!

## [Our Level in the Great Filter](#table-of-contents)
_Posted on 2022-07-08_

An exchange between Astrophysicist [Charles Lineweaver](https://www.mso.anu.edu.au/~charley/) and myself:

In their 2019 paper “The [Timing of Evolutionary Transitions Suggests Intelligent Life is Rare](https://www.liebertpub.com/doi/10.1089/ast.2019.2149)”, Snyder-Beattie, Sandberg, Drexler, and Bonsall argue that the expected time for “intelligent life” to appear on Earth “likely exceed the lifetime of Earth, perhaps by many orders of magnitude” which “corroborate[s] the original argument suggested by Brandon Carter that intelligent life in the Universe is exceptionally rare.”

In a Feb. 2022 comment in _Inference_, “[A Lonely Universe](https://inference-review.com/article/a-lonely-universe)”, Charles Lineweaver disagreed:

> The Snyder-Beattie et al. result depends on the assumption that … the major transitions that characterize our evolution happen elsewhere. There is little evidence in the history of life on earth to support this assumption. … transition to human-like intelligence or technological intelligence occurred only about 100,000 years ago and is species-specific. The latter trait is strong evidence we should not expect to find it elsewhere.
> 
> It [is not] reasonable to argue that … the features of life on earth … most likely to appear in life elsewhere are those that have evolved independently many times, such as complex multicellularity, eyes, wings, and canines. … [because] these … have only occurred within a unique [never-repeated] eukaryotic branch that represents a tiny fraction of the diversity of life on earth. …
> 
> Attempting to compute the probability of human-like intelligence elsewhere based on our lineage is akin to analyzing the evolution of the English language on earth and trying to use the timing of the Great Vowel Shift to estimate its timing on other planets

My July 2022 [reply](https://inference-review.com/letter/understanding-the-chances-for-life), also in _Inference,_ says:

> Lineweaver suggests that without good reasons to think “the major transitions that characterize our evolution happen elsewhere,” estimates regarding Earth do not allow us to make estimates regarding other planets.
> 
> On the contrary, I see two ways to compare planets so that Earth estimates become relevant for other planets, allowing us to infer a low overall rate at which advanced life appears elsewhere. First, if Earth is a random sample from planets that succeed in making life at our level, the success rate on Earth cannot be too different from the typical success rate on other such planets. Second, if there is a substantial chance that our descendants will soon become very visible in the universe, the fact that no other star in our galaxy has yet done so can set a low upper bound on the fraction of such stars that can have reached our level by now. …
> 
> Let R be the chance of life at our current level—i.e., controlling nuclear power and practicing spaceflight—appearing on a particular planet within some fixed planet habitability duration. … chance Q that, within the following ten million years, a planet at our level would give rise to a civilization that becomes permanently visible across its entire galaxy. [I elaborated with math examples for both these approaches.]

In that [same](https://inference-review.com/letter/understanding-the-chances-for-life) place, Lineweaver then responded:

> I don’t believe in the general group that he and many others call “advanced life.” … No other life-forms in the universe will be genetically or phenotypically more similar to us than chimps, bonobos, gorillas, naked mole rats, or frogs. Since Hanson and many others exclude our closest relatives from “advanced life,” they are—by their definition—not talking about a generic group with other members. …
> 
> On Earth, humans are the only ones who have become humans at our level of technology. To then conclude that among all species, our species had an average chance of becoming humans at our level is meaningless. …
> 
> Morris … argues that strong selection pressure leads to [convergent](https://link-springer-com.mutex.gmu.edu/chapter/10.1007/978-1-4020-8837-7_17) evolution which then produces human-like intelligence. Hanson and most physicists subscribe to this view, but most biologists and I don’t. … Hanson refers to … life at our level … I … ask: If we exclude our species from consideration, does this talk of levels make any sense when applied to the rest of life? Are dogs or red oak trees at a higher level?

Reading Lineweaver’s response, I see my reply was off target; his issue is with the very idea of “life at our level”. So let me try again.

A key datapoint is this: we do _not_ now see any big visible civilizations (BVC) in the sky who have greatly changed the natural universe into something more to their liking. In order to explain this fact, we must postulate a “[great filter](https://en.wikipedia.org/wiki/Great_Filter)”, i.e., a process whereby simple dead matter _might_ give rise first to simple life, and then to a BVC, or various filter obstacles might end this progress, so that it never produces a BVC. We must conclude that so far, averaging across the universe, this filter process has a _very_ low total pass-through rate to a BVC. After all, _no_ dead matter in the entire universe has yet given rise to a BVC we can see. That is, this great filter is on average very large.

In contrast, Earth today seems to plausibly have a much higher rate for creating BVC. I’d say we have at least a one in a million chance of doing so within the next ten million years. (This isn’t value judgement, just an estimate.) As Earth is now thus much closer to this BVC endpoint than it was originally, there is a sense in which Earth has now passed through part of the great filter, so that a substantially smaller filter lies before us than once lied before a simple dead Earth.

To talk about how much of the great filter we have so far passed, we’d like a way to talk about where we “are now” in this filter process. And this is where we can want to talk about our current “level” along some linear path from dead matter to BVC. But, as Lineweaver points out, evolution is in many ways a tree, instead of a line, and we cannot construct such a level concept merely by creating a conjunct of various random specific features of our species and planet.

Even so, I do think there are useful ways to define “our level” (OL) within the great filter. What we want is an equivalence class OL of alien civilization-moments such that (a) Earth today is in OL, (b) almost all BVC were once in OL at some prior point in their history, and (c) OL covers only a short “time slice” during which few civilizations go extinct. If we have more choices, we’d further like to pick OL so that (d) it minimizes the variance in the (coarse-grained) chance that each civilizations in OL later gives rise to a BVC. The lower this variance, the more it makes sense to talk in terms of the average chance within OL of giving rise later to a BVC.

One option would be to just define OL as the class that meets criteria (a,b,c) and actually minimizes (d). But while this might be well defined, it seems unwieldy. Which is why I tried above to define OL above in terms of a civilization having just mastered the basics of both nuclear power and spaceflight. It might be reasonable to add a few other techs to this list, such as computers.  
Sure, we’d define somewhat different OL sets if we added or cut techs from this list. But the key point is that any civilization that had mastered all of them would be well on its way to being able to start a BVC soon. And most likely the chance of extinction is low between the point of having mastered half of these techs and mastering all of them. Thus the exact list of techs in our OL definition probably doesn’t make that much difference.

Yes, this way to define OL can let humans pass through OL, while chimps never do. But I just don’t see why that’s a problem. There is in fact a big important difference between what humans and chimps have accomplished, and I’m fine with our OL definition reflecting that.

## [At Least Two Filters](#table-of-contents)
_Posted on 2010-11-28_

Where lies the great filter, i.e., the obstacles that make it extremely unlikely that any one chunk of pre-organic matter originates a visibly expanding interstellar civilization? While it seems [unlikely](brain-size-is-not-filter) our ancestors passed through much of a filter in the last half billion years, our descendants may face a big filter in the next few thousand years, and there may have been big filters associated with the origin of life, the spread of life, the invention of complex cells, sexual reproduction, or multicellular life.
In many folks eyes, an elegantly simple resolution, which is likely because of its simplicity, is to assume there is just one huge filter: the origin of life. Assuming that first step is enormously hard allows one to think all the other steps are pretty easy. They wouldn’t be sure things of course, but conditional on a big enough origin-of-life filter, one wouldn’t have a strong reason to fear that common analyses underestimate future filters.

Unfortunately, the elegantly simple hypothesis that the great filter is mainly a big origin-of-life filter seems at odds with our best evidence. Why? Because if the spread-of-life step had the weakest possible associated filter, then life spreading must be easy. Over billions of years life could have [spread](pondering-panspermia) to many star systems from its place of [origin](all-hail-william-napier):
Life could spread across a galaxy via giant molecular clouds reliably collecting life from the stars they drift near, and then passing that life on to a few of the thousands of new stars they create.

If over billions of years life spread to many <a href="http://www.scientificamerican.com/article.cfm?id=the-long-lost-siblings-of-the-sun">hundreds</a>, or even billions, of star systems, and no substantial filters stood between arrival of life near a star, and its eventual development of advanced technical civilizations like ours, then why would we now see no any evidence of other civilizations? Yes it is <em>possible</em> that we are the very first, but that hypothesis is of course unlikely by default.

It seems to me that if the great filter is to consist of just one big step, the only plausible possibility is the development of multi-cellular life. All the steps before that one seem able to spread to other star systems via single-celled life hidden in dust, and [it seems](brain-size-is-not-filter) we haven’t had a big filter step since the multi-cellular innovation.
So if the idea of just one big filter appeals to your sense of elegance, you’ll have to presume that life, including complex life with sexual reproduction etc., is very common in our vast universe, but that Earth is one of the handful of places in all that vastness with multi-cellular life.

If you don’t find that plausible, well then you’ll have to grant there are at least two filters. And if two, why not three? So you must find the possibility of a third filter in our future plausible; beware [future](beware-future-filters) [filters](fertility-the-big-problem).

## [Fertility: The Big Problem](#table-of-contents)
_Posted on 2010-11-15_

Many folks want to save the world. Especially young, single, energetic folks. Especially if they also get to:

<ol>
<li>Support their side in common political/etc. [divides](questions-for-great-divides).</li>
<li>Affiliate with statusful prestigious folks who share their cause.</li>
<li>Network with other young energetic single folk in the process.</li>
<li>Show off being informed on progress, options on this issue.</li>
<li>Show off via gadget making, activity organizing, or art.</li>
<li>Show devotion and self-control via paying exceptional costs.</li>
<li>Have a vivid chance of making a huge personal difference.</li>
</ol>
But alas, while popular save-the-world causes offer many such perks, the cause of fertility, my guess for the world’s biggest problem today, is neglected in part because it offers few such perks.

The problem is this: If the falling-fertility trend of the last two centuries continues for another century (see fertility vs time and income <a href="http://www.bit.ly/9sf6v8 ">here</a>; more fertility stats <a href="http://www.un.org/esa/population/publications/worldfertility2007/worldfertility2007.htm">here</a>), we might well see a fully-developed world with fertility &lt;1.5, lifespan &gt;90, tax funded [leisure](old-are-lazy-but-fit) for all over 65, and [perhaps](remember-the-hmo-revolution) also &gt;30% of GDP spent on “free” medicine for all. The resulting rapidly falling population would cut the scale economies that contribute to economic growth today. And strong intrusive innovation-limiting global governments might be required to keep young workers paying &gt;75% <a href="https://turbotax.intuit.com/">income tax rates</a> to support the retired masses. (Imagine young low-tax African nations forced at gunpoint to pay “their share” of the world’s retiree burden.)
Yes, robots might save us, yes even if they don’t growth will probably continue anyway, and yes eventually if incomes fell far enough or with enough time fertility would eventually rise again. So this is not directly an existential risk. But such a long stressful period would at least make us more vulnerable to other risks, risks that [great filter](beware-future-filters) considerations suggest are bigger than they seem. Yes, other potential problems may seem more serious than falling fertility, but remember those are mostly hypothetical, while falling fertility is actually happening.
This fertility problem is in principle easily reduced: just have more kids. But since that strategy offers few of the extra cause-perks listed above, I don’t expect fertility to become a popular cause. After all, we’ve seen this problem coming for a while, and it will take a long while to play out. So you can’t claim to be in the vanguard of a perceptive few who finally see the problem, or who will finally solve it. Elites have long been leaders in lowering fertility, making more-fertility folks seem lower status. The fertility problem doesn’t offer many excuses for new gadgets or networking events, and the joys of parenthood have long been explored in the arts. Furthermore, if you pick mates before having kids, having kids works poorly as an excuse to meet potential mates. Finally, your having more kids can only make a tiny dent in the overall problem, and the sacrifices you’d make to have kids would not be exceptional relative to your ancestors’ sacrifices. It is hard to tell grand hero stories here.

The good news is that we understand our likely biggest problem well enough that you can do something substantial about it, nearly as much as anyone can do. And, alas, that is also the bad news.

Now for many long quotes from two articles. First a <a href="http://www.weeklystandard.com/articles/america%E2%80%99s-one-child-policy">recent article</a>:<span id="more-24888"></span>

[The] Chinese fertility rate … now sits somewhere between 1.9 and 1.3, depending on who is doing the tabulating. … “In some major population centers—Beijing, Shanghai, and Tianjin among them—it appears that the average number of births per woman is amazingly low: below one baby per lifetime.” … By 2050, the age structure in China will be such that there are only 1.6 workers—today the country has 5.4—to support each retiree. …

America’s 2.06 is one of the highest fertility rates in the First World. Only Israel (2.75) and New Zealand (2.10) are more fertile. … China and America have yet to witness the effects of falling fertility because of demographic momentum. Populations increase even as fertility rates collapse, until the last above-replacement generation dies, after which the population begins contracting. The rate of contraction speeds up as each generation passes. No society has ever experienced prosperity in the wake of contracting population. …

In 2010, the Japanese fertility rate is 1.2. … the most prevalent new demographic archetype is the … “parasite single,” … college-educated, working women who live with their parents well into their 30s—not because they are too poor to pay rent, but because they spend their salaries on designer clothes, international travel, and fancy restaurants. The parasite singles are Japan’s biggest consumer group. ….

The problem with immigration as it relates to fertility isn’t the old complaint that the newcomers are out-breeding the natives. Rather, the problem is that the newcomers start behaving like natives too soon, with their [fertility] regressing quickly to the mean. … One of the best predictors of fertility is education. … It drops to 1.6 for [US] women with a graduate degree. One of the drivers of our fertility decline was the making of college de rigueur for middle-class women. …

Throughout history, governments have tried to get people to procreate. Augustus levied a “bachelor tax” on unmarried, aristocratic men. In 1927, Mussolini imposed a tax on all unmarried men between the ages of 25 and 65. … In 1944, … Stalin created the Motherhood Medal, given to any woman who bore at least six children. None of these attempts was successful. …

Singapore’s fertility rate was already in decline, having fallen from 5.45 in 1960 to 4.7 in 1965. … The government wanted to drive the fertility rate down even faster. .. Abortion was sanctioned—and even encouraged—at every stage. Parents who had more than two children were punished with no paid maternity leave and higher hospital charges for the delivery of the extra babies. Couples were encouraged to volunteer for sterilization. Parents who did so after having just one or two children were reimbursed for the medical costs of delivering those babies and their children were given preference in registering for the best schools.

The tactics were frighteningly effective. In 1976—just ten years after the campaign began—Singapore reached its target of 2.1. … But the rate kept diving, down to 1.74 by 1980. The biggest fertility decline came from the elites. … In an attempt to boost fertility rates among the elites, the government began offering big tax breaks to highly educated women who had three or more children. … None of it worked. … By 1984, Singapore’s fertility rate was 1.62 and falling. … Unpaid maternity leave for government workers was increased from one year to four years. … Yet the effort has met with total and unremitting failure. In 2001, Singapore’s fertility rate was 1.41. By 2004 it was 1.24. Today [in 2010] it is 1.1.

Next, a good 2006 <em>Science</em> <a href="http://www.sciencemag.org.mutex.gmu.edu/cgi/content/summary/312/5782/1894">article</a>:

As fertility rates decline across the developed world, governments are offering big incentives for childbearing. Experts don’t expect them to have much effect.

The E.U. will lose between 24 million and 40 million people during each coming decade. … Population losses could bring a raft of negative economic consequences in the industrialized world, as well as greater stresses on social security and health care systems as the proportion of older citizens increases. … Some believe very low fertility rates are here to stay. … “While the additional [government] financial support is bound to be welcomed by parents, the overall effect on fertility is likely to be small.” … Both sides agree that falling fertility rates might be irreversible once they drop<br/>
below a certain level—what some demographers have begun to call the “low-fertility trap.” …

Demographers define a replacement-level [fertility] as 2.1—slightly more than a flat rate, to account for the small fraction of children who die before reaching reproductive age. Yet nearly all of the world’s industrialized nations have [fertilities] well below this magic number. … Only the United States, exceptional in the developed world, hits the replacement mark, with a [fertility] of 2.09. … Although [fertilities] remain high in some of the world’s poorest countries … the demographic transition is either under way or completed in most nations. … The process has taken place even in relatively poor countries such as Mexico, where [fertility] dropped from 6.5 to 2.5 between 1975 and 2005. … Demographers had [incorrectly] assumed that the decline would stop when replacement-level [fertilities] were reached. “During the early 1970s, everyone talked about the magic floor of replacement. … Nobody thought it would go below 2.1.” …

Several factors that make the [fertility] in the U.S. higher[:] … higher rate of unwanted pregnancies, … a lower unemployment rate, and a greater tendency for women to have children earlier in life, … [and] a stronger emphasis on religion and “traditional values” …

The key reason that economists and other experts are worried about low fertility rates is that they accelerate an overall “aging” of a population. … One way that many developed countries meet the challenge now is through immigration, which tends to increase the number of younger workers. Yet few demographers see immigration as the [long term] answer. …

The “window of opportunity” for family policies [to influence fertility] might actually be as little as 0.1 to 0.2 children per woman. … “Policies that would work would be so expensive that they will never be implemented.” … And some researchers have begun to think that it might actually be too late to reverse the trend in countries with the lowest fertility levels. … Once a nation’s [fertility] falls below 1.5, a downward demographic spiral sets in that makes it much more difficult to recover. … In Germany and Austria—nations with [fertility] of 1.39 and 1.36, respectively—young adults now consider their ideal family sizes to be as low as 1.7 children on average. “[In] Germany .. 30% of young people [are] not intending to have children. …

Reher maintained much of the world is now on the cusp of a prolonged period of population decline. The resulting population aging would lead to labor shortages even in developing countries. The result could be an economic disaster. … Santow … sees “nothing terrifying about a drop in the size of Europe’s population. Any decline will take time, and economies will adjust.”

## [Humans Are Early](#table-of-contents)
_Posted on 2021-02-03_

Imagine that advanced life like us is terribly rare in the universe. So damn rare that if we had not shown up, then our region of the universe would almost surely have forever remained dead, for eons and eons. In this case, we should still be able to predict <em>when</em> we humans showed up, which happens to be now at 13.8 billion years after the universe began. Because we showed up on a planet near a star, and we know the rate at which our universe has and will make stars, how long those stars will last, and which stars where lived far enough away from frequent sterilizing explosions to have at least a chance at birthing advanced life.

However, this chart (taken from our <a href="https://arxiv.org/abs/2102.01522">new paper</a>) calculates the percentile rank of our current date within this larger distribution. And it finds that we are surprisingly early, <em>unless</em> you assume <em>both</em> that there are very few hard steps in the evolution of advanced life (the “power n”), <em>and also</em> that the cutoff in lifetime above which planets simply cannot birth advanced life is very low. While most stars have <em>much</em> longer lives, <em>none</em> of those have any chance whatsoever to birth advanced life. (The x-axis shown extends from Earth’s lifetime up to the max known star lifetime.)

[](earlyness_k0p12d4)
In the paper (in figures 2,17), we also show how this percentile varies with three other parameters: the timescale on which star formation decays, the peak date for habitable star formation, and a “mass favoring power” which says bu how much more are larger mass stars favored in habitability. We find that these parameters mostly make only modest differences; the key puzzle of humans earliness remains.

Yes, whether a planet gives rise to advanced life might depend on a great many other parameters not included in our calculations. But as we are only trying to estimate the date of arrival, not many other details, we only need to include factors that correlate greatly with arrival date.

Why have others not reported the puzzle previously? Because they neglected to include the key hard-steps power law effect in how chances vary with time. This effect is not at all controversial, though it often seems counter-intuitive to those who have not worked through its derivation (and who are unwilling to accept a well-established literature they have not worked out for themselves).

This key fact that humans look early is one that seems best explained by a grabby aliens model. If grabby aliens come and take all the volume, that sets a deadline for when we could arrive, if we were to have a chance of becoming grabby. We are <em>not</em> early relative to that deadline.

## [An Alien War Nightmare](#table-of-contents)
_Posted on 2022-10-24_

Grabby aliens are advanced civs who change the stuff they touch in big visible ways, and who keep expanding fast until they meet each other. Our [recent analysis](http://grabbyaliens.com/) suggests that they appear at random stars roughly once per million galaxies, and then expand at roughly half the speed of light. Right now, they have filled roughly half of the universe, and if we join them we’ll meet them in roughly a billion years. There may be far more quiet than grabby alien civs out there, but those don’t usually do much or last long, and even the ruins of the nearest are quite far away.

While I’ve so far avoided thinking about much war within this scenario, I’ve decided to go there now. So here we go.

First, consider quiet alien wars. Such quiet civs may have internal wars, but different civs rarely get close enough to each other for physical fights. Maybe more advanced ones would sometimes conquer less advanced ones via malicious messages, but I’m skeptical that such events are common. The rare civs who expanded long and quietly mainly to preserve a natural universe and prevent grabby origins within their sphere of control should share goals and thus have little reason to war when they meet. Furthermore, when grabby civs meet quiet ones, abilities would be terribly unequal, and so not much of an occasion for war.

What about grabby civs? After a few million years they’d probably reach near max possible tech abilities. Which I guess makes them pretty immune to malicious messages. But such civs and their parts might vary in how well they had used a shared origin to promote internal cooperation. And a lack of perfect cooperation would likely result in some internal wars. The higher the rate at which they spend a fraction of their fast-access resources to fight or prevent fights, the [faster](https://www.overcomingbias.com/2015/04/stock-vs-flow-war.html) they’d use up such resources. As a result, such fast spending civs might only get resources for a long time if some of their resource sources, like black holes, only allowed slow extraction.

Long-distance ballistic directed energy weapons, which couldn’t be redirected along the way, would only be of use on targets whose locations could be predicted long enough in advance. As a result, grabby cis would usually ensure that the locations of important resources vulnerable to such attacks could not be so predicted. Similarly, they’d end or stay away from objects like stars that might be induced to explode by outside prods. Thus militarily-useful resources would likely need to maintain unpredictable locations and would need to be located quite close to where they’d be used. So conflicts would tend to be won locally by those with more military resources locally available near the point of conflict.

If grabby civs are not more able to or inclined to cooperate internally than with other civs, then each small part of such a civ should be similarly wary of neighboring advanced life, regardless of its civ of origin. In which case, the boundary at which different grabby civs meet might not have that much significance. Who wins each local conflict would mainly depend on their relative size, resources, level of internal cooperation, and local geography, but not civ of origin. On 100Mlyr and larger scales, this should add up to a pretty uniform picture.

However, what if at least some parts of some grabby cigs could use their shared origin to cooperate more strongly internally than they could with other grabby civs? In this case, they’d expect more conflict at the border where grabby civs meet, compared to at other locations. As a result, the cooperating units on both sides might then try to send resources to that border, in anticipation of such conflicts. And then a key question arises: just how fast is it feasible to move militarily useful resources?

Grabby civs expanding at half the speed might seem surprisingly fast, but this does seem roughly feasible given that they can afford to spend huge resources on speeding tiny seeds that can then use local resources to quickly grow exponentially into huge civs. Alas, no similar exponential strategy seems available to move resources from one place to another. If the resources required to accelerate resources to near the speed of light can be efficiently recaptured at a designation location, then perhaps resources could in fact be efficiently sent very far very fast. But otherwise, sending resources far fast (e.g., >2% of c) may only be possible at crazy high costs.

At the border between two grabby civs, imagine that one of the civs had better managed to tax internal regions to send more resources to that border from within that civ, and at a very rapid speed. In this case, then after a while the resources accumulated on one side of that border might be far larger than that on the other side. Then if the natural advantage of defense over offense were not too large, the stronger side might be able to initiate a war and take territory from the other side. And in fact this outcome might become so obvious that the losing side would be very sure to lose, and not even want to fight.

If merely threatening to attack with overwhelming force was usually sufficient to quickly rout the weaker side and win new territory, via induced surrender or flight, or if actual fights did not take too long or destroy too much of an attacker’s resources, then an attacker might continue to move forward into the other side’s territory at a rapid pace. And if that pace were on the order of 2% of the speed of light, that might be sufficient to completely take over all the territory of a neighboring grabby civ within the roughly hundred billion years remaining before the time when, it is now estimated, dark energy makes galaxy clusters disconnected, never more able to see or reach each other. Such attack threats might then be seen as existential risks to such a civ.

Putting this all together seems to me to create a nightmare scenario, one which might greatly worry many young grabby civs who take very long term views. And, importantly, they’d have to decide how scared to be of this scenario long before they had much info on each particular neighboring civ, or even on any other civs besides themselves. Thus fear of the unknown might push many such civs into paying huge costs to maintain strong governance able to heavily tax internal activity to fund the movement of large amounts of resources out to be ready for unknown future border conflicts. Resources which might be mostly wasted if two such well-prepared civs were to meet.

Thus the possibilities of (A) long term civ-level views, (B) cheap fast movement of military resources which were hard to convert back to civilian use, (C) a sufficiently low advantage of defense over offense, (D) within-civ governance strong enough to tax and transfer resources to the border, and (E) weak enough governance unable to prevent your side from fleeing or surrendering given overwhelming attackers, all of this together might induce the waste of much, or perhaps even the vast majority of, available resources. Resources that could instead be used to compute far more meaningful peaceful lives near where the required resources sat originally.

Also note that at the line-shaped borders where three grabby civs meet, all three might have equal resources. Even so, two of them allying against the third would gain an advantage. And if this were sufficient, they might together advanced into the third region, sharing the gains. After which, each of them might have a geometric advantage, partially encircling the other side where their border bends. The possibility of this ally advantage should induce grabby civs to try to seem more similar to each other, to induce others to ally with them.

## [Non-Grabby Legacies](#table-of-contents)
_Posted on 2021-03-08_

Our descendants will have far more effects on the universe if they become grabby, and most of their expected effects come in that scenario. Even so, as I discussed in my [last post](seti-optimism-is-human-future-pessimism), most see only a small chance for that scenario. So what if we remain a non-grabby civilization? What will be our long-term legacies then?
In roughly a billion years, grabby aliens should pass by here, and then soon change this whole area more to their liking. At that point, those grabby aliens will probably have never met any other grabby aliens, and will be very interested in estimating what they might be like, and especially what they might do when the two meet. And one of their main sources of concrete data will be the limited number of non-grabby alien civilizations that they have come across.

Which is all to say that these grabby aliens will be very interested in learning about us, and should be willing to pay substantial costs to do so. So in the unlikely event that our civilization could last the roughly billion years until they get here, those aliens would probably pay substantial costs to protect and preserve us, if that were the cost of learning about us. Of course if they had more advanced tech, they might have other less-fun-for-us ways to achieve that goal.

In the more likely case where we do not last that long, the grabby aliens who arrive here will be looking for any fossils or remnants that they could study. Stuff left here on the surface of the Earth probably won’t survive that long, but stuff left on the surface of geologically dead places like the moon or Mars might well. As could stuff left orbiting between the planets or stars.

Anticipating this outcome, some of us might try to leave data stores about us for them to find. Like we did on the Voyager spacecraft. As our long term legacy. And some of those folks might try to tie their personal revival to such stores. I’m not sure how it could be done, but if you could mix up the info they want with the info that specifies you as an em, maybe you could make it so that the easiest way for them to get the info they want is to revive you.

Of course if a great many people tried this trick, they might bid the “price” down very low. “They want you to revive them for a week to get your info; I only ask one day.” So elites might regulate who is allowed to leave legacy data stores, to keep this privilege to themselves.

Long before grabby aliens got here, they would pass through spacetime events where we’d be active on their past light cone. In fact, sending out a signal from here in most any direction should eventually hit some grabby aliens expanding in our direction. So if we could coordinate with them to send signals out just when they’d be looking at us (such as by sending signals following those from a cosmic explosion), we could tell them about us, and influence them, via such signals.

Some of us might want to try the trick of mixing up their em code with the info aliens want, to force their revival at the receiver end, but the bandwidth to send signals to be received in a 100Myr is rather small. However, as I’ve discussed before, one key function for such signals is that they can prove that they were sent on the date claimed. Later data stores found here are less trustworthy, as they could have been modified in the interim. So perhaps we could send out hash codes to verify datastores saved here now.

We could of course also tell them about any other non-grabby aliens we have discovered. But they’d probably already know about them, assuming they have vastly greater capabilities and tech at least as good as ours.

So is this an exciting legacy to you? A few stories about us that might help some other ambitious civilization calibrate how yet other ambitious civilizations will react upon meeting? No, well then maybe we should work on figuring out how to become grabby ourselves.

<strong>Added 4Nov:</strong> I missed a big potential legacy: Non-grabby aliens could help to mediate between and coordinate grabby aliens. Before two grabby aliens civs meet, they may have both seen and received messages from dozens of the <em>same</em> non-grabby civilizations. Messages sent by those mediators might set expectations and reference points that help the grabby aliens to coordinate. They might even distribute entangled qubits.

Such messages would be more credible if they embodied costly signals. So what could a non-grabby alien civ do, and show they did, to convince grabby aliens re their expectations of what will happen when grabby aliens meet?

## [Why We Can’t See Grabby Aliens](#table-of-contents)
_Posted on 2021-01-08_

In [two](how-far-aggressive-aliens) [posts,](how-far-aggressive-aliens-part-2) I recently explained how a simple 3 parameter model of grabby aliens can explain our apparent early arrival in the universe, via a selection effect: we might give rise to a grabby civ, but that had to happen before other grabby civs took over all the volume.
With some collaborators, I’ve been exploring computer sims of this model, and found one striking statistic: at the origin time of a grabby civ, on average ~40% of universe volume is controlled by grabby aliens. A stat which seems obviously contradicted by what we see, namely nothing. In the volumes we see, they can’t be controlling much, at least if control would make it look much different. What gives?

In this post I want to show how this apparent emptiness can be explained by a parameter choice and a selection effect. First, let’s get oriented. Here is a spacetime diagram showing us now, and all the events that we can see from here, as our red backward light-cone.

[](Screen-Shot-2021-01-06-at-1)
Next, consider the fact that if we extend a yellow cone back in time from where we are at the grabby civ expansion speed, no grabby civ could have had their origin in that excluded volume, because if so then they would have prevented us, to prevent us from becoming grabby.

[](Screen-Shot-2021-01-06-at-1)
Because that’s the definition of grabby: they expand and prevent the origin of other grabby civs within the volumes they control. We could only see grabby civs who have their origin in the green volume, as their expansion would not have reached us yet.

Now if the expansion speed were small, that green area would encompass most of the volume in our past light-cone, and we’d still have a puzzle: why don’t we see them? But as their expansion speed approaches the speed of light, the green volume gets small, making for a low chance of seeing any grabby aliens. (The chance of not seeing one goes as roughly the fraction of their expansion speed to the speed of light.)

Now let’s look at one of those grabby civs we could see:

[](Screen-Shot-2021-01-06-at-1)
Since its origin is in the green volume, its forward expanding cone of control (in orange) intersects our backward light-cone. At the closest intersection point, the spatial extent of that civ is given by the horizontal purple line, which is large compared to its distance away. (Imagine space were 2D, fixing one end of the purple line at the origin axis, and rotating the other end out of the diagram.) So it would be absolutely huge in the sky. This diagram also shows our forward expansion cone intersecting its forward cone relatively soon in the future; we meet them soon.

Now look at the vertical purple line in this next diagram. Holding constant the spatial location of this alien origin, consider the other possible times at which this civ could have originated at that location and still be visible to us.

[](Screen-Shot-2021-01-06-at-1)
The higher is that origin point in the diagram, and the closer is that origin to our red backward light cone, then the smaller is that vertical purple line. And since geometrically the two purple lines must move in proportion, the smaller of an appearance that civ would make in the sky.

As civ origin times should be roughly uniformly distributed over that vertical range, there is thus only a tiny chance of seeing aliens that take up a tiny fraction of our sky. Either we see them huge, or not at all. So there’s little point in building bigger SETI telescopes or deeper surveys to try to see very tiny grabby aliens very far away.

Thus our grabby aliens model can use selection effects to explain not only why we have appeared so early in the history of the universe, but also why we don’t see them even though they should on average take up (and modify) ~40% of universe volume at the moment. At least if we postulate that their expansion speed is a substantial fraction of the speed of light. Which we already had reason to believe, just based on the idea that “grabby” civs try to grab as fast as they can.

<strong>Added 7Mar:</strong> Here is the likelihood ratio for seeing our data of no big alien volumes in the sky, as a function of power n and speed s/c:

[](plotS1-logn)

## [Beware General Visible Prey](#table-of-contents)
_Posted on 2015-04-19_

Charles Stross <a href="http://www.antipope.org/charlie/blog-static/2015/04/on-the-great-filter-existentia.html">recently</a> on possible future great filters:

So IO9 ran a piece by George Dvorsky on ways we could wreck the solar system. And then Anders Sandberg responded in depth on the subject of existential risks, asking what conceivable threats have big enough spatial reach to threaten an interplanetary or star-faring civilization. … The implication of an [future great filter] is that it doesn’t specifically work against life, it works against interplanetary colonization. … much as Kessler syndrome could effectively block all access to low Earth orbit as a side-effect of carelessly launching too much space junk. Here are some example scenarios: …

<strong>Simplistic warfare</strong>: … Today’s boringly old-hat chemical rockets, even in the absence of nuclear warheads, are formidably destructive weapons. … War, or other resource conflicts, within a polity capable of rapid interplanetary or even slow interstellar flight, is a horrible prospect.

<strong>Irreducible complexity</strong>: I take issue with one of Anders’ assumptions, which is that a multi-planet civilization is … not just … distributed, but it will almost by necessity have fairly self-sufficient habitats that could act as seeds for a new civilization if they survive. … I doubt that we could make a self-sufficient habitat that was capable of maintaining its infrastructure and perpetuating and refreshing its human culture with a population any smaller than high-single-digit millions. … Building robust self-sufficient off-world habitats … is vastly more expensive than building an off-world outpost and shipping rations there, as we do with Antarctica. …

<strong>Griefers</strong>: … All it takes is one civilization of alien ass-hat griefers who send out just one Von Neumann Probe programmed to replicate, build N-D lasers, and zap any planet showing signs of technological civilization, and the result is a galaxy sterile of interplanetary civilizations until the end of the stelliferous era. (<a href="http://www.antipope.org/charlie/blog-static/2015/04/on-the-great-filter-existentia.html">more</a>)

These are indeed scenarios of concern. But I find it hard to see how, by themselves, they could add up to a big future filter.<span id="more-31077"></span>

On griefers (aka “berserkers”), a griefer equilibrium seems to me [unstable](berserker-breakout) to their trying sometimes to switch to rapid growth within a sufficiently large volume that they seem to control. Sometimes that will fail, but once it succeeds enough then competing griefers have little chance to stop them. Yes there’s a chance the first civilization to make them didn’t think to encode that strategy, but that seems a pretty small filter factor.
On simple war, I find it hard to see how war has a substantial chance of killing everyone unless the minimum viable civilization size is large. And I agree that this min size gets bigger for humans in space, who are more fragile there. But it should get smaller for smart robots in space, or on Earth, especially if production becomes more local via nano-factories. The chance that the last big bomb used in a war happens to kill off the last viable group of survivors seems to me relatively small.

Of course none of these chances are low enough to justify complacency. We should explore such scenarios, and work to prevent them. But we should work even harder to find more worrisome scenarios.

So let me explain my nightmare scenario: <em>general non-diminishing prey</em>. Consider the classic post-apocalyptic scenario, such as described in <em>[The Road](cannibals-die-fast)</em>. Desperate starving people ignore the need to save and build for the future, and grab any food they can find, including each other. First all the non-human food is gone, then all the people.
Such situations have been modeled formally via “<a href="http://en.wikipedia.org/wiki/Lotka–Volterra_equation">predator-prey dynamics</a>”:

[](Volterra_lotka_dynamics)These are differential equations giving the rates at which counts of predators and prey grow or decline as a function of each other. The standard formulation has a key term whereby prey count falls in proportional to the product of the predator count and the prey count. This formulation embodies an important feature of diminishing returns: the fewer prey are left, the harder it is for predators to find and eat them.
Without enough such diminishing returns, any excess of predators quickly leads to the extinction of prey, followed quickly by the extinction of predators. For example, when starving humans are given easy access to granaries, such granaries are emptied quickly. Not made low; emptied. Which is why granaries in famines are usually either well-protected, or empty.

In nature, there are usually many kinds of predators, and even more kinds of prey. So the real predator-prey dynamic is high-dimensional. The pairwise relations between most predators and preys do in fact usually involve strongly diminishing returns, both because predators must usually search for prey, and because some prey hiding places are much better than others.

If the relation between any one pair of predator and prey types happens to have no diminishing returns, then that particular type of prey will go extinct whenever there is a big enough excess of that particular type of predator. Since this selects against such prey, the prey we see in nature almost all have diminishing returns for all their practical predators.

Humans are general predators, able to eat a great many kinds of prey. And within human societies humans are also relatively general kinds of prey, since we mostly all use the same kinds of resources. So when humans prey on humans, the human prey can more easily go extinct.

For foragers, a key limit on human predation was simple distance. Foragers lived far apart, and were unpredictably located. Also, foragers had little physical property to grab, wives were not treated as property, and land was too plentiful to be worth grabbing. These limits mattered less for farmers, who did predate often via war.

The usual source of diminishing returns in farmer war predation has been the wide range of protection in places to hide; humans have often run to the mountains, jungle, or sea to escape human predators. Even so, humans and proto-humans have quite often driven one another to local relative extinction.

While the extinction of some kinds of humans relative to others has been common, the extinction of all humans in an area has been much less common. This is in part because, when there has been a local excess of humans, most have focused on non-human prey. Such prey are diverse, and most have strongly diminishing returns to human predation.

Even if humans expand into the solar system, and even if they create robot descendants, we expect our descendants to remain relatively general predators, at least for a long while. We also expect the physical resources that they collect to constitute relatively general prey, useful to a wide range of our descendants. Furthermore, we expect nature that isn’t domesticated or descended from humans to hold a decreasing quantity of useful resources.

Thus the future predator-prey dynamic should become lower dimensional than it has been in the past. To a perhaps useful approximation, there’d be only a few kinds of predators and prey. Which raises the key question: how strong are the diminishing returns to predation in that new world? That is, when some of our descendants hunt down others to grab resources, how fast does that task get harder as fewer prey remain?

One source of diminishing returns in predation is a diversity of approaches and interfaces. The more different are the methods that prey use to create and store value, the smaller the fraction of that value a predator can obtain via a simple hostile takeover. This increases the ratio of how hard prey and prey fight. As many have noted, in nature prey fight for their lives, while predators fight only for a meal. Even so, nature still has plenty of predation. Even if predators gain only part of the value contained in prey, they still predate if that costs them even less than this value.

As I said above, the main source of diminishing returns in predation among foragers was travel cost, and among farmers it was the diversity of physical places to run and hide. Such effects might still protect our descendants from predator-prey-dynamic extinction, even if they have only one kind of predator and prey. Alas, we have good reasons to fear that these factors may less protect our descendants.

The basic problem here is our improving techs for travel, communication, and surveillance. We are steadily able to move bits and people more cheaply, and to more cheaply and accurately watch spaces for activity. Yes moving out into the solar system would put more distance between things, and make them harder to see. But that one-time effect will be quickly overwhelmed by improving tech.

A colonized solar system is plausibly a place where predators can see most any civilized activities of any substantial magnitude, and get to them easily if not quickly. So if we ever reach a point where predators fight to grab civilized resources with little concern to save some for the future, they might be able to find and grab pretty much everything in the solar system. Much as easy-access granaries are quickly emptied in a famine.

Whether extinction results from such a scenario depends how small are minimum viable civilization seeds, how obscure and well protected are the nooks and crannies in which they might hide, and how many of them exist and try to hide. Yes, hidden viable seeds drifting at near light-speed to other stars could prevent extinction, but such a prey-collapse scenario could play out well before such seeds are feasible.

So, bottom line, the future great filter scenario that most concerns me is one where our solar-system-bound descendants have killed most of nature, can’t yet colonize other stars, are general predators and prey of each other, and have fallen into a short-term-predatory-focus equilibrium where predators can easily see and travel to most all prey. Yes there are about a [hundred billion comets](all-hail-william-napier) way out there circling the sun, but even that seems a small enough number for predators to careful map and track all of them.
Worry about prey-extinction scenarios like this is a reason I’ve focused on hidden refuges as protection from existential risk. Nick Beckstead has argued against refuges <a href="http://www.effective-altruism.com/ea/5r/improving_disaster_shelters_to_increase_the/">saying</a>:

The most likely ways in which improved refuges could help humanity recover from a global catastrophe are scenarios in which well-stocked refuges with appropriately trained people help civilization to recover after a catastrophe that leaves a substantial portion of humanity alive but disrupts industrial and agricultural infrastructure, and scenarios in which only people in constantly-staffed refuges survive a pandemic purposely engineered to cause human extinction. I would guess that, in the former case, resources and people stocked in refuges would play a relatively small role in helping humanity to recover because they would represent a small share of relevant people and resources. The latter case strikes me as relatively far-fetched and I would guess it would be very challenging to do much better than the largely uncontacted peoples in terms of ensuring the survival of the species. (<a href="http://www.effective-altruism.com/ea/5r/improving_disaster_shelters_to_increase_the/">more</a>)

Nick does at one point seem to point to the scenario that concerns me:

If a refuge is sufficiently isolated and/or secret, it would be easier to ensure that everyone in the refuge had an adequate food supply, even if that meant an inegalitarian food distribution.

But he doesn’t appear to think this relevant for his conclusions. In contrast, I fear that a predatory-collapse scenario is the most likely future great filter, where unequal survival key to preventing extinction.

<strong>Added 10a:</strong> Of course the concern isn’t just that some parties would have short term orientations, but that most would pursue short-term predation so vigorously that they force most everyone to put in similar effort levels, even if they take have long-term view. When enemies mass on the border, one might have to turn farmers into soldiers to resist them, even if it is harvest time.

## [If The Future Is Big](#table-of-contents)
_Posted on 2018-08-21_

One way to predict the future is to find patterns in the past, and extend them into the future. And across the very long term history of everything, the one most robust pattern I see is: <em>growth</em>. Biology, and then humanity, has consistently grown in ability, capacity, and influence. Yes, there have been rare periods of widespread decline, but overall in the long run there has been far more growth than decline.<span class="Apple-converted-space"> </span>

We have good reasons to expect growth. Most growth is due to innovation, and once learned many innovations are hard to unlearn. Yes there have been some big widespread declines in history, such as the medieval Black Death and the decline of the Roman and Chinese empires at about the same time. But the historians who study the biggest such declines see them as surprisingly large, not surprisingly small. Knowing the details of those events, they would have been quite surprised to see such declines be ten times larger than as seen. Yes it is possible in principle that we’ve been lucky and most planets or species that start out like ours went totally extinct. But if smaller declines are more common than bigger ones, the lack of big but not total declines in our history suggests that the chances of extinction level declines was low.<span class="Apple-converted-space"> </span>

Yes, we should worry about the possibility of a big future decline soon. Perhaps due to global warming, resource exhaustion, [falling fertility](fertility-the-big-problem), or [institutional rot](more-than-death-fear-decay). But this is mainly because the consequences would be so dire, not because such declines are likely. Even declines comparable in magnitude to the largest seen in history do not seem to me remotely sufficient to prevent the revival of long term growth afterward, as they do not prevent continued innovation. Thus while long-term growth is far from inevitable, it seems the most likely scenario to consider.
If growth is our most robust expectation for the future, what does that growth suggest or imply? The rest of this post summarizes many such plausible implications. There far more of them than many realize.<span class="Apple-converted-space"> </span>

Before I list the implications, consider an analogy. Imagine that you lived in a small mountain village, but that a huge city lie down in the valley below. While it might be hard to see or travel to that city, the existence of that city might still change your mountain village life in many important<span class="Apple-converted-space">  </span>ways. A big future can be like that big city to the village that is our current world. Now for those implications: <span class="Apple-converted-space"> </span><span id="more-31849"></span>

<strong>The Great Filter</strong> – If our descendants continue to grow, some of them should eventually occupy and rearrange much larger volumes of space. It is unlikely that the best way to rearrange that space will look from a distance just like the dead matter that was there before our descendants arrived. So eventually the large volumes we change should look visibly different from far away. Yet when we look out now into the universe seeking aliens who have visibly changed the universe near them, everything we see looks dead. 

This suggests that a <a href="http://mason.gmu.edu/~rhanson/greatfilter.html">great filter</a> lies along the evolutionary paths between simple dead matter and an expanding visible civilization, and raises the key question: how far along this filter are we? Evidence of alien life having reached further along such a path would be bad news, suggesting that our past filter is easier than we thought, and thus our future filter is harder. We [have](two-types-of-future-filters) [other](hope-for-a-lumpy-filter) [clues](at-least-two-filters) to where we are in this filter.<span class="Apple-converted-space"> </span>

<strong>Cryonics</strong> – Today when people’s bodies fail them and current medical science fails them as well, we usually let their bodies decay into nothing. For example, we burn them, or let worms eat them. But it is possible to instead freeze those bodies in liquid nitrogen. The freezing process does some damage, as do antifreeze chemicals often used to limit freezing damage. But once frozen in liquid nitrogen, bodies should stay almost exactly the same for many centuries.<span class="Apple-converted-space"> </span>

And so if our descendants grow in technical capability, eventually they may be able to repair both the freezing damage and whatever went wrong with those bodies before freezing. And if brain emulations are possible, at an earlier date it should be able to create brain <a href="http://ageofem.com">emulations</a> from only mildly damaged frozen brains. Thus people today who “die” might be revived in the future, if their brains can be frozen and stored for long enough. You [might](break-cryonics-down) [want](cryonics-as-charity) [to try to](revival-prizes) be one of those people.<span class="Apple-converted-space"> </span>
<strong>Simulation Argument</strong> – We today are often interested in the past. Some of us study it formally, as historians, while others explore history via fiction and games. Some of these ways we explore history can be seen of as “simulations”, intended to mimic some details of a historical period via playing it out step by step.<span class="Apple-converted-space"> </span>This sort of thing has been going on for thousands of years, and we expect that our descendants to continue this tradition.

Our much more capable descendants should be able to create much more detailed simulations. Such simulations could include individual people simulated to such a detail that they are real living people who don’t know that they are in a simulation; they believe that they are actually living in the historical period being simulated. For a historical event of great interest to our descendants, there might be a great many simulations, containing more simulated people than existed during that actual historical event. Which raises the provocative question: what is the chance that each of us is [actually](reversible-simulations) [living](im-a-sim-or-you-arent) in [such](am-i-a-sim) a future simulation? <span class="Apple-converted-space"> </span>

<strong>Biology Replaced</strong> – Today the capabilities of our industrial economy are powerful and important, but are overshadowed in many ways by the capacities of biology. Biology has designed and can produce vast complex systems than we today only crudely understand. But our industrial abilities are increasing far faster than those of biology, so eventually industry should displace or assimilate biology. Future industry may incorporate particular biological chemicals, reactions, structures, and even systems, but within a more industry-style system. That is, things will be more designed by engineers, tested in labs, made in factories, and then controlled as centrally as is found useful. They’d be made out of materials from organized mines and trash recyclers, fed energy from large-scale energy systems, move around via large-scale transport systems, and communicate via large-scale communication systems.<span class="Apple-converted-space"> </span>

If so, the familiar self-reproducing self-feeding self-moving autonomous biological organisms that have dominated Earth for billions of years have a limited future. Some of this might be saved in nature/history preserves/museums, but the distant future will be filled with stuff more integrated into an advanced industrial economy. This includes the descendants of humans, also made more in structured factories than in autonomous wombs. In this sense our descendants, and any advanced aliens we meet, should be “robots.” <span class="Apple-converted-space"> </span>

<strong>Who To Influence</strong> – You may not be very interested in influencing empty space or even empty deserts, because you mostly care to influence and help people. If so, then note that you should expect to find the vast majority of people, or at least people-like creatures, in the distant future. So as long as the future contains at least moderate variety, then according to a wide range of criteria of which people you’d most like to influence, most of those people will be found in the distant future. (Helping is a kind of influence.)<span class="Apple-converted-space"> </span>

Yes, if your interest in people declines with cultural distance, and if future people tend to be more culturally distant, then on average future people are less interesting to you. But you don’t have to influence average people, you can select who to influence, and the future will have a lot more people. So most of those that you most want to influence may still be in the distant future. Even if future folks are richer on average (which <a href="http://ageofem.com">I doubt</a>), there are likely to be plenty of suffering folks there as well to help. <span class="Apple-converted-space"> </span>

In addition, for thousands of years the average rates of return on investments have been consistently higher than economic growth rates. Given this, if you save and invest resources to be spent to influence people later, your influence should <em>rise</em> as a fraction of the world economy! Yes investments are often stolen, and you suffer losses when trying to control future agents who are supposed to execute your instructions about who to help. Even so, after these losses it may still be cheaper to influence future folks than folks today. The future will be big and by you might even be able to influence a larger fraction of that future world than today’s world. This makes the future an obvious place to consider when looking to influence others. <span class="Apple-converted-space"> </span>

<strong>Where to Migrate</strong> – If you would consider migrating from where you are now to some new place, a big future means that most interesting places to move may be out there in that future. Yes, you might be picky about the kind of place you want to go, and yes all else equal future place might be more cultural distant and hence less attractive. But being bigger the distant future may still contain most of the places that best fit any given migration criteria.<span class="Apple-converted-space"> </span>

To get to the future you either need to live a long time, to use some sort of suspended animation such may be found in cryonics, or to have change accelerate so that the future gets bigger faster during your lifetime. All of these seem like viable options, and they can be combined. In addition, using rates of return on investment you can bring more resources with you into the future than you left with from our time. And for a big, rich and capable future, many things you want will be cheaper to buy in that future than they are here today. Furthermore, one of those things is the ability t0 migrate to still even more distant futures.

<strong>Who To Impress</strong> – We like to pretend that we don’t care what other people think, that we just do things to please ourselves. But this isn’t <a href="http://elephantinthebrain.com">remotely true</a>, and we can’t just will it to be true. But we do have some flexibility in <em>who</em> exactly we want to impress. And when choosing which people to try to impress, most of our available candidates will be in the distant future. Thus most of the potential audiences who meet any particular audience criteria will also be in the distant future.

In addition, not only will future people be wiser and more knowledgable in many ways, and thus better judges to impress, it should be cheaper to get future folks to pay attention to evaluating us. High rates of return on investments means that a small sum today might be enough to pay several future people to spend a lifetime carefully considering our accomplishments and contributions. Yes, one needs to make sure to save the right sort of data to allow future folks to make such judgements. But it seems possible to save a lot of data cheaply, and many scale and scope economies seem available to groups seeking to share related data together. <span class="Apple-converted-space"> </span>

<strong>Incentives For Honesty</strong> – Today we often want to rely on the honesty and forthrightness of experts and advisors, but fear that those experts and advisors often have incentives to mislead us. One simple way to encourage honesty is to have experts make bets supporting their claims. Or, similarly, have them post bonds that they lose if proven wrong. Such approaches, however, require that we can sometimes settle such bets. There must at least be a substantial chance that eventually enough people will know the answer. In this context, we can be encouraged by the fact that the distant future should be wise and knowledgable on many topics. So we can try to have the distant future settle bets. <span class="Apple-converted-space"> </span>

For a fanciful example, imagine that we save lots of data about today’s academics, including not just their publications, but also their tweets, emails, and much more. We commit to paying distant future folks to use that data to do very detailed analyses of a random one percent of today’s academics, carefully judging their overall intellectual contribution to things that turned out to matter. We then set up assets today that pay in proportion to those distant evaluations, and use the current prices of these assets to rank people for key decisions today like jobs and grants. This sort of system could cut incentives for citation grubbing, mutual-admiration societies, and other games people play today to create the illusion of a consensus on who is good. All by making stronger use of the fact that the future can know more, and can learn more cheaply, including about people today.

Here’s another fanciful example. Today if I sue you today for $10,000, we soon have a court decide if you have to pay me or not. You and I each pay big amounts for lawyers and preparation, and the court also pays to make a decision. A decision that could be wrong. Imagine instead that we saved lots of data about this lawsuit and put off the decision, committing to have distant future folks decide, when the world is wiser and analysis is cheaper. In the meantime you pay $10,000 immediately and in return get the asset “$10,000 if found innocent”, while I get the asset “$10,000 if found guilty.” We can trade these assets to others if we like, and anyone who has both kinds of these assets can merge them back into simple money.<span class="Apple-converted-space"> We might treat these assets as if they were closely tied to our true innocence or guilt.  </span>

<strong>In sum</strong>, we expect continued growth to create a big distant future, a future that can influence life today just as a big city in the valley might influence life in nearby mountain village. A big future suggests that we worry about being smashed by a great filter. Big future tech abilities suggest that we could survive current death via cryonics, that we might be today actually be part of a future simulation of history, and that biology as we know it won’t last. A big future is an attractive place to migrate, to influence, and to impress. And a wise knowledgeable future offers many ways to improve current incentives for honesty. Like a big city in the valley below, a big future can matter to us now.

## [UFOs – What The Hell?](#table-of-contents)
_Posted on 2021-06-24_

<em>(This post is more of an essay, intended to be especially widely accessible.)</em>

Long ago, my physics teachers taught me to arrogantly dismiss the “paranormal”, like ghosts or telepathy. Or UFOs. Yes, we once saw meteorites as paranormal, but not today. Yes, we now accept ball lightning, even though evidence for it is weaker than for UFOs, but we have plausible theories there.

However, when the topic of distant “aliens” came up recently in my research on the origin of life and the future of the universe, I browsed UFO evidence and found it to be much stronger than stuff on ghosts or telepathy. And now a U.S. military report says that intelligently controlled UFOs with amazing abilities seem real to them, even if they don’t know their cause.

Hence my and perhaps your titular reaction, “What the Hell?” How can this make any sense?

Turns out, my prior research prepared me to address this very question, once I gave it some thought. Not on the evidence for UFOs, where others are more expert than I. But on how to fit this idea of strange objects with amazing abilities under intelligent control into your scientific world view. (Note that I’m not claiming this as fact; I’m saying it isn’t crazy.)

While there are many possibilities here, it suffices for me to show just one. And yes, it involves aliens.

We can easily believe that aliens are very advanced, and thus have amazing abilities. But two questions remain:

<ul>
<li>In a vast universe that looks dead everywhere, how is it that advanced aliens happen to be right here right now?</li>
<li>Even if aliens did travel to be here now, why would they act as UFOs do: mute and elusive, yet still noticeable?</li>
</ul>
First, note that our standard best scientific theories <em>predict</em> aliens. That is, they predict that life sometimes arises from simple dead matter, and can eventually evolve to make intelligent creatures like us. And this could happen most anywhere.

Yes, the universe looks completely dead; we see no signs of life outside Earth, even though over millions of years advanced aliens could have made some <em>big</em> visible changes. Some possible explanations:

<ol>
<li>Aliens arise so rarely that the nearest ones are too far to see, or to have travelled to here,</li>
<li>Aliens are common but simply can’t travel between stars or make big visible changes,</li>
<li>Aliens are common and travel everywhere, but enforce rules against visible changes, or</li>
<li>Aliens arise rarely, but in small clumps; the first in clump to appear can control the others.</li>
</ol>
Of these, only the last two can put aliens here now, and #3 seems too much a conspiracy (i.e., coordinate to hide) theory for my tastes. But scenario #4 [works](on-ufo-as-aliens-priors), and could plausibly result from “[panspermia](panspermia-siblings).”
That is, simple life might have arisen on a planet Eden long ago, via a very rare event. (My research <a href="http://grabbyaliens.com">suggests</a> this happens only once per million galaxies.) After life evolved at Eden for billions of years, a rock hit Eden, kicking up another rock that drifted for millions of years carrying life to seed our Sun’s stellar nursery. A nursery that held thousands of new stars packed close with many rocks flying around, allowing life to spread quickly to them all.

Our sun’s siblings then drifted apart, while life evolved on each planet for billions of years. The first sibling planet to develop civilization did so millions of years ago, and it wasn’t Earth. These aliens then sought out their sibling stars and traveled to them to watch civilization maybe evolve there.

Now, to explain the fact that these aliens have not visibly changed our shared galaxy, even though they can travel to here, we must postulate that they enforce a rule against making big visible changes, probably enforced by a strong central government. A rule against mass aggressive expansion, colonization, and disassembling of planets, stars, etc. Maybe due to environmentalist values, maybe to [enable](the-coming-cosmic-control-conflict) regulation, or maybe just to protect central control and status. Yes, this is something of a conspiracy theory, but being smaller, it seems easier to swallow.
Okay, that is a not-crazy answer to the first question, on why aliens are here now in a dead universe. What about the second question, on why UFOs act so weird and coy?

To answer this, I postulate two features of sibling alien preferences: 1) they want us get us to comply with their rule against making big visible changes to the universe, and 2) they are reluctant to just kill, crush, enslave, or dominate us to get this outcome (or they’d have already done one of these). Aliens somehow value something about us independent of their influence, and thus prefer us to organically and voluntary comply with their rule.

To induce our voluntary min-change compliance, their plan is put themselves gently at the top of our status ladder. After all, social animals consistently have status ladders, with low status animals tending to emulate the higher. So if these aliens hang out close to us for a long time, show us their very impressive abilities, but don’t act overtly hostile, then we may well come to see them as very high status members of our tribe. (<em>Not</em> powerful hostile outsiders.)

If we are smart enough to figure out that they have a rule against big visible changes, enforced via a “world” government, we may naturally emulate those policies. We may even come to treat UFO aliens as ancient humans treated their [gods](would-ufo-aliens-be-our-gods) and top leaders, with respect, deference, and obedience. After all, most ancient people knew little about their gods and leaders beyond their impressive wealth and abilities, but that was usually enough.
But why not just land on the White House lawn, meet with our leaders, and explain their agenda? Because once they start talking to us, we will have a <em>lot</em> of questions. Such as on their nature, practices, history, and future plans. And many of us would surely <em>hate</em> some of their answers. They are complete <em>aliens</em> after all, and we are often offended by humans from slightly different subcultures. They reasonably guess that we are just not as open-minded as we like to think.

Sure, maybe if they understood us really well they could just say “no comment” when a discussion got near something likely to offend us. But we’d then reasonably infer that they were hiding bad news near there, make a guess at what it is, and get somewhat offended at that. Far simpler and more robust to not talk at all, except in dire emergencies.

That’s a plan that [could](explaining-stylized-ufo-facts) be approved long in advance by a far away central power wary of allowing much improvisation and discretion by their local representatives. Especially if their very old and stable centralized civilization has atrophied and lost much of its prior generality and flexibility. Or if they worry that such representatives may “go native” and be persuaded by us to go grabby.
This strategy works best if they carefully limit what they show us. Just give us brief simple impressive glimpses that don’t let us figure out their tech, or even the locations of their local bases. The package of simple geometric shapes, crazy accelerations, no sounds or other local side effects, clear intelligent intent, and avoiding harms to us seems to do the trick.

And that’s my story. How UFOs as aliens can make sense. Not an inspiring story, but a plausible one. They could be our panspermia siblings, here to get us to voluntarily obey their rule against aggressive expansion, hopefully via our emulating them because they sit at the top of our status ladder. Not to say that this is obviously true, just to say that it isn’t crazy. Yes, there are other possible scenarios. But UFOs as aliens, that’s no crazier than ball lighting.

<strong>Added 4July:</strong> The many comments on this post are mostly the result of this mention at <a href="https://pjmedia.com/instapundit/459703/">Instapundit</a>.

## [Our Alien Stalkers](#table-of-contents)
_Posted on 2023-05-06_

> Sam Quirk was ten years old. While on a field trip, his bus had paused at a rest stop, and Sam was sitting in a bathroom stall. From the next stall over, he heard clearly but quietly, “Sam Quirk, ask your parents about ‘royal propriety’”. By the time he could check the next stall, it was empty. 
> 
> Back at home, Sam asked his parents. They looked grave, and told him that something similar had happened to them roughly every ten years for their entire lives. Except that after the first time, all they heard was just the phrase “royal propriety”. Now they told Sam what their parents had told them: “We are royalty.”
> 
> Sam’s dad was descended from a rich and powerful line of royalty that was famous for its being very secretive. Hardly anything was known about them, including where any of them lived. The lawyer who represented them issued rare press releases, which said little. 
> 
> Long ago, a representative of this royal org had contacted Sam’s great granddad, and explained to him how very important it was that no one from their line ever appear in the public eye. To demonstrate their determination, they promised to give these personal once-a-decade “royal propriety” reminders. The message: they are still around, and still care.
> 
> They also hinted at dire consequences for any violations of this rule. So far their family had always complied. And as far as anyone in the family knows, this royal org has never helped them in any way, nor suggested that it ever would.
> 
> Should Sam feel lucky to be part of such a rich illustrious royal family? Or unlucky to have such a powerful and hostile family stalker? 

I offer this story as an allegory of my best guess of humanity’s situation if some UFOs are in fact aliens. Remember that I don’t claim that they are. Only that I find it [hard](https://www.overcomingbias.com/p/my-awkward-situation) to see honest mistakes as explaining our strongest most dramatic UFO reports, that due to my [grabby aliens](http://grabbyaliens.com/) work I am something of an expert on the _[prior](https://www.overcomingbias.com/p/on-ufo-as-aliens-priorshtml?utm_source=%2Fsearch%2Fprior%2520UFO&utm_medium=reader2)_ for this some-UFOs-are-aliens hypothesis, and also on its social implications, and I thus feel obligated to resist the [usual](https://www.overcomingbias.com/p/skirting-ufo-tabooshtml) [taboos](https://www.overcomingbias.com/p/when-the-tabooed-taboohtml) to give my best estimate on both these topics.

Many suggest that UFOs-as-aliens would put us in a position of radical uncertainty, wherein we’d have almost no idea who are these aliens, how many others are out there, or what any of them want. In contrast, I [think](https://www.overcomingbias.com/p/ufos-what-the-hellhtml) we can actually say quite a lot. Alas, it is not a pretty picture.

The most likely scenario that I can find consistent with some UFOs being aliens starts with life appearing ~9Gya on a one-in-a-million-galaxies-rare planet _Eden_ somewhere in our galaxy. Then ~5Gya life was transferred via panspermia to many of the ~1000 newborn stars in our Sun’s stellar nursery. Life continued to evolve on those stars, until >~0.1Gya one of them gave rise to an advanced alien civilization.

Civilizations that allow interstellar colonization probably cannot maintain civ-wide governance to regulate, prevent war, and prevent their descendants from evolving into strangeness. For this or other reasons, this particular alien civilization chose to prevent any part of itself from leaving its home system to colonize the universe. However, it made rare exceptions for expeditions to stellar siblings that could be seen in telescopes as hosting life, and thus at risk of birthing another advanced civilization. The main motive was to prevent such a “panspermia sibling” civilization from violating their rule against expansion. But they’d rather achieve this via persuasion, rather than extermination.

As each expedition risked violating their rule by going rogue, home authorities wanted a simple robust strategy; they didn’t trust expeditions to exercise much discretion. While alien visitors to Earth could have remained completely invisible, or become very obvious, they instead chose a third way. Their strategy was this: hang out on Earth at the periphery of our vision, act peacefully, show very impressive abilities, but reveal little else about themselves.

Why? Social animals consistently have status hierarchies, and we humans have consistently domesticated other animals (and ourselves) by putting ourselves at the top of their status hierarchies. So the aliens hoped to do this with us. However, they also guessed that if they revealed too many details about themselves, we’d likely find something to hate, spoiling this status effect.

Thus the plan: if we didn’t come to hate them, then once we became convinced that they really exist, we could figure out their agenda by ourselves without their saying a word. And then we had a decent chance of going along with it; after all, most today who believe that some UFOs are aliens also seem to trust aliens more than their local authorities. If we didn’t go along, they would at some point have to intervene.

So this is our fate if some UFOs are aliens. We will either go along with their no-expansion rule, or become more directly controlled, or exterminated. In the coming centuries they will probably tell us no more about themselves, nor help us in any other way, and we will not figure out much on our own. Not even the location of their local base. Maybe we will identify their home world via telescope. And maybe they have hidden messages in the details of their frequent displays, but probably not.

We should be quite impressed by the fact that these aliens not only managed to keep their civilization going for over 100My, but they’ve also managed to enforce their no-expansion rule for that whole time. After all, it would have taken just one tiny successful rebel slipping away unseen to end it. But we should also wonder how much their abilities may have decayed and rotted over that long period. With enough rot, we might just have a long-shot chance of developing [ems](http://ageofem.com/) or AIs and then slipping them off to colonize the universe unnoticed. But probably not.

Which seems to me much like Sam’s situation. So, do you feel lucky punk?

## [On UFOs-As-Aliens Priors](#table-of-contents)
_Posted on 2021-06-08_

A careful analysis of UFOs should consider <em>lots</em> of data, and consider it in much detail. I oft hear skeptics seek shortcuts, such as by declaring all testimony invalid, or insisting that only some long conjunction of encounter features could be sufficient. But consider a legal accusation of attempted murder. Even though the prior odds that a random  X attempts to kill Y during hour Z is terribly low (<10<sup>-12</sup>), we are still willing to entertain such claims, and we accept personal testimony as an important part of supporting evidence.

Yes, advocates of things like UFOs seem willing to put more time into such details, and it may seem unfair to expect skeptics to put in as much work. But jurors and lawyers must put in a lot of effort in legal trials. This is the great problem of how to divide intellectual labor; as with most topics, we do best if we task a few with going into great detail on each topic, so the rest of us can defer to their analysis. If you aren’t willing to go into sufficient detail, then admit this isn’t one of your topics, and defer to others on it.

In that spirit, instead of expressing opinions on many UFO topics, let me instead focus on the area where I have the most relative expertise: the priors to associate with the some-UFOs-are-aliens hypothesis. As far as I can tell, the main reason that most give for skepticism that aliens visit Earth in the UFO style is that this theory seems a priori crazy unlikely. But that estimate seems wrong to me. Let me explain.

A full Bayesian analysis of the four main UFO theory categories (error, hoax, secret Earth orgs, aliens) needs eight numbers: one prior and likelihood for each theory. In this post I try only to estimate one of these eight numbers: the prior for the aliens theory. Here goes.

Life exists here on Earth, and our standard best theories say that this was not a miracle, nor was Earth the only place such things could happen. Furthermore, our universe also seems very large (perhaps infinite). Thus our standard best theories predict that advanced life has appeared and will appear many times out there.

These standard best theories also predict a wide range of dates when this could happen. As a result, two independent alien origins are likely to be millions to trillions of years apart in time. Which gives aliens a lot of time to travel to visit other aliens.

So we can break down doubts on prior expectations about UFO as aliens into three parts:

<ul>
<li>What is the chance that advanced aliens appear often enough in space and time for some of them to have been born early and close enough to travel to Earth to be here now?</li>
<li>What is the chance that aliens (or, more likely their robot descendants) who <em>can</em> travel actually <em>do</em> travel to Earth by now, but do <em>not</em> visibly remake the local universe?</li>
<li>Given that aliens exist, and travel to here, but don’t remake the local universe, what is the chance that they would act the way that UFOs seem to act, i.e., being somewhat evasive, but not completely hiding nor announcing themselves?</li>
</ul>
First, how close might aliens be? As my co-authors and I discuss <a href="http://grabbyaliens.com">here</a>, humans seem to have arrived quite early in history, at least if one assumes that the universe would remain empty and wait indefinitely for advanced life like us to appear. This is the main reason we offer for postulating a grabby aliens deadline, to explain human earliness. And our grabby aliens model implies that aliens do appear often enough for maybe some of them to have come here by now.

Now grabby aliens arriving by here now would also be quite visible to us now. But our basic model is quite consistent with variations wherein there are many, perhaps thousands or millions, of non-grabby alien civs per grabby civ, all born at the same sort of places and times. These non-grabby civs do <em>not</em> remake their local universe. So either they die fast, life long but do not expand, or they expand long but do little to remake their universe.

In my view, the most likely scenario that puts long-expanding-but-not-remaking aliens here now is [panspermia siblings](panspermia-siblings). Life arose long ago on some very rare [Eden](searching-for-eden), which then seeded our Sun’s stellar nursery, with life quickly spreading to most stars in that nursery. At least two of these stars eventually developed advanced life, but Earth was not the first. Aliens at the first star looked for their panspermia siblings, noticed simple life on Earth here long ago, and then long ago traveled to near here to await the arrival of advanced life. Where they now do their weird UFO encounter things.
So to explain UFOs as aliens, we must postulate that these first star sibling aliens had preferences and coordination abilities sufficient to do the following:

<ul>
<li>prevent any parts of their own civ from expanding and visibly remaking the local universe,</li>
<li>travel to sibling stars that might birth civs, to stand ready to prevent them from also expanding, but also not kill them, and</li>
<li>while waiting here they allow or induce the sort of UFO encounters we see, but prevent any clearer more direct interactions.</li>
</ul>
I estimate a chance of at least 10% for each of the following events, given the prior events:

<ol>
<li>Earth was seeded by panspermia in its nursery</li>
<li>A sibling star gave rise to a long-lived advanced civ long before now</li>
<li>That civ prevents itself from expanding, tries to prevent siblings from expanding, and long ago traveled to here to wait to enforce this preference,</li>
<li>They induce or allow UFO-style encounters while they wait here.</li>
</ol>
Note that #1 requires a high enough rate of rock transfer between star systems, #2 requires that most of the great filter happened on Eden, #3 is more likely when civs adopt strong “world” governments, and #4 is relatively likely because we shouldn’t really expect to be able to predict detailed behaviors of strange alien civilizations.

Four factors of 10% gives a minimum prior chance of 10<sup>-4</sup>, but as most of the probability weight should above these minimums, I estimate the total chance to be at least 10<sup>-3</sup>. As I’ve said [before](ufos-say-govt-competence-is-either-surprisingly-high-or-surprisingly-low), combining all the relevant priors and posteriors I judge the hoax and aliens theories to be most likely for the hardest-to-explain UFO cases. But I don’t claim as much expertise on all the other numbers required to judge that, as I do for the one number I estimate here:
> The prior chance of the aliens theory of the hardest-to-explain UFO cases is at least 10<sup>-3</sup>, relative to the other three theory categories of error, hoax, and secret Earth orgs.

This prior is actually pretty high compared to the usual priors in most legal cases. So the types and amounts of evidence on particular cases that is sufficient to convict in legal cases seems sufficient to judge UFOs-are-aliens as more or less likely than not. But again, I have no special expertise to offer you for judging the details of UFO encounters. I can just say that you need to look at such details; you can’t just dismiss UFOs-as-aliens theory with a wave of your philosophical hands.

<strong>Added 10June:</strong> Many take issue with my estimating 1/10 for the chance that aliens waiting here would be somewhat evasive, but not completely hide nor announce themselves. They don’t see this as a good plan for any goals they can think of.

But we are talking about an entire alien civilization here! Human societies often do things, like fight wars or stop having kids, that seem counter-productive from the point of view of that society as a whole. In addition, individual humans often do things that seem counter-productive until you consider their signaling incentives. I wrote a whole <a href="http://elephantinthebrain.com">book</a> on this.

If we often have trouble explaining the behaviors of human societies and individuals, I don’t think we should feel very confident in predicting detailed behaviors of a completely alien civilization. After all, many have reasonably doubted if we could even communicate with aliens, or recognize them when we saw them. Having [outlined](explaining-stylized-ufo-facts) [some](ufos-and-status) possible signaling motives for alien UFO behavior, I can see that there are many possible explanations for aliens-as-UFO behavior. Thus a 1/10 prior seems reasonable to me.
<strong>Added 13Jun</strong>: I did 6 Twitter polls to elicit relative priors and likelihoods for the four main theory categories:


Medians of lognormal fits to my recent polls gives these Bayesian estimates re 4 different UFO theories. These numbers seem crazy wrong to me. <a href="https://t.co/0jY93DJy2m">https://t.co/0jY93DJy2m</a> <a href="https://t.co/B7itXtHDyd">pic.twitter.com/B7itXtHDyd</a>

> — Robin Hanson (@robinhanson) <a href="https://twitter.com/robinhanson/status/1402784956439425024?ref_src=twsrc%5Etfw">June 10, 2021</a>

<script async="" charset="utf-8" src="https://platform.twitter.com/widgets.js"></script>

<strong>Added 14Jun:</strong> Thinking through the consequences of the show-but-don’t-talk strategy [suggests](would-ufo-aliens-be-our-gods) that it will work out pretty well for the aliens.

## [UFOs Show Govt. Competence As Either Surprisingly High Or Low](#table-of-contents)
_Posted on 2021-05-05_

Sometimes I pride myself on my taking an intellectual strategy of tackling neglected important questions. However, one indicator of a topic being neglected is that it seems low status; people who discuss it are ridiculed, and their intellectual ability doubted. Thus my strategy risks lowering my status.

To protect against this risk, I can set a policy of only tackling topics that seem to have a substantial synergy with my skills and prior topics. Which seems a valid policy, even if not entirely honest. For a long time this protected me against UFOs as aliens, one of the most ridiculed topics ever. But then I started to study <a href="http://grabbyaliens.com">loud</a> very distant aliens, and the topic of alien UFOs became more relevant.

To limit the damage, I once [tried](explaining-stylized-ufo-facts) to talk only on what UFOs would imply <em>if</em> they really were aliens, but not crossing the line to discuss if they actually are. But on reflection I can see that this topic is in fact neglected, important, and has synergies with my skills and other topics. So now I am shamed into trying to live up to my intellectual ideals, which if truth be told aren’t as strongly rooted in me as I’d like to pretend. Sigh. So here goes, let’s talk about explaining UFO encounters.
I see four major categories of explanation:

<ul>
<li>(A) <strong>Honest mistakes</strong>: This includes misunderstandings of familiar phenomena, delusions and mental illness, and natural phenomena that we now poorly understand.</li>
<li>(B) <strong>New Govt. Tech</strong>: Some current Earth government is testing new tech far more advanced than anything publicly admitted. Or is using it for limited secret purposes.</li>
<li>(C) <strong>Hoaxes &amp; Lies</strong>: Some are going out of their way to fool observers into thinking they see weird stuff, or just straight lying to say they saw stuff they didn’t see.</li>
<li>(D) <strong>Aliens, Etc.</strong>: This tech seen is far more advanced than anything available to any current Earth government. So it is from a hidden more advanced society on Earth, aliens from elsewhere, time-travelers from the future, or something even weirder.</li>
</ul>
Now it seems pretty obvious that if we are rather inclusive in our definition of “UFO encounter” then (A) is the best explanation for most of them. The interesting question is how best to explain the few hardest to explain encounters. Here is a related Twitter poll I just did:


How best explain top 10 hardest-to-explain UFO cases?<br/>
(A) Delusions, mistakes, & misunderstandings,<br/>
(B) Secret advanced our govt. tech, honestly seen,<br/>
(C) Lies & hoaxes, both decentralized & govt. funded/ coordinated,<br/>
(D) Adv. tech by foreign powers, secret societies, or aliens.

> — Robin Hanson (@robinhanson) <a href="https://twitter.com/robinhanson/status/1389734641154904068?ref_src=twsrc%5Etfw">May 5, 2021</a>

<script async="" charset="utf-8" src="https://platform.twitter.com/widgets.js"></script>

Notice that I made the mistake here of lumping foreign governments into option (D), instead of into option (B) as I do above. If I had done the poll right, my guess is that we’d see: (A) 57%, (B) ~23%, (C) 10%, (D) ~10%.

Over the last few months I’ve been doing a lot of reading and watching and thinking on this topic, and I do think I have a judgement to report, a judgement that should represent news to those inclined to copy my judgment. First, (A) or (B) seems to me much less likely than (C) or (D). Second, between (Ca) spontaneous decentralized hoaxes and lies, and (Cb) hoaxes and lies coordinated by a big central organization, (Cb) seems much more likely. And third, among (Da) aliens, (Db) secret societies, (Dc) time-travelers, and (Dd) something even weirder, (Da) seems more likely.

Thus I see the main choice as between (Cb) and (Da), which would together be supported by only ~10% of poll respondents, and between which I can’t decide. Thus I am making a relatively strong claim here, at least relative to poll opinions. Let me outline some of my reasons.

First, if you look at the details of the usual hardest cases, to ones to which UFO fans most often point, you will see that there are often a lot of pretty sober looking people who all say they saw the same pretty clear and dramatic things under pretty good observing conditions. And often what they say they saw is solid-looking objects with remarkable combinations of location, speed, and acceleration, with no attendant thrust or control surfaces of the sort we’d use if we were trying to achieve those combinations.

I know enough physics and tech to know that these claimed abilities are just far beyond anything Earth governments will have access to for a long time, at least if the past is any guide. Or anything that natural weather could make. And similar abilities have been seen for over a half century, so if governments were hiding these abilities they’d be hiding them for far longer than they usually hide techs.

I also know enough human nature to know that these are not close to the sort of things that honest sober sane people would claim to see, if they just somewhat misinterpreted something that they saw or heard. And most of the people reporting in these strongest cases do seem pretty sober and sane. Thus in these strongest cases, the story that all these people are merely mistaken or deluded just doesn’t work, at least for the sorts of things they say they saw in these hardest cases. Nor does the story work that this is advanced government tech that they will release to show everyone in at most a few decades. So I must reject cases (A) and (B), which leaves me only with cases (C) and (D).

[<strong>Added 6May:</strong> Note that I am making <em>judgements</em> here about particular cases that I’ve considered in some detail. I am <em>not</em> saying I always believe what anyone says they saw. For a comparison, I find the usual evidence presented re ghosts and fairies to be <em>much</em> less persuasive. ]

Yes, humans like to play practical jokes on one another, and sometimes they take those jokes to some pretty <a href="https://twitter.com/robinhanson/status/1371629385233657858">far</a> extremes. Sometimes they even try to make the jokes last for years. And often they are inspired to copy the jokes of others. But to explain most of these hardest cases mainly in terms of practical jokes seems just a bridge too far. Really, thousands of disconnected people all around the world playing the same big scary jokes for decades, and then almost never breaking down and laughing and crowing about their jokes even decades later? In contrast, governments, especially their spy parts, have run some pretty big, well-funded, and long-lasting disinformation campaigns. So I have to favor (Cb) over (Ca) by a big margin.

Regarding (D), time-travel seems impossible without crazy extreme physics, and known secret societies on Earth have never reached within orders of magnitude of the scale and degree of secrecy that this would require. Yet spirits or creatures from other dimensions seems even more crazy. Aliens, in contrast, are predicted to exist by our best theories. It is just a matter of finding a plausible scenario wherein they’d be here now doing what we see them doing, and not doing other stuff we don’t see them doing. I’ve tried to work out such a [scenario](explaining-stylized-ufo-facts), and find one that is a bit tortured, but far more believable than secret societies or travel across time or between dimensions.
Note that both (Cb)  and (Da) are hypotheses that I would have found priori implausible. So the entire existence of the familiar pattern of UFO encounters was a priori implausible, and so now that I see it I struggle to explain it. And as both of the most likely explanations are low status topics, i.e., aliens and a record-breaking-huge government conspiracy, you can see why most people would rather just avoid the topic.

This post is already too long, so I will stop here once I make one last point: (Cb) is a theory of remarkable government <em>competence</em>. Some governments, or a consortium of them, have managed to get thousands of people to either lie and say they saw stuff they didn’t, or paid for expensive enough tech to fool them. And yet this conspiracy has remained hidden for a great many decades, even from the top levels of their own governments.

In contrast, (Da) seems to require a scenario of remarkable <em>incompetence</em>, [among](explaining-stylized-ufo-facts) the aliens themselves, among our governments, and even among the UFO activists. So which is more likely: surprisingly high government competence, or incompetence?
<strong>Added 7June:</strong> <a href="https://twitter.com/EricRWeinstein/status/1401372265564889091">This poll</a> agrees with me.

## [My Awkward Inference](#table-of-contents)
_Posted on 2023-04-29_

There have been over 100K UFO sightings reported worldwide since 1940. Roughly 5% or so are “strong” events, which seem rather hard to explain due to either many witnesses, especially reliable witnesses, physical evidence, or other factors. Many of these events are also “dramatic”, wherein the UFOs seem to display amazing abilities, ones well beyond those held by any known Earth orgs. I’d guess there are at least a thousand such strong dramatic reported events. 

Due to my work on [grabby aliens](http://grabbyaliens.com/), I felt a bit of responsibility to consider the UFOs as aliens thesis, as I seem to be a world-level expert on the [prior](https://www.overcomingbias.com/p/on-ufo-as-aliens-priorshtml) for such a hypothesis. And while I’m not so much an expert on UFO hypothesis likelihoods, I have been tempted to learn about many of these strong dramatic events. ([E](https://www.amazon.com/gp/video/detail/B08HR8SFRC/ref=atv_hm_vid_c_vJb2Rv_1_12)[g](https://www.penguinrandomhouse.com/books/201625/ufos-by-leslie-kean/)) Which I now somewhat regret, as I now find myself the awkward position of having made an awkward inference.

There are four main ways to explain UFOs:

1.  observers fooled by error/mistakes/hallucinations,
2.  people lying or observers fooled by purposeful hoaxes,
3.  real amazing devices/organisms from secret groups on Earth, or
4.  real amazing devices/organisms from secret groups beyond Earth.

My awkward inference starts here: it seems clear to me that #1 can only plausibly explain a modest fraction of strong dramatic events. Most errors would have to be much closer to gross incompetence than to “oops”. (If you’ve also looked but can’t see this, I just don’t know what to say. Pay more attention?)

Why is that awkward? Because for the last seventy years, elites, especially STEM elites, have had a very strong social consensus against explanations #3,4. Those who say otherwise are ridiculed and excluded. 

But that still leaves explanation #2, right? Well, yes, elites are okay with explaining many UFOs this way. But in order to explain _most_ strong dramatic events this way, I just don’t think it works to postulate scattered amateur liars and hoaxers. Instead I think one needs a big conspiracy, wherein a coalition of orgs has secretly and professionally coordinated to spend big budgets over many decades to have many lie, and to fool others via what are essentially magic tricks. 

The US military seems the most plausible anchor for this coalition, as they could have [tried](https://www.overcomingbias.com/p/ufos-as-usa-psychophtml) to fool the USSR into hesitating before a nuclear first strike. That could be worth a big budget. (Though note, if they’d lie about UFOs, they’d probably also lie about the moon landings.) Yes, the scale of this lie is only modestly bigger than that of some other known US lies, now revealed. But even so, elites also look down pretty hard on those who endorse such big conspiracy theories. 

Alas, my situation gets even worse. This is due to my noticing that the USSR also had many UFO reports, including many strong dramatic ones. And I just can’t believe that the US could get so many people inside the USSR to lie or arrange sufficiently elaborate magic tricks. Nor can I believe that both the US and USSR both created huge long-lasting hoaxes trying to fool each other. Nor that the whole Cold War was faked. 

Thus I’m stuck with putting substantial weight on explanations #3,4 for at least some of the thousand-plus strong dramatic UFO events. Which feels pretty awkward. 

Yes, I might now become more accepted by UFO folks, even if I become less accepted by the usual elites. But while there are exceptions, overall these aren’t exactly our intellectual creme de la creme. Yes, it seems that in this case they happen to be more right, and on something pretty big, but that is probably more due to their contrarian instincts than their brilliant analysts.

Note that if the evidence now supports my inference, it probably has done so for decades, and so can plausibly continue to do so for many more decades, or perhaps even centuries, without the relevant elite consensus changing much. So I’m not yet seeing bet-table predictions to make here.

**Added 1p:** I should mention that I’ve also looked at the best evidence offered for angels, ghosts, and fairies, and those just look much worse to me. The best evidence for ball lightning in the field is also pretty bad, even though the usual consensus believes in that due to it being created in the lab.

## [UFO Stylized Social Facts](#table-of-contents)
_Posted on 2021-03-28_

Even though many or even most UFO sightings are best explained as delusions, hoaxes, and ordinary stuff misunderstood, there appears to be a large remnant (>1000) that are much harder to explain, and which <a href="http://www.cufos.org/FAQ_English_index.html">show</a> consistent <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.555.5611">patterns</a>. Such as ~30-1000 second episodes peaking near ~9pm (<a href="https://journalofscientificexploration.org/index.php/jse/article/view/580">tied</a> to local sideral time), at random spatial locations, of quiet lights or objects in the sky with intelligent purposes and amazing speeds and accelerations. Sometimes confirmed by many people and recorded by many instruments.

If they aren’t delusions, hoaxes, or misunderstandings, the main remaining explanations are a) some sort of secret society or agency that arose on and is tied to Earth, or b) some sort of aliens. I’m not saying its aliens, but in this post, it’s aliens. That is, here I want to “go there”, and think about how best to explain UFOs, <em>if</em> they are in fact aliens.

Many have worked on trying to explain UFOs in terms of their immediate physical effects. I kinda like “laser pointers for cats” <a href="https://journalofscientificexploration.org/index.php/jse/article/view/580">style</a> theories wherein aliens in orbit send beams to paint a local disturbance, while using telescopes to watch local reactions. But these details aren’t that important for whether we believe that UFOs are aliens, as aliens would almost surely be a lot more advanced than us, and so plausibly capable of a wide range of such approaches.

No, it seems obvious to me that the main reason that most resist believing that UFOs are aliens (or secret societies for that matter) is the apparent implausibility of the social thesis. We find it hard to integrate this hypothesis with the rest of our social world views. That is, with our views on what agents can exist, how they are socially organized, and the sorts of behaviors that we expect of social agents within particular kinds of organizations. If aliens are around, why haven’t they made more direct contact, or built more obvious stuff, or traded with us, or conquered or killed us?

If the main block to believing in UFOs as aliens is a lack of a plausible enough social theory of aliens, then it seems a shame that almost no one who studies UFOs is a social science theorist. As I’m such a person, why don’t I step in and try to help? If we can find a more plausible social theory, we could become more willing to believe that UFOs are aliens. And if we can’t, we can at least confirm more expertly that the usual reluctance is justified; the social theories you’d have to invoke are so crazy unlikely that yeah, we gotta attribute UFOs to delusions, hoaxes, and misunderstandings, no matter what our eyes and instruments seem to say.

In social science, we often prepare for theorizing about a topic by first summarizing its “stylized facts”. These are key data patterns in need of explanation, phrased in language that is closer to theory. In this post, I will attempt this “stylized fact” exercise for UFOs-as-aliens. In my [next post](explaining-stylized-ufo-facts) I’ll take my shot at explaining them. Here are three key stylized facts:
<hr/>
<strong>1. LIMITATION </strong>– The very idea that UFOs are aliens, rather than a secret society on Earth, implies either a completely independent origin from us, or that any common ancestor was long ago. (~100Myr+.) So unless aliens civilizations are very short-lived, then any modest randomness in the timing along either evolutionary path implies that one of us reached our current level of civilization millions of years before the other. And since we just got here, it must be they who reached our level millions of years ago.

(Note that having a civilization last for many millions of years is itself quite an achievement. Which raises obvious questions: what sort of genetic, cultural, organizational, etc. changes were required to achieve that, and at what cost came such longevity?)

If UFOs on Earth are aliens from elsewhere, then there are in fact aliens out there, who can and do travel between the stars. Because here they are, aliens who have actually traveled between the stars. So right off the bat we must reject theories that say that such travel is impossible or crazy impractical. Or that some motivational convergence ensures that advanced life almost never does actually travel.

Now put these two facts together: they’ve been around for many millions of years, and they can and do travel between the stars. With so many millions of years and this same tech they used to get here, they could have gone everywhere. The big dramatic implication: they could have remade the universe, or at least a big chunk including our galaxy, but have not done so. Somehow they have self-limited their expansion.

(Note that in addition to limiting their expansion, aliens behind UFOS also seem to have limited their tech; UFO tech seems advanced, but not 100Myr+ level advanced.)

Now one possibility that I want to note, and set aside, is that the universe is in fact chock full of aliens who have in fact remade it, but that we are fooled to see otherwise by crazy advanced tech wielded by a vast tightly-coordinated alien conspiracy based on arbitrary inscrutable motives. Like theories of powerful intrusive gods and simulation managers with arbitrary inscrutable motives, it is not that such theories are impossible, but that they offer little room for structured analysis. I see little to gain from discussing them.

<em>Stylized fact #1: UFO aliens are very old, and could have remade universe, but some self-limit stops them.</em>

<hr/>
<strong>2. CORRELATION </strong>– This failure to remake the universe gets more puzzling the more common are aliens in space and time. If UFOs-as-aliens are as thick on all planets at all times as they are here and now, then there must be a crazy huge number of well-hidden alien facilities out there where UFO equipment is made, repaired, refueled, staffed, etc. All strongly limited to ensure that it never remakes its local universe.

Worse, there have been literally an astronomical number of opportunities for any one deviant alien to start to remake its local universe. If a deviation could last long enough, to acquire enough local resources and power, other aliens would have a hard time shutting it down without also acquiring similar levels of local resources, and thus also remaking their local universe. Even if some sort of local conformity pressure tends to stop most deviations, that pressure has to be crazy extreme reliable to work everywhere always in a vast densely populated universe.

The simplest way to resolve this puzzle is to posit that aliens are in fact pretty rare, and that they coordinate to preserve that rarity. After all, the fewer are the possible alien travel events, the higher of a deviant event chance that we can tolerate in our theory of their behavior.

(If aliens are very short-lived, then there have to be even huger numbers of them for one to be here now, requiring an even more crazily-low chance of any of them allowing any deviations.)

Besides perhaps interstellar travel being impractical, advanced life arising very extremely rarely is the simple story most of us most start out with to explain our empty universe. And even if one must <a href="http://grabbyaliens.com">postulate</a> that aliens are only extremely rare, not very extremely rare, to explain humanity’s early arrival in the universe, that still means aliens are so rare that we won’t meet them for roughly a billion years.

But for aliens that rare we have a different problem: why are they right here right now, but almost nowhere else? Something has caused a huge correlation between them and us, so that even though aliens are rare enough for their facilities to stay hidden, and even though they have created local pressures to ensure that they only rarely travel or have opportunities to try to remake the universe, they’ve made an exception for traveling to be with us here now.

The rarer are such aliens, the more time they’d need to get here from where they started. So either they’ve been around for a <em>very</em> long time, and decided to come here based on what Earth looked like a very long time ago, or they happened to start very close to us, a remarkable spatial coincidence in need of explanation.

<em>Stylized fact #2: UFO aliens are rare and self-limited, and yet are here now.</em>

<hr/>
<strong>3. INDIRECTION</strong> –  We can think of a number of plausible practical motives for rare self-limited aliens to make an exception to visit us. First, they may fear us as rivals, and so want to track us and stand ready to defend against us. Second, if their limitation policies are explicit and intentional, then they’d anticipate our possibly violating them, and so want to stand ready nearby to enforce their limitation policies on us.

In either of these two cases, aliens might want to show us their power, and even make explicit threats, to deter us from causing problems. And note the big the question of why they don’t just destroy us, instead of waiting around. A third possible motive that can explain this is that the origins of independent aliens like us are a rare valuable [datapoint](non-grabby-legacies) to them on far-more-capable aliens who they may fear eventually meeting. In this case they’d probably want to stay hidden longer, and then maybe destroy us later.
Note that all of these motive theories suggest a substantial ability of these aliens to organize and plan actions on the basis of such abstract, collective, and long-term considerations. A very decentralized alien society might not be capable of it, nor perhaps of maintaining whatever pressures prevent their own travel and remaking the universe.

The most striking fact about UFO encounter events is how little they seem to accomplish, not for any of these goals, nor for any other easily identifiable practical goals. Advanced aliens could surely monitor us sufficiently from a distance unseen, and to control us via commands or threats would require much more direct contact. These UFO events don’t seem to much help them collect useful info or resources, nor do they much limit or expand our info, powers, or resources. Yes, they show some of us that the universe can look weird, but surely they know that we know that fact regardless.

Now we humans are widely known to often act on indirect motives, not tied very closely to simple direct practical outcomes. Many animals “play.” Human ancestors who did things for “symbolic” reasons are often seen as especially “advanced”. People today often have “obsessions” that make them spend far more on some things than simple practical ends can explain. Lazy secure organizations are at times quite “wasteful”, doing things that pretend to achieve practical ends, but in fact achieve them at best quite ineffectively. And I’ve recently coauthored a <a href="http://elephantinthebrain.com">book</a> on how common are hidden motives in humans today; many things we do just don’t much accomplish the goals to which we give lip service, like learning at schools, and healing at hospitals.

So it isn’t crazy to think that aliens might have indirect obsessive lazy motives for UFO encounters, motives hidden perhaps even from themselves. But this case, of overcoming the usual coordinated limits to take eons to fly to a distant star just to glow-buzz their treetops, seems spectacularly extravagant even by the standards of [dreamtime](this-is-the-dream-time) humans today.
For this to happen, aliens need a sufficient level of “slack” resources available to spend on such symbolic activities. And even with hidden motives and lazy organizations, we humans usually at least make up vague stories about practical ends served by our actions, even when such stories don’t stand up to close scrutiny. So a decent theory of aliens should explain their level of slack, and suggest some ideas for what stories aliens are telling themselves about the ends they accomplish via UFO encounters. And why they haven’t just destroyed us.

<i>Stylized fact #3: Alien-driven UFO encounters accomplish little, yet must somehow be justified to them. </i>

<hr/>
And those are the key stylized facts that a social theory of aliens must explain. Again, it is the lack of seeing a sufficiently plausible explanation of such facts that is why most are reluctant to believe in UFOs-as-aliens. (Yes, many are not so reluctant, but mostly because they don’t understand enough to be puzzled.)

<strong>Added 31Mar:</strong> My explanation attempt is [here](explaining-stylized-ufo-facts).

## [Explaining Stylized UFO Facts](#table-of-contents)
_Posted on 2021-03-30_

In my [last post](social-ufo-stylized-facts) I summarized some key stylized social facts that a theory of UFOs-as-aliens would need to explain:
> Any aliens behind UFOs would be amazingly long-lived creatures who have somehow coordinated to limit any small part of themselves from expanding and remaking the universe. This gets easier to believe the smaller and rarer they are. (They also seem to have limited their tech.) Yet they’ve overcome their self-limits to travel to be here now, so they must be close enough to come quickly once they saw signs of advances, or they saw signs of interest very early and traveled very far.
> We can see practical reasons for them to come here, at least if they can coordinate to achieve such plans. But most such motives seem better served by destroying us than by the usual reported UFO encounters, which seem to accomplish little. Yes, humans today do many things for indirect “symbolic” motives, and lazy organizations often pretend to achieve more than they do. But these require slack, and fig-leaf stories to justify them. So from whence comes alien slack, and what could be their justifications?

My tentative explanation for all this has four main supporting elements: <em>panspermia siblings, world government, moral ideology</em>, and <em>complexity rot</em>.

<hr/>
<strong>1. Panspermia Siblings </strong>– Imagine that life started on some previous planet [Eden](searching-for-eden), where it went through some very hard steps. Life then spread from Eden to Earth, as well as to some other planets. Even if it were hard for life to spread between planets, the Eden-to-Earth scenario could still be statistically favored compared to the Earth-only scenario, as Eden can start much earlier and can be in more places. This is because Earth had to have a much calmer environment than Eden to host the last half-billion years of fragile multi-cellular life. These few seeded planets might be the only ones with life in the million nearest galaxies.
Once seeded, Earth and its sibling planets would then compete to complete the remaining hard steps required to reach advanced life. If one of those planets succeeded before Earth, then it would host close but rare aliens, who share a lot of biology detail (e.g., DNA) with Earth. Those aliens could have then searched out their sibling stars (which have a clear signature that we can see even now), found Earth around one, and then waited perhaps millions of years for civilization to appear here. These aliens had several good and practical possible reasons for coming here in time to see us now up close.

Positing that the aliens behind UFOs come from just one nearby sibling planet, with the nearest other aliens many galaxies away, makes it easier to believe that these aliens have successfully imposed sufficiently-strong self-limits on expansion and on tech advance, leaving the empty universe we see.

<hr/>
<strong>2. World Government</strong> – Over the last few centuries, one of the most consistent world trends has been an increase in human organization size and complexity, with more functions and decisions drifting up to higher levels. We have developed both better networks and better hierarchical organizations. Plausibly this trend is behind most others; it seems to be the main driver of faster innovation, which is the main cause of more wealth, which drives most other trends.

A straightforward long-term prediction from this trend is “world” (really “civilization-wide”) [government](is-world-government-inevitable). After all, a few have come close to creating this via force, we now have a United Nations by consent, and regulators worldwide share an [elite culture](the-world-forager-elite) that creates a de facto world government on many issues. Stronger, more formal versions seem likely within centuries.
Within a star system, talk delays are modest, and it is easy to see and shoot at most anything, making a world government quite feasible there. However, world government is quite hard to start (and if started harder to maintain) once independent self-sufficient colonies at other stars can grow as fast as at the home star system. Thus the existence of such colonies becomes a deadline for the creation of a world government. As near Earth this deadline seems likely to be met, that may also have happened for sibling star aliens.

The advantages of a world government will seem clear and compelling: a civilization that can better coordinate on global problems like war, pollution, and innovation. And that can better enforce widely-liked regulations. Also, global majorities will be eager to impose their will on global minorities, and to lock down their temporary advantages via a permanent world government.

By its very nature, a world government reduces innovation and adaptation in, but also promotes the stability of, the largest scale civilization structures. An advanced star-system-wide civilization probably has a large enough base of knowledge and resources, and a stable enough environment, for this tradeoff to allow for stability over many millions of years. Thus the fact that aliens have lasted for millions of years weakly suggests that they have a world government.

<hr/>
<strong>3. Moral Ideology</strong> – While pre-human primate groups were held together mainly by kin and alliances, human groups could be larger due to social norms, which were enabled by human weapons and language. Social norms have also aided our other more recent methods of social organization. As norms matter more in collective politics than in private life, a world government would gain legitimacy and stability by more strongly supporting widely-held moral norms.

Thus a world with a world government is likely to impose more stronger regulations in support of widely-held moral intuitions. And in an era of rapidly changing technology often in tension with moral intuitions that evolved in prior eras, that may result in substantial limitations on tech. Sibling star alien world governments might ban advanced artificial intelligence, brain emulations, or nuclear-powered space ships. They might also insist on preserving their biological bodies.

By using strong surveillance, embedded political officers, and using the threat of destruction from a distance, a world government might hope to keep control over a rapidly expanding sphere of interstellar colonies. But surely such control is far easier if substantial interstellar colonies are simply banned. Independent colonies would threaten not only the relative status of the current world government leaders and polity, but they’d also threaten to allow evasion of morally-treasured regulations.

Thus aliens with a world government might limit expansion, and also tech, not just to support environmental and anti-colonialism type ideologies, but also to preserve the relative status of locals and their ability to impose civilization-wide regulations. We have often seen similar behavior in human history, when secure isolated local regimes have discouraged contact with outsiders. The fact that aliens have not yet destroyed us also suggests that they have moral ideologies.

In a very long-lived civilization with a stable world government, the high-level organization of government and its key principles and regulations might become so stable that other structures of that civilization evolve to match them more often than the government evolves to match other structures. The government becomes like a mountain, where life adapts to behave differently at the mountain’s foot versus near its peak. So over millions of years the intuitions and practices of individuals and local groups may well evolve differently to match different parts of the stable world government with which they most strongly interact.

And if their world is more stable than our ancestors’ worlds have been, their minds might become less general, being adapted more to a particular range of situations.

<hr/>
<strong>4. Complex System Rot</strong> – Since the origin of life, competition has been the main driver of adaptation and innovation. Yes, cooperation has been important, but it is competition that has designed and promoted cooperation. While genetic forms of competition once dominated, cultural competition now matters more. Individuals and their practices compete within organizations, while organizations and their practices also compete for members, customers, investors, and more.

Across this long history, individual organisms, species, human organizations, and even empires have consistently tended to “rot”. That is, their long-lasting materials and structures slowly decay, becoming less flexible or general and more fragile, until they simply die or are eaten or displaced by rivals. This continues to happen today even with software and legal systems, as they try to adapt to new circumstances, and it happens even when their materials do not decay. It is competition that has corrected for this tendency to rot, by ensuring that simpler more general robust structures are available to replace failing fragile versions.

A civilization lasting for millions of years with a stable ideological world government preventing most expansion and tech innovation seems to me a recipe for high level system rot. New agencies, rules, and regulations would slowly accumulate on top of old ones, instead of being sufficiently culled, refactored, or reorganized. Agency growth and changes would have been often made to suit local ambitions instead of external needs, often using newly invented moral imperatives.

In our limited Earth history, we have often seen spectacular waste by stable secure empires, religious authorities, and secure monopolistic firms. Each example has found ways to spin stories justifying its waste, stories accepted by many observers. Many observers have also often believed decaying organizations who claimed that they had not yet lost any flexibility or generality, claims only clearly disproved when they were displaced by rival competitors.

Over millions of years, an ancient alien world government would accumulate far worse wasteful habits, and yet always offer semi-plausible if tortured justifications, stories not yet clearly disproved by competitors, who are not allowed to exist. Such governments would proudly tell themselves that they are still flexible and general, and up to most any challenge. But they’d be lying to themselves.

<hr/>
<strong>5. Putting it all together</strong> – So here is my best scenario to explain UFOs as aliens. I’m not saying it is good enough to let us believe that some UFOs are more likely than not aliens. I’m just saying that it is the best I’ve been able to come up with. You judge how good.

Life started long ago on Eden, which then seeded both Earth and our siblings’ home planet. Their home is somewhere in our galaxy, and yet they are the nearest advanced aliens for a million galaxies. For many millions of years, they’ve had a stable world government enforcing ideologically-justified regulations limiting expansion, tech innovation, and perhaps much more. Local intuitions and practices have long since adapted to this stable mountain; it feels to them <em>very</em> legitimate.

This world government made an exception to its expansion bans to allow trips to sibling planets hosting life, and allowed the development of whatever tech that required. This was done in support of key ideologies, which is probably why they haven’t destroyed us, and yet they plan to make sure we obey their regulations on expansion and tech innovation. And data on us may help prepare them to meet other aliens. (They may or may not believe they will eventually meet future <a href="http://grabbyaliens.com">grabby</a> aliens.)

This long trip, and their management of Earth, is a task calling for great generality and flexibility, which their government mostly lacks, though it claims otherwise. Worse, their fear of allowing an expansion escape led them to tightly control this expedition. So most key choices have been made ahead of time, and aliens here at Earth are kept on a tight leash, dependent on resources and equipment shipped from home, and on tight rules of engagement.

These aliens long ago made their plans for how to monitor Earth civilization, and how to control it if that became necessary, and they built and shipped equipment and resources here based on that plan. Local alien administrators here have little discretion, are watched by local political officers, and have very limited abilities to make equipment or to collect resources beyond their pre-anticipated needs.

In drug regulation on Earth today, we have an ideology wherein conclusions drawn from observations are declared insufficient; one must also have proper government-managed “experiments.” If these aliens have a similar epistemological ideology, they would plan to observe Earth hidden safely from a distance, but they’d also need to periodically “poke” the locals and watch reactions. Alternatively, they might have an ideology of “touch”, wherein they couldn’t in good conscious control us unless they had before touched us “directly” somehow.

So, maybe, this was the safest most robust plan they could come up with to touch/poke us, when planning long ago back on their home planet: <em>They poke us via making local disturbances in air or water usually by sending dark beams from safely hidden orbital projectors</em>.

At a controlled distance, these beams can cause glowing balls of air, or smooth surfaces. (These can cause radar reflections, burn marks on the ground, and even sounds.) Their orbiting projectors would be safe from retaliation by Earthlings who would at first not even notice the beams, and who later would find it hard to trace those beams back to their orbital origins. And even finding those projectors probably doesn’t find the weapons by which they stand ready to destroy us if we get out of control.

So long ago these aliens sent to Earth equipment for installing telescopes, beam projectors, and weapons in orbit around Earth. All supplied with energy, covered to remain unseen, and with supports to keep them running for eons. And the main thing that aliens have done since their arrival is to maintain these facilities, and process the info collected.

These local administrators send regular and positive <a href="https://en.wikipedia.org/wiki/TPS_report">TPS reports</a> back home regardless of how well things are actually going. The local aliens are likely bad at interpreting all the info they collect, their home world is bad at judging the quality of their efforts, and also at incentivizing such efforts. Thus maybe they not have learned much so far, and may not even be able to understand our electronic traffic that they can hear from space. They may not have detectors on the ground tapping into communications here. Perhaps they don’t even have language among themselves, and so aren’t capable of understanding our talk.

Maybe the fastest that their economy grew back on their home planet, before it slowed down due to regulations, was much slower than the Earth economy is growing now. So they never really planned much for how to react to the rapid change that we are undergoing here now. Local administrators keep sending TPS reports back home, doing the scheduled UFO projector runs, and keeping their fingers nervously on their weapons buttons. But like most government administrators, they are terrified of having to take the initiative to make a big decision, and so would rather wait until the choice becomes completely obvious.

If this all sounds implausibly incompetent to you, consider that if many UFOs are in fact aliens, the U.S. military and many militaries around the world have in fact been spectacularly incompetent at considering UFO reports and studying the threats that they imply. Yet these militaries existed in an era of competition and a burst of UFO reports started during a major war (WII) during which militaries had rapidly evolved to become unusually competent. Imagine how worse would be a military with a secure budget but no actual war for a million years. (Note that UFO activists have also been spectacularly incompetent in many ways.)

You might think that all this alien incompetence would give humans a fighting chance to defy these aliens and break out of their control. Possibly, but probably not. They probably do have their finger on the big kill-all-humans button, and that button probably does actually work. We might have a chance to sneak off and start a very distant stealthy interstellar colony, but that also seems damn hard.

But if the aliens behind UFOs are incompetent at understanding us and communicating with us, that sounds like bad news for our ability to learn and abide by their rules. It would be nice if they had some effective plan for integrating us into their world, beyond just pushing the button on us when we cross some line. But I wouldn’t count on that.

Note that not all of the elements of the story I’ve just told are strictly necessary to explain the stylized facts I’ve outlined. Those extra story elements are indicated by words like “maybe”, and are added to help you see how this story might be realized. If you don’t like my story, what story would you tell to explain <em>all</em> these stylized facts, not just one or two?

Note also that I’ve told this [story](non-ufo-local-alien-clues) [thrice](do-foo-fighters-show-our-snafu-fubar-future) [before](if-aliens-are-near), though this version is more elaborate.

## [Non-UFO Local Alien Clues](#table-of-contents)
_Posted on 2020-05-09_

> [US] Department of Defense formally released three Navy videos that contain ‘unidentified aerial phenomena.’ … When the videos were published in 2017 and 2018 by <em>The New York Times</em> …, they gave new hope to those looking for signs of extraterrestrial life. … ‘it’s time that we make progress to understand the extraordinary technology being observed during these events.’ (<a href="https://www.nytimes.com/2020/04/28/us/pentagon-ufo-videos.html">More</a>)
> Still, when you run all the arguments through your mind, is it not possible to come away with an estimate of at least a one-in-a-thousand chance that alien visitations are a real thing? Even such a small chance would be worthy of more discussion. (<a href="https://www.bloombergquint.com/view/ufo-sightings-u-s-military-takes-them-seriously-you-should-too">More</a>)
> Alexander Wendt, a professor of international relations at Ohio State University. Wendt is a giant in his field of IR theory, but in the past 15 years or so, he’s become an amateur ufologist. … ‘It’s possible they’ve been here all along. … They could just be intergalactic tourists. Maybe they’re looking for certain minerals. It could just be scientific curiosity. It could be that they’re extracting our DNA. I mean, who knows? I have no idea. All I know is that if they are here, they seem to be peaceful.’ (<a href="https://www.vox.com/policy-and-politics/2020/5/8/21244090/pentagon-ufo-videos-navy-alexander-wendt">More</a>)

In the above, two social scientists, economist Tyler Cowen and political scientist Alexander Wendt, say to take UFOs-as-aliens more seriously. But in a quick search I can’t find any serious <em>social</em> analysis of this hypothesis. I see studies of why humans might want to believe in aliens, or why they might have a taboo against considering aliens. But not an analysis of alien social behavior, to help us evaluate the UFOs-as-aliens hypothesis. What I find is mostly like the above, “who knows?” So let me try.

To do Bayesian inference well, we need a set of not-crazy scenarios describing what might really be going on, we need a prior describing our beliefs about which of these scenarios seems how likely using our background knowledge, we need some more specific data to consider, and we need likelihood functions that say how likely each piece of specific data would be given each scenario.

<em>Note:</em> to study priors and likliehoods, I’ll need to make some assumptions, and see where they lead. That doesn’t mean I actually believe them.

While many UFO reports can be easily dismissed, a remnant of reports seems harder to dismiss, apparently showing artificial physical objects in the sky with amazing velocities and accelerations, but without the usual physical effects on nearby things.

Regarding these puzzling UFOs, I see three key explanation categories:

<ul>
<li><strong>Measurement Error</strong> – What look like artificial objects with crazy extreme abilities are actually natural stuff looked at wrong. Perhaps due to intentional fakery. This is widely and probably correctly judged to be the most likely scenario. Nevertheless, we can’t be very confident of that without considering its alternatives in more detail.</li>
<li><strong>Secret Societies</strong> – There really are artificial objects with amazing abilities, though perhaps somewhat overestimated via partially misleading observations. These are created and managed by hidden groups in our world, substantially tied to us. Secret local military research groups, distant secret militaries, non-state Bond-villains, time-travelers from our near future, dinosaur civilizations deep in the Earth’s crust, etc.</li>
<li><strong>Aliens</strong> – Again these objects really do have amazing abilities, and are created by hidden groups. But in this case the relevant groups are much less integrated with and correlated with our societies and history. Little green men, their super-robot descendants, universe-sim admins, gods, etc. If these groups had a common origin with, competed with, or were much influenced by the groups that we know of, such things mostly happened long ago, and probably far away.</li>
</ul>
These three alternatives don’t obviously exhaust all options, but then again I can’t really think of much else.

Assuming that third scenario, hidden groups whose history and features are not much integrated with ours, we can confidently conclude that they most likely arose long ago and far away. Otherwise their space-time correlation with us would be an unlikely coincidence. Perhaps we and they arose from stars in the same stellar nursery, or Earth life was seeded by them, but that still leaves huge relevant durations and distances. And these pretty strongly support their having spectacular technologies and capacities. They have progressed and innovated for many millions and perhaps billions of years more than we. So they can travel <em>very</em> long distances, and survive <em>very</em> long durations.

Now it isn’t at all crazy to expect that many alien powers might arise over the scope and history of the universe. Our prior there has to start out high. But it is a bit more surprising that over billions of years this hasn’t resulted in visible changes to the universe we see. Somehow, all these advanced aliens have not widely rearranged galaxies, deconstructed stars, and so on. Once we condition on the “<a href="http://mason.gmu.edu/~rhanson/greatfilter.html">great filter</a>” fact that we don’t see aliens out there, it become much less clear how likely we should consider aliens to be, especially aliens capable of and inclined to come near us. But that scenario also isn’t obviously impossible, so let us continue.

To consider UFOs-as-aliens, we must consider ancient aliens who were once very far away long ago, had spectacular tech and capacities, did <em>not</em> visibly change the universe, eventually traveled to here now, and are doing stuff around here now. The most likely scenarios consistent with that description tend to have those aliens be clearly visible around here. They’d be living near, building things, using local resources, dumping trash, fighting with each other, etc. But they are not clearly visible. So we must downgrade our prior again, perhaps a lot, to consider scenarios where active local aliens are clearly visible neither on cosmic scales nor on local scales.

For example, perhaps these aliens have found other attractive resources somewhere else hard-to-see nearby, perhaps dark matter or another dimension, resources so much more attractive than ours that they see no point in using the stuff we see. (But then why do their UFOs come here and interact with our matter?) Or perhaps they’ve coordinated to make our region into a nature preserve, not to be used much. Or perhaps they want to observe Earth and human evolution untouched and uninfluenced. Not crazy scenarios, but also not obviously the most likely ones consistent with our prior knowledge.

We have so far had to cut down our aliens prior to account for the lack of clear alien visibility at both cosmic and local scales. But we still have at least one more puzzling data point to integrate into our analysis: these aliens are sometimes somewhat visible as UFOs. Surely such advanced aliens are well aware of our existence, and can figure out roughly what we can see and infer about them. So either they are purposely allowing us to see glimpses of them in this way, or they are failing to prevent such glimpses.

So far, everything I’ve said I’ve heard before from others. Now come my original points, which I haven’t heard from elsewhere, though I wouldn’t be surprised if others have said them. (Far more is written on this than I have time to survey, as I lack good quality filters in this area.) Under either of these scenarios, purposeful or accidental revelation, it isn’t obvious that UFOs would be the only or main channel of such revelations to us about aliens.

If UFOs are shown to us on purpose, to influence our society in some way via a weak suspicion of local aliens, surely such capable aliens would also have a great many other way to influence us. And it is hard to imagine a purpose, or ability package, which would limit their influence to letting us see UFOs. They could edit our DNA, start pandemics or earthquakes, whisper hints to key leaders or innovators, kill off opponents, etc. And while they might be able to do all these things in ways that remain quite hidden, they could also work less hard to hide, and let some of us sometimes get glimpses of their influence.

Perhaps the fact that we see strange UFO behavior is due to accidental failures to sufficiently monitor or incentivize local alien actors who would otherwise want to influence us. Their abilities to prevent such failures would be quite good, but not quite perfect. But if so, similar failures could also allow local aliens to influence us in other ways. On our end, perhaps editing DNA, whispering hits, etc. On their end, they must at some points collect materials and energy sources, stay at home locations, and discard trash.

So under both types of scenarios, if UFOs are due to aliens we should also expect to sometimes see rare but striking alien influences in many other domains. Thus we should be able to get data to confirm or refute this UFOs-as-aliens theory by looking at many other areas of life, not just at strange objects in the sky. (Or in sea, caves, forests, and other sparse places.)

Sure, it is logically possible that aliens intend for us to see them only via strange sky objects. But our prior doesn’t at all favor that, even after modification to condition on low visibility at cosmic and local scales. So a lack of apparent alien influence in many other areas of life must count as evidence against the UFOs-as-aliens scenarios, both the purposeful-but-weak and the barely-accidental versions. Conversely, UFOs-as-aliens would be confirmed by a consistent pattern of rare but striking hard-to-explain influences in other areas of life, influences that aliens might plausibly want to cause.

I am somewhat of a polymath, pursuing a wider range of areas and topics than do most intellectuals or social scientists. So I consider myself to be more qualified than most to consider the possibility of strange influences on human behavior. And while I have in fact seen many strange things, for almost none does alien influence seem especially helpful in explaining what we see.

Now you might argue that aliens want to limit their purposeful revelations to one main most-effective area, or that due to varying costs of coordination and enforcement, one main area will end up being the hardest to control, and thus the area where the most accidental revelations occur. So why couldn’t strange stuff in the sky be that main area in either case?

Yes, that’s not crazy. But assume then that aliens are trying hard to just barely weakly reveal themselves in only one area, or that they are trying hard to prevent us from seeing them but just failing a bit in one worse area. Neither of these scenarios offer much encouragement for more careful analysis of this UFOs-as-aliens theory. In both cases, aliens are controlling how much we see, and so can plausibly quickly adjust their efforts to hide better if we surprise them with being more perceptive than expected. And if we are less perceptive than expected, they can relax their efforts a bit, to make it easier for us to see.

This is somewhat like the problem of inferring that we live in a sim via errors in the sim. If we lived in a sim, and the people running it could see us noticing errors, then they cold stop the sim at that point, [back](reversible-simulations) it up, and restart after putting more effort into cutting errors. So we’d only remember errors if they wanted us to remember them. In this scenario, if we just barely sometimes notice errors that we are not very sure are errors, our putting more effort into studying possible sim errors would only be rewarded by stronger efforts on their parts to hide their errors.
That is, if there really are gods around who don’t want us to easily see them, but who sometimes reveal themselves to some of us, we can’t gain much by trying to together better analyze our shared data, to see if they exist. They can control what we see, and control us more directly, in enough ways that we will only know and see what they want us to know and see. Yes, okay, maybe they intend to reward us by revealing themselves to us, but only after we do a good enough collective analysis of our data. But really, given all the other plausible motives and priorities that they might have, our prior has to count that as a <em>quite</em> unlikely scenario. Most likely, when they want us to see them, we’ll see them, but not before.

Yes, aliens might just happen to be at the edge of detectability to us, but <em>not</em> due to efforts on their part to prevent or encourage detection. Yet if that edge region in detectability space is narrow, then it seems that our relative prior on that scenario should be low. The fact that UFOs have remained near our edge of detectability for 80 years of improving sensor tech and increasing sensor density also weakly suggests that more than coincidence is at work here. However, an increasing taboo against UFO-as-aliens may be an adequate explanation for this, and the edge region of detectability space may not in fact be narrow.

Of course even more likely, perhaps, no nearby aliens cause UFOs. But if they do, the best hypothesis, for its combo of likelihood and productivity, seems to be aliens who can travel very far in space and time, who sometimes travel near us, but who care little about us or the types of resources that we can see or use. They do visit here sometimes, where we sometimes meet them accidentally. The rest of us only hear of such meetings when our taboo against reporting them happens to be especially weak. Weirder than I expected, but then the universe has been weirder than we’ve expected before.

<strong>Added 5a:</strong> A creative scenario is humans finding & using ancient alien tech. Alas, the prior chances seem quite low that alien tech would be abandoned near us, found, still functional after this long, functional outside supporting resources of their civilization, useable by us, and still kept hidden.

## [UFOs and Status](#table-of-contents)
_Posted on 2021-06-04_

Status seems pretty central to the UFO phenomena.

For example, reports have been filed on well over 100K encounters worldwide so far, but most of the books & movies on the topic focus on the same few cases. These cases are chosen in part for having more witnesses, detail, and physical evidence. But they seem especially chosen for having prestigious witnesses and locations. Seen by police or military workers, especially pilots. At military bases, especially housing nukes. These same books and movies are most eager to interview sympathetic people who are very high status, such as heads of state.

Similarly, many ancient legal systems had formal rules relating status to whose legal testimony to believe. And the social status of witnesses matters greatly today in court, even when there are no explicit rules requiring this.

Apparently most who witness UFOs as part of their job don’t report them, fearing reputation consequences. Because UFO fans are widely seen as very low status, at least among cultural elites. Similarly, organizations like police, militaries, airlines, and airports don’t want to be associated with such events, and so discourage reports by members. Unless some outside monitoring system discourages it, such orgs probably simply destroy such reports when they can.

When high officials have been asked privately why they would be reluctant to publicly admit to UFOs, they consistently say that the public rewards them for projecting ability and knowledge re their topic areas. UFOs require them instead to admit that they don’t know, and that there may be other parties around far more able than they. This effect is larger for police and militaries, compared to other agencies. And it is largest for the United States, at least during the period when it has been nearly the world’s dominant military power.

This all fits with several other militaries around the world releasing their UFO reports, long before the US has considered doing so. And it predicts that coming US release will be minimal, at least compared to the data the US could have and may have been collecting.

As a mildly elite academic, I can directly feel the status hit. If UFOs have an exotic intelligent cause, then we as a species have a lot less freedom than we thought to direct our destiny. And our governments, elites, and academics can do less to protect or inform us.

Yes, we might fund more UFO research, but I honestly don’t see the evidence situation changing that much for the indefinite future. Given how much data we already have, I don’t see more funding changing the overall data situation that much. These aren’t events you can seek out; you have to wait for them. And if there are intelligent exotic UFO causers here, they are clearly not eager to clearly show themselves.

And as long as the data situation remains ambiguous, I expect academic elites to remain adamant in dismissing exotic explanations. They’ll probably divert most funding they get on this topic to other topics they respect more. It will be hard to make much intellectual progress here, and those who do will be consistently slighted by academic elites. Even if society comes to accept UFOs more as a legitimate topic of investigation, elites will make very sure that the people who have so far championed this cause will <em>not</em> get more respect. Instead funding and respect will go to existing elites who deign to touch on the topic, at least from acceptable angles.

Status effects may even help explain some key features of UFO behavior. For example, among humans today, the response to an aggressive physical attack usually depends on how strong is the attacker relative to the defender. (The strengths of both sides’ allies are usually included in this calculation.) When the attacker is much stronger, the usual response is submission. And if they have similar strength, then the defender is likely to react vigorously.

However, if the attacker is far weaker, like a toddler attacking an adult, the usual response is to signal one’s strength by easily deflecting the attack, with little harm to either side. And in the reports I’ve read, this seems to be the usual reaction of UFOs to human attack: easy deflection. Which seems to signal their awareness, intelligence, abilities, and status stance. Maybe they sometimes let us seem them just so they can dis us in this way.

Status effects might even explain their lack of communication. (If they exist, of course.) Often small nations are eager to “enter into talks” with big nations just for the status bump this gives; “they take us seriously, and include us among those who must be consulted”. Conversely, the ultimate status dunk is to refuse to talk to or about someone; you act as if they are as worthy of this as a gnat. Might this explain the otherwise-puzzling lack of direct communication from intelligent exotic causes of UFOs?

A perhaps related and more ominous possible reason for their lack of communication is that they expect this to lead to us asking them some awkward questions. About our history, their expectations about us, their previous behavior toward us, their future plans regarding us, etc. Often the simplest way to avoid having to answer awkward questions is simply refusing to talk. Maybe how they plan to treat us reflects their view of our relative status, and we might not react well to hearing this.

## [Why Age of Em Will Happen](#table-of-contents)
_Posted on 2019-07-09_

In some technology competitions, winners dominate strongly. For example, while gravel may cover a lot of roads if we count by surface area, if we weigh by vehicle miles traveled then asphalt strongly dominates as a road material. Also, while some buildings are cooled via fans and very thick walls, the vast majority of buildings in rich and hot places use air-conditioning. In addition, current versions of software systems also tend to dominate over old older versions. (E.g., Windows 10 over Windows 8.)

However, in many other technology competitions, older technologies remain widely used over long periods. Cities were invented ten thousand years ago, yet today only about half of the population lives in them. Cars, trains, boats, and planes have taken over much transportation, yet we still do plenty of walking. Steel has replaced wood in many structures, yet wood is still widely used. Fur, wool, and cotton aren’t used as often as they once were, but they are still quite common as clothing materials. E-books are now quite popular, but paper books sales are <a href="https://www.publishersweekly.com/pw/by-topic/industry-news/bookselling/article/75760-print-sales-up-again-in-2017.html">still</a> growing.

Whether or not an old tech still retains wide areas of substantial use depends on the average advantage of the new tech, relative to the variation of that advantage across the environments where these techs are used, and the variation within each tech category. All else equal, the wider the range of environments, and the more diverse is each tech category, the longer that old tech should remain in wide use.

For example, compare the set of techs that start with the letter A (like asphalt) to the set that start with the letter G (like gravel). As these are relatively arbitrary sets that do not “cut nature at its joints”, there is wide diversity within each category, and each set is all applied to a wide range of environments. This makes it quite unlikely that one of these sets will strongly dominate the other.

Note that techs that tend to dominate strongly, like asphalt, air-conditioning, and new software versions, more often appear as a lumpy change, e.g., all at once, rather than via a slow accumulation of many changes. That is, they more often result from one or a few key innovations, and have some simple essential commonality. In contrast, techs that have more internal variety and structure tend more to result from the accumulation of more smaller innovations.

Now consider the competition between humans and computers for mental work. Today human brains earn more than half of world income, far more than the costs of computer hardware and software. But over time, artificial hardware and software have been improving, and slowly commanding larger fractions. Eventually this could become a majority. And a key question is then: how quickly might computers come to dominate overwhelmingly, doing virtually all mental work?

On the one hand, the ranges here are truly enormous. We are talking about <em>all</em> mental work, which covers a very wide of environments. And not only do humans vary widely in abilities and inclinations, but computer systems seem to encompass an even wider range of designs and approaches. And many of these are quite complex systems. These facts together suggest that the older tech of human brains could last quite a long time (relative of course to relevant timescales) after computers came to do the majority of tasks (weighted by income), and that the change over that period could be relatively gradual.

For an analogy, consider the space of all possible non-mental work. While machines have surely been displacing humans for a long time in this area, we still do many important tasks “by hand”, and overall change has been pretty steady for a long time period. This change looked nothing like a single “general” machine taking over all the non-mental tasks all at once.

On the other hand, human minds are today stuck in old bio hardware that isn’t improving much, while artificial computer hardware has long been improving rapidly. Both these states, of hardware being stuck and improving fast, have been relatively uniform within each category and across environments. As a result, this hardware advantage might plausibly overwhelm software variety to make humans quickly lose most everywhere.

However, eventually brain emulations (i.e. “ems”) should be possible, after which artificial software would no longer have a hardware advantage over brain software; they would both have access to the same hardware. (As ems are an all-or-nothing tech that quite closely substitutes for humans and yet can have a huge hardware advantage, ems should displace most all humans over a short period.) At that point, the broad variety of mental task environments, and of approaches to both artificial and em software, suggests that ems many well stay competitive on many job tasks, and that this status might last a long time, with change being gradual.

Note also that as ems should soon become much cheaper than humans, the introduction of ems should initially cause a big reversion, wherein ems take back many of the mental job tasks that humans had recently lost to computers.

In January I [posted](how-does-brain-code-differ) a theoretical account that adds to this expectation. It explains why we should expect brain software to be a marvel of integration and abstraction, relative to the stronger reliance on modularity that we see in artificial software, a reliance that allows those systems to be smaller and faster built, but also causes them to rot faster. This account suggests that for a long time it would take unrealistically large investments for artificial software to learn to be as good as brain software on the tasks where brains excel.
A contrary view often expressed is that at some point someone will “invent” AGI (= Artificial General Intelligence). Not that society will eventually have broadly capable and thus general systems as a result of the world economy slowly collecting many specific tools and abilities over a long time. But that instead a particular research team somewhere will discover one or a few key insights that allow that team to quickly create a system that can do most all mental tasks much better than all the other systems, both human and artificial, in the world at that moment. This insight might quickly spread to other teams, or it might be hoarded to give this team great relative power.

Yes, under this sort of scenario it becomes more plausible that artificial software will either quickly displace humans on most all jobs, or do the same to ems if they exist at the time. But it is this scenario [that](30855) [I](foom-debate-again) [have](tegmarks-book-of-foom) repeatedly argued is pretty crazy. (Not impossible, but crazy enough that only a small minority should assume or explore it.) While the lumpiness of innovation that we’ve seen so far in computer science [has been](how-deviant-recent-ai-progress-lumpiness) modest and not out of line with most other research fields, this crazy view postulates an enormously [lumpy](how-lumpy-ai-services) innovation, far out of line with anything we’ve seen in a long while. We have no good reason to believe that such a thing is at all likely.
If we presume that no one team will ever invent AGI, it becomes far more plausible that there will still be plenty of jobs tasks for ems to do, whenever ems show up. Even if working ems only collect 10% of world income soon after ems appear, the scenario I laid out in my book <a href="http://ageofem.com"><em>Age of Em</em></a> is still pretty relevant. That scenario is actually pretty robust to such variations. As a result of thinking about these considerations, I’m now much more confident that the <em>Age of Em</em> will happen.

In A<em>ge of Em</em>, I said:

> Conditional on my key assumptions, I expect at least 30 percent of future situations to be usefully informed by my analysis. Unconditionally, I expect at least 5 percent.

I now estimate an unconditional 80% chance of it being a useful guide, and so will happily take bets based on a 50-50 chance estimate. My claim is something like:

> Within the first D econ doublings after ems are as cheap as the median human worker, there will be a period where &gt;X% of world income is paid for em work. <em>And</em> during that period <em>Age of Em</em> will be a useful guide to that world.

Note that this analysis suggests that while the arrival of ems might cause a relatively sudden and disruptive transition, the improvement of other artificial software would likely be more gradual. While overall rates of growth and change should increase as a larger fraction of the means of production comes to be made in factories, the risk is low of a sudden AI advance relative to that overall rate of change. Those concerned about risks caused by AI changes can more reasonably wait until we see clearer signs of problems.

## [How To Not Die (Soon)](#table-of-contents)
_Posted on 2020-01-22_

You don’t want to die. If you <a href="https://twitter.com/robinhanson/status/1197182569051303937">heard</a> that an asteroid would soon destroy a vast area around your home, you’d pay great costs to help you and your loved ones try to move. Even if you’d probably fail, even of most of your loved ones might not make it, and even if success meant adapting to a strange world far from home. If that’s not you, then this post isn’t for you.

Okay, you think you don’t want to die. But what exactly does that mean?

“You” are the time sequence of mental states that results from a certain large signal processing system: your “brain.” Each small part in this system takes signals in from other parts, changes its local state in response, and then sends signals out to other parts. At the border of this system, signals come in from “sensors”, e.g., eyes, and are sent out to “actuators”, e.g., hands.

You have differing mental states when these signals are different, and you live only as long as these signals keep moving. As best we can tell, from all the evidence we’ve ever seen, when these signals stop, you stop. When they stop for good, you die. As your brain is made out of completely ordinary materials undergoing quite well understood physical processes, all that’s left to be you is the <em>pattern</em> of your brain signals. That’s you; when that stops, you stop. (So yes, patterns [feel](feels-data-is-in).)<span id="more-32292"></span>

This signal processing system that produces you happens to be an organ of a particular complex multi-cellular animal: a “human”. This animal keeps working as long as all its organs keep providing their functions to each other. Your brain provides the function of monitoring and controlling high level decisions in this animal. Other organs provide functions like structure (bones), force (muscles), resource and waste distribution (blood), gas intake and disposal (lungs), light to signal conversion (eyes), and so on.

Given peace and prosperity, thankfully the norm for us today, the death of a multi-cellular animal like you typically results from the worn-down failure of just as one key part in one key organ, usually not the brain. At that moment of death, all the other organs, and all other parts of that failing organ, are still working acceptably. Unlike the artificial systems that we make today, natural animal systems usually can’t deal with broken parts by swapping in identical replacements from large stockpiles.

If you lived like most animals, you’d have to rely almost completely on the natural abilities of your organs to repair failing parts and to use redundant parts to replace failing ones. However, you, lucky you, are part of an advanced technical civilization, which offers many additional ways to help prevent and fix organ failures. For example, we can sometimes artificially clean out parasites and random garbage that gets in the way.

And at other times, we can substitute artificial parts and systems to replace failing natural organs. For example, we can ingest chemicals made in factories when our organs fail to make those chemicals. We have made artificial bones to give structure, artificial lungs to exchange gas, artificial hearts to pump blood, and so on. We’ve even made artificial extensions of some signal systems, such as artificial eyes and ears and artificial limb controllers.

Of course there are many organs for which we have not yet made artificial substitutes. But progress is rapid, and we have great reason for hope in the long run. Our civilization has come very far in a relatively short time, and it is quite reasonable to hope that within a few thousand years at most our descendants will have vastly greater capabilities. There’s no good reason to think we can’t eventually make artificial substitutes for all human organ systems. Yes, our civilization <em>might</em> destroy itself, or halt its growth, but there’s a good chance we won’t.

Now you might wonder: how is it even possible for us today to create artificial substitutes for natural organs? After all, our bodies are incredibly complex and detailed on atomic scales, yet we stand meters tall. If our artificial substitutes had to have a complexity that matched our existing complexity in fine detail over the entire scale of our bodies, then to design artificial organs we’d need to understand and design systems of a complexity vastly beyond anything we have ever managed.

What saves us is modularity. Evolution also had a limited ability to handle design complexity. So it used a modular system design. When there are only a few key organs systems, and when each organ system provides only a few main functions, then design variations within each organ can focus on how they effect the key functions of that organ, and mostly ignore other functions. And that means that our artificial design efforts can also focus mainly on how our replacements effect those key functions.

In addition, there’s the key fact that we are multi-cellular animals. Long ago, evolution spent a very long time honing single-celled creatures to survive and thrive on their own in the wild. Those designs became very subtle and efficient, but after a while because also hard to greatly change. When ecological niches opened up later for much larger organisms, evolution couldn’t be bothered to redo single cell type designs for much larger scales. That would require re-thinking many issues through nearly from scratch. Evolution instead just pasted many small cells together to make large organisms. So now each of our organs is mostly made up of many small cells pasted together.

In our organs, each cell is modified only a bit to allow it to serve its key function of allowing its organ system to provide its key functions. The vast majority of the complexity of each cell is mostly left alone. This is enormously wasteful in the sense that most of that cell complexity isn’t actually very useful in helping that cell to do what it needs to as part of that organ. But evolution found it easier to mostly just leave that cell package alone, rather than to try to greatly redesign cells for their roles in differing body systems.

The net result is that our bodies, and our organs, are actually much simpler than all that atom-scale complexity suggests. (Not simple of course, but <em>simpler</em>.) Which is why we can often design artificial replacements for key organs, to let us live longer before the failure of some part of an organ keeps our whole body from working, and thus killing the brain which produces the sequence of mental statues which is you.

Consider now the possibility of an artificial replacement for the brain organ. If we could make a work-alike for the brain, we might put it into a natural body, to prevent brain failures from killing that whole body. But if the main thing you really care about is your brain, where <em>you</em> live, then a much more dramatic anti-dying solution presents itself: replacing <em>all</em> the other body organs with artificial versions. Once you can make an artificial brain, that’s actually <em>much</em> easier than replacing one organ at a time. With artificial eyes, ears, muscles, energy sources, etc. we might make artificial bodies that can last as long as our artificial machines do today: indefinitely.

Today we can keep large complex systems like cars and buildings working indefinitely, by replacing parts as they break. Of course we don’t always do that, as we often find it cheaper to toss old systems and make new systems from scratch. We can even keep very complex computer systems working indefinitely, by making archive copies of files, and moving files to new computers as old ones die or become obsolete. With artificial bodies, we might similarly keep your mind going via replacing other body parts and swapping your mind to new brain hardware as needed. Not forever, but plausibly a lot longer than the current limit of roughly a subjective century.

So what are the prospects for making artificial brains? Unless brains are quite strange compared to other organs, we expect that most of the complexity in each brain cell is not very relevant for the function that cell provides the rest of the brain. Evolution searched for and found modest modifications of existing cell designs, modifications which let cells provide their key brain functions: sending signals to each other, and storing internal states that change in response to those signals.

When humans have designed signal processing systems, we have usually tried hard to [insulate](signal-processors-decouple) (in signal terms) the degrees of freedom in such systems that represent signals and states from all the other physical degrees of freedom in those systems. For example, in electronics we have wires and devices separated by insulators. It makes sense for evolution to also try this, for similar design reasons.
So when we find some signals and states in the brain, we should be able follow them to other strongly connected signals and states, and mostly ignore the much larger and more complex relatively isolated nearby parts of the system. This is like following the wires in a piece of electronics, allowing one to ignore all the parts of the system not collected by something like wires to the wired parts. This strategy usually works great for electronics, and when applied to our bodies has already enabled us to create artificial eyes and ears.

So far, we have started in on this wire-tracing task regarding brains, but are far from having finished. In fact, completion may take a century or more. We can clearly see some signals and states, and have traced them to some others, but have not obviously found them all, nor worked out all the local transformation mappings that map incoming signals to changed states and outgoing signals.

Thus it [seems](how-does-brain-code-differ) [reasonable](brains-simpler-than-brain-cells) to expect that we will eventually be able to “follow the wires”, and figure out where human brains store their signals and states relevant for brain functions, and learn (to a sufficient accuracy) the signal-state-signal transformation mappings for all the different parts of the brain. Knowing this could enable us to scan a particular functioning human brain, seeing which kinds of cells are where connected to what, and create a work-alike of that particular brain in artificial hardware. If that was your brain, with your signal patterns, then then new artificial brain would be you as well. (And perhaps the only you remaining if the scanning process destroyed your original brain.)
Once you were in an artificial brain, how long til you died would become an economic question, not a biological one. You could live for as long as you could afford new replacement parts, and remained part of a civilization available to sell you such parts, and to protect you from predation. To someone like you, who doesn’t want to die, that should seem a very intriguing prospect.

Furthermore, the facts that you are a [multi-cellular](humans-cells-in-multicellular-future-minds) animal with a body design based on modular organs suggests that this isn’t some distant future in billions of years after inconceivable growth; it could happen in the next few centuries. Brains are probably more complex than the other organs for which we have already created artificial substitutes, but not fantastically more complex. So the fact that we have created many artificial organs in the last century or so suggests we might we make artificial brains within a few centuries.
Of course the obvious problem for you today is that we don’t know how to make artificial brains today, and your existing body will probably fail well before the centuries it may take to figure out how to make artificial brains. So how does any of this help you not die soon?

Well, consider the fact that we’ve seen cold people where all the usual apparent brain activity has stopped, and yet they revived just fine when warmed up soon afterward. This tells us that the brain scans that will be needed to create artificial brains can mostly ignore contributions to temporary brain activity. Scans just need to see the more permanent structures that channel temporary activity.

And that raises this key question: does the relevant info that a brain scan will need to see you in your brain, to allow the creation of an artificial brain holding you, does this key info survive the process of freezing your brain down to liquid nitrogen temperatures? At those temperatures, your brain is solid, and basically doesn’t change, for many centuries at least. So if the key info that specifies you survived that freezing process, why then we might freeze you now, keep you frozen for a long time, and then later when we know how to make artificial brains we could scan your frozen brain, grab that key info, and then make a new you in an artificial brain! <em>That’s</em> the relation between all this and your not dying.

Yes, it would cost some money to freeze you soon, to store you for centuries, and then to revive you later. Someone will have to pay for all that. And many things can go wrong over such timescales. Whatever contracts and organizations you support to enable this plan may fail. And even with success you’d come back to a strange different world, one that may lack many people you love. But the costs and risks would come way down if many people wanted this, and you said you are a person who wants to live, even if the chances are low and you’d have to live in a strange world.

We’ll talk more about these social issues in a bit, but before that we should engage this key info question: does the info that specifies you survive freezing your brain? We are pretty sure that the info that specifies you is there in your brain right now, encoded in the spatial patterns of densities of certain chemicals. (That is, we are pretty sure the needed info is <em>not</em> in orientations, spins, electric currents, magnetic fields, etc.) Perhaps many chemicals, though a tiny fraction of all the chemicals in your brain. So if we could suddenly and instantly freeze your brain, keeping all the chemicals in the same place, so that nothing moved afterward, then we’d be pretty sure to have saved the key info.

Unfortunately, changes do happen during the available freezing processes. Some unusual chemical reactions occur, and some things move around. However, we have many reasons to believe that enough info can be saved for it to be extractable from a frozen brain at a reasonable cost. Yes, in principle <em>all</em> info is saved in the universe, and could be regained by a powerful enough entity that could inspect the entire universe in great detail. But we don’t want to pin our hopes on such an extreme and unlikely-looking solution.

When we use our best current freezing tech, and then look via powerful microscopes at post-freezing tissues, most everything seems to be nearly the same, in nearly the same place. Also, we expect a lot of redundancy in brain design and encoding. We know that our organs are designed to function with great randomness in their detailed construction process. We seem to see a lot of redundancy in brain encoding, and know that the brain still functions under many big modifications, such as being whacked in the head or flooded with new chemicals. And we know that a modest rate of errors that require best-guess interpolation to fix seems tolerable; many things now change who we are over the decades, yet we still see ourselves as the same person enough to be worth preserving.

Okay, so it seems there’s a good chance that artificial brains will be possible within a few centuries, and so if we use our best current tech to freeze you today, and store that frozen brain for centuries, then future folks could make an artificial brain of you, and thus something close enough to you could be revived and live in that future world. Given that you don’t want to die, this plan should seem attractive to you.

Not just you, many millions like you. Sure, people vary in how much they don’t want to die, and in their beliefs on future tech progress and brain info freezing damage. But given what people say about how much they value life, a great many should find this attractive. At a typical value of life of five million dollars, then if this plan cost $50 thousand then only a 1% chance of success should be enough to make it seem worthwhile. Or if a strange future life was only worth a fifth of an ordinary life today, then a 5% chance of success should be enough.

Amazingly, a way to freeze your body when medicine gives up on you, and to preserve this frozen body for centuries with at least this chance of success, has been available at near this price for over six decades. Yet even though this option has regularly received free international publicity during this period, only about 3000 people have ever signed up for it, and only about 300 people have ever been frozen. Why so little interest?

One possible explanation is that this “cryonics” option has usually been framed around about a more ambitious plan: the full repair of your ordinary biological body, including all the damage done by freezing, and including the growing of a replacement body if they froze only your brain. While such full repair will probably be feasible eventually, this task seems <em>much</em> harder than creating an artificial brain and body. This really does require handling all the complexity of meter-scale and atomic-scale-detailed bodies, at least for the purpose of identifying failures and repairing them.

I’ve thought a lot about what life would look like given artificial brains, and in fact have written a whole book on it: <a href="http://ageofem.com"><em>The Age of Em</em></a>. Since the artificial brain option seems to me better, and should be available much earlier, that’s the option I’d prefer. Even so, I don’t think we can explain very much of the lack of interest in cryonics in terms of this option being less highlighted so far. The full repair option isn’t crazy, and should still be attractive to people who don’t want to die.

In my other book, <a href="http://elephantinthebrain.com"><em>The Elephant in the Brain</em></a>, I explain how we humans are often wrong about our motives. In particular, regarding medicine we think we care about health, yet we actually mainly care about showing that we care about others, and letting them show they care about us. As a result, far more of us actually die faster than we would under a system that was actually driven mainly by a concern for not dying. And yes that probably applies to you as well; <em>you</em> will probably die sooner than you would if your attitude toward medicine were driven mainly by your not wanting to die.

Perhaps your conscious mind is now screaming “No, that’s not me, <em>I</em> really don’t want to die.” But those thoughts aren’t very honest. They might feel sincere, but they aren’t integrated into a whole mental system that actually works that way. When thoughts of death come to your mind, your mind freezes and goes crazy just like everyone else’s, and can’t think death threats through very carefully. You also tell yourself that on something this important you don’t care much what others think, and you’d do whatever it takes even in the face of criticism and conformity pressures. But if your plan to not die faces strong opposition from loved ones, you’ll usually cave.

Regarding the option to not die that I’ve been discussing here, freezing to await artificial brains, your mind has either latched on to some lame excuse why this plan is infeasible or uninteresting, or you’ve told yourself yeah you do want to do this, and you’ll make a vague plan to do so, but you won’t actually do it. Perhaps your loved ones will [object](modern-male-sati), and you’ll cave. Remember: a significant fraction of folks who hear about cryonics plan to sign up, but almost none ever do. (Amazingly, a similar number of people pay a similar amount to have their remains burned and their ashes [thrown](space-ashes-vs-cryonics) into space, an option produces far less hostility from loved ones.)
Perhaps your mind is still screaming “No, that’s not me, I control my plans, and I don’t want to die!” Well then, go ahead, prove me wrong, <a href="https://ebf.foundation">and</a> <a href="https://alcor.org">join</a> <a href="https://www.cryonics.org">the</a> 3000. Make me proud of you. (That 3000 is only 10% of my Twitter followers; if I could double that number to 6000, I’d consider my life well spent.)

Is your doubt [of](cryonics-as-charity) [the](brin-says-cryonics-selfish) form, “But isn’t this selfish of me, to spend money on <em>my</em> not dying?” Note that we all supposedly spend lots on ourselves via medicine to not die. And as this cryonics product is dominated by [fixed](cryonics-as-charity) costs, the more who sign up to be frozen, the cheaper and more reliable this product can become.
This product happens to be unpopular now, but no law of nature requires this. If we could switch to a new social equilibrium where people used cryonics to show that they care, then at a quite modest cost most everyone who is alive today need not die! Isn’t <em>that</em> a noble altruistic cause worth fighting for?

Look, large surveys [show](today-ems-seem-unnatural) that the main reason people actually reject artificial brains is because they seem “unnatural”, the same reason people rejected IVF before that was possible, and yet now widely accept it:
> People who value purity norms and have higher sexual disgust sensitivity are more inclined to condemn mind upload. Furthermore, people who are anxious about death and condemn suicidal acts were more accepting of mind upload. Finally, higher science fiction literacy and/or hobbyism strongly predicted approval of mind upload.

Whatever reasons you think you have in your head are mainly excuses for these factors. Will you really let yourself die because a purity norm now makes a thing as yet unseen seem unnatural to you?

## [How Does Brain Code Differ?](#table-of-contents)
_Posted on 2019-01-21_

<strong>The Question</strong>

We humans have been writing “code” for many decades now, and as “software eats the world” we will write a lot more. In addition, we can also think of the structures within each human brain as “code”, code that will also shape the future.

Today the code in our heads (and bodies) is stuck there, but eventually we will find ways to move this code to artificial hardware. At which point we can create the world of brain emulations that is the subject of my first book, <a href="http://ageofem.com"><em>Age of Em</em></a>. From that point on, these two categories of code, and their descendant variations, will have near equal access to artificial hardware, and so [will](can-human-like-software-win) [compete](humans-cells-in-multicellular-future-minds) on relatively equal terms to take on many code roles. System designers will have to choose which kind of code to use to control each particular system.
When designers choose between different types of code, they must ask themselves: which kinds of code are more cost-effective in which kinds of applications? In a competitive future world, the answer to this question may be the main factor that decides the fraction of resources devoted to running human-like minds. So to help us envision such a competitive future, we should also ask: where will different kinds of code work better? (Yes, non-competitive futures may be possible, but [harder](coordination-is-hard) to arrange than many imagine.)
To think about which kinds of code win where, we need a basic theory that explains their key fundamental differences. You might have thought that much has been written on this, but alas I can’t find <a href="http://scienceblogs.com/developingintelligence/2007/03/27/why-the-brain-is-not-like-a-co/">much</a>. I do sometimes come across people who think it obvious that human brain code can’t possibly compete well anywhere, though they rarely explain their reasoning much. As this claim isn’t obvious to me, I’ve been trying to think about this key question of which kinds of code wins where. In the following, I’ll outline what I’ve come up with. But I still hope someone will point me to useful analyses that I’ve missed.

In the following, I will first summarize a few simple differences between human brain code and other code, then offer a deeper account of these differences, then suggest an empirical test of this account, and finally consider what these differences suggest for which kinds of code will be more cost-effective where.<span id="more-32019"></span>

<strong>Differences</strong>

The code in our heads is the product of learning over our lifetimes, inside a biological brain system that has evolved for eons. Though brain code was designed mainly for old problems and environments, it represents an enormous investment into a search for useful code. (Even if some parts seem simple, the whole system is [not](how-good-99-brains).) In contrast, the artificial code that we’ve been writing started from almost nothing a few decades ago.
Our brain code seems to come in a big tangled package that cannot easily be broken into smaller units that can usefully function independently. While it has identifiable parts, connections are dense everywhere; brains seem less modular than artificial code. Relatedly, brains seem much more robust to local damage, perhaps in part via having more redundancy. Brains seem designed for the case where communication is relatively fast and cheap within a brain, but between brains it is far more expensive, slow, and unreliable.

The code in our head does not take much advantage of many distinctions that we often use in artificial code. In our artificial systems, we gain many advantages by separating hardware from software, learning from doing, memory from processing, and memory addresses from content. But it seems that evolution just couldn’t find a way to represent and act on such distinctions.

Artificial code seems to “[rot](why-does-software-rot)” more quickly. That is, as we adapt it to changing conditions, it becomes more fragile and harder to usefully change. As a result, most of the code we now use is not an edited version of the first code that accomplished each task. Instead, we repeatedly re-write similar systems over from scratch. In contrast, while the aspects of brain code that we learn over a lifetime do seem to slowly rot, in that we become less mentally flexible with age, we see [little](more-than-death-fear-decay) evidence of rot in the older evolved brain systems that we use to learn.
Our brain code is designed for hardware with many parallel but slow computing units, while our artificial code has so far mostly been designed for fewer fast and more sequential units. That is, brains calculate many things all at once slowly, while most artificial code calculates one thing at a time.

Our brains do some pre-determined tasks quickly in parallel. Such as simultaneously recognizing both visual and auditory signs of a predator. However, when we humans work on the tasks that most display our versatile generality, the sort of tasks that are most likely to matter in the future, our brains <a href="https://www.penguin.co.uk/books/285/285465/the-mind-is-flat/9780241208779.html">mostly</a> function both slowly and sequentially. That is, we accomplish such tasks by proceeding step by step, and at each step our whole brain works in parallel by adding up many small contributions to that step.

Even so, the power and generality that often results from this process is truly stunning, being far beyond anything we know how to achieve with artificial code, no matter how much hardware we use and how many man-years we devote to writing it. This generality is why humans brains still earn the vast majority of “wages” in our economy. Artificial code is quite useful but gets paid much less in the aggregate, and in this sense is still far less useful than brain code. (“AI” software, i.e., artificial software intended to more effectively mimic brain abilities, earns a much smaller fraction of aggregate wages.)

While the code in our heads resulted largely from simple variation and selection of whole organisms, human brains use more directed processes to generate the artificial code. For example, one common procedure is to repeatedly have a brain imagine the results of particular small sections of code being executed in particular contexts, and search for edits to code such that imagined executions produce desired outcomes. This is commonly interleaved with less frequent actual execution of trial code on test cases. This process works better with more modular code expressed in terms of logical concepts that have sharp boundaries and implications, as it is easier for our brains to predict what happens in such contexts. A few parts of artificial code are generated via statistical analysis of large datasets.

When we have invested lately in having a kind of code actually do a task, such investments tend to give an advantage to that kind of code in continuing to do that task. Also, when each type of code can more easily connect to, or coordinate with, other code of its type, each type of code gains an advantage at a task when more of the tasks that it must coordinate with are also done by the same type of code. Thus each type of code has momentum, in continuing on where it was, and it naturally clumps together, especially in the most highly clumped sections of the [network](a-tangled-task-future) of tasks.
<strong>Easy Implications</strong>

So what do these various considerations imply about which kinds of code win where? We can identify some weak “all else equal” tendencies.

Brain code has at least a small temporary advantage on tasks that have been done by brain code lately, and that must coordinate with many other tasks done by brain code. The fact that that brain code requires an entire brain when being general suggests that artificial code is more cost-effective on small simple problems where brains do not have special abilities. At least when hardware costs to run code are important relative to costs to write code. Artificial code also seems much cheaper to run when a relatively simple sequential algorithm is sufficient, as brain code uses a whole brain to execute simple sequential computations. Artificial code also has advantages when lots of fast precise communication is desired across scopes larger than a brain.

The fact that brain code was designed for old problems suggests that it has advantages on old and long-lasting problems, relative to new problems. On the one hand, brain code being old and old systems being less flexible suggests using artificial code when great adaptability is required beyond the range for which brains were designed. On the other hand, the fact that artificial code rots more quickly suggests that artificial code has advantages when problems or contexts change quickly, which would force new code soon in any case, but disadvantages for stable long-lasting tasks.

In addition to these relatively simple and shallow implications, I have found a somewhat deeper perspective that seems useful. Let me explain.

<strong>Code Principles</strong>

Two key principles for managing code are: abstraction and modularity. With abstraction, you cut redundancy, via doing the same tasks but replacing many similar systems with a smaller number of more abstracted systems. Abstracted systems are smaller, in a code-length sense, though they may cost more in hardware to execute their code. By avoiding redundancy, abstracted systems make more efficient use of efforts to modify them.

With modularity, you try to limit the dependencies between subsystems, so that changes to one subsystem force fewer changes to other subsystems. Better ways to integrate and organize systems, which better “carve nature at its joints”, allow more effective modularity, and thus fewer design change cascades, wherein a change in one place forces many changes elsewhere.

It can take a lot of work to search for better design architectures that better facilitate modularity. Given the usual rate at which artificial code rots, only a limited amount of work here is justified. Sometimes systems that have partially rotted are “refactored”, by changing their high level architecture. Though expensive, such refactoring can cut the level of system rot, and is often cost-effective in terms of delaying a complete system rewrite. Better abstractions tend to promote better organization, which induces more effective more-slowly-rotting modularity.

<strong>A Deep Difference</strong>

Today, humans wanting code to do a task will first search for close-enough pre-existing code that might be lightly edited to do the job. Absent that, they will think for a bit, open a blank screen, and start typing. They will connect their new code to existing code as convenient, but will be wary of creating too many dependencies that reduce modularity. They will think and search for good ways to organize this new system, including good abstractions, but will put only limited effort into this. That is, to manage code complexity, humans tend to make new code that is highly modular, but not very well organized.

In contrast, as evolution designed and redesigned brains, it faced strong limits on the amount of brain hardware available. Brains take precious volume and are energetically expensive. And because evolution never managed to separate hardware and software, this hardware limit created strong limits on the amount of software possible. Limits far more restrictive than the memory limits imposed on humans who write code today. To add new software to brains, evolution could only a) add more hardware, b) delete old software, or c) seek more efficient representations and abstractions in order to save space.

Thus evolution just could not take the usual human approach of just opening a blank screen and writing new highly-modular but sloppily-organized code. Evolution instead had to keep searching hard for better ways to organize and integrate existing code, including better abstractions. This was a much more expensive process, but as it played out over eons it resulted in code that was much better organized and integrated, though less modular.

This perspective helps us to understand why brain code seems less modular than artificial code, why brain code doesn’t rot as fast and is more robust to damage, why it is harder to usefully break brain code into small units to do small tasks, why brain code is better at being more general, and why artificial code is more sequential. It also helps us understand why the usual focused processes for having brains make artificial code work reasonably well: brains know enough to predict how small chunks of code will behave, but only when that code is relatively modular.

This perspective also helps us to understand why abstraction is one of the brain’s key organizing principles. As I’ve [said](separate-top-down-bottom-up), human brains
> collect things at similar levels of abstraction. The rear parts of our brains tend to focus more on small near concrete details while the front parts of our brain tend to focus on big far abstractions. In between, the degree of abstraction tends to change gradually.

Another important if less well understood organizing principle of brains is to separate a left and a right brain, [perhaps](separate-top-down-bottom-up) to separate systems of credit assignment that don’t mix well. That is, to separate bottom-up processing that searches for fewer big things to explain many details, from top-down processing that searches for details to best achieve abstract goals.
In sum, a deeper perspective can help us to understand how brain and artificial code differ, and thus which kinds of code can win where: <em>Brain code is better integrated and abstracted, but less modular, than artificial code</em>.

<strong>Code-Cubed</strong>

Let me suggest a way to test this perspective, via data that should already be available, but which I haven’t yet found. In addition to brain code written by evolution, and artificial code written by brains, we can also consider “code-cubed”, i.e., code that is written by artificial code. This is “cubed’ because it is written by artificial code, which we can think of as “code-squared”, which is written by brain code, which we can think of as plain code, which is written by a non-code evolutionary process. Such code-cubed can obviously be written much more quickly than can ordinary code, at least at low levels of quality. But how else does it differ?

A large well-integrated brain that focuses its whole effort on thinking about particular chunks of code should produce in that artificial code a substantial degree of coherence and integration, at least on the scale of those chucks. However, when more modular and less well integrated artificial code writes code, that resulting code-cubed should be less well integrated. And as artificial code can write code much faster than can humans, and has plenty of empty memory available, artificial code will be tempted all the more to rely on new code and modularity to help manage complexity in the code it writes.

Thus this perspective suggests that code-cubed is even more modular, less well organized, and rots more quickly, than ordinary artificial code written by humans. For example, when we change the source code or compiler for a system, we then typically re-execute that complier on the source code, in effect re-writing from scratch. We do this instead of trying to edit the old compiled code in order to match the new source or new compiler.

Thus when we want code that can be usefully modified over longer periods of change, we should prefer ordinary artificial code to code-cubed. We should also prefer this when it is important that the code writer had a wider more general understanding of the task to be performed, and the other systems with which it needs to integrate.

<strong>What Wins Where</strong>

So in a future world where all types of code have access to the same cheap artificial hardware, and where competition pushes each application to use the type of code that is most locally cost-effective there, where should we expect to find brain code, where artificial code, and where code-cubed?

<p style="text-align: left;">Brain code represents an enormous investment into a large tangled but well integrated and abstracted package that is hard to understand and modify, but has so far shown unparalleled power when applied to stable general broad tasks. This [suggests](humans-cells-in-multicellular-future-minds) that brain code may have a long future in applications that play to its strengths. (Long at least [in terms](reply-to-christiano-on-ai-risk) of my usual favorite parameter: number of economic doubling times.)  Yes, eventually fully artificial code may become comparably well-integrated, but if ems are possible before then, the descendants of brain code may then have become even better organized and designed.
Most human organizations today are hierarchical, with low level activity focused more on relatively narrow contexts that matter less, need faster responses, and evolve more rapidly. In contrast, higher level activity allows slower responses, has more stable considerations, and must consider broader scopes and implications. Most artificial code today is also hierarchical, with low level code that tends to have a more narrow focus and contexts that change more rapidly, compared to high level code that must consider a wider range of more stable contexts, inputs, and other systems. In both types of systems, lower level tasks are more naturally modular, depending on fewer other tasks.

As smaller more focused more modular tasks that change more rapidly are better suited to artificial code, while less modular tasks that must consider wider context are better suited to brain code, we should expect artificial code to be more common at low organization levels, and brain code to be more common at high organization levels. That is, brains will manage big pictures, while artificial code manages details.

Among the tasks that humans do today, we can also distinguish more vs. less [tangled](a-tangled-task-future) [tasks](connected-tasks-cities-win). Tangled tasks are closer to the more tangled center of a network of which tasks must coordinate with which other tasks. While tasks at higher organization levels do tend to be more tangled, some lower level tasks are also highly tangled. Brains also have advantages in these more tangled tasks, and once they are entrenched in many connected tasks are harder to displace from them.

## [Progeny Probabilities: Souls, Ems, Quantum](#table-of-contents)
_Posted on 2019-06-02_

Consider three kinds of ancestry trees: 1) souls of some odd human mothers, 2) ems and their copies, and 3) splitting quantum worlds. In each kind of tree, agents can ask themselves, “Which future version of me will I become?”

<strong>SOULS </strong> First, let’s start with some odd human mothers. A single uber-mother can give rise to a large tree of descendants via the mother relation. Each branch in the tree is a single person. The leaves of this tree are branches that lead to no more branches. In this case, leaves are either men, or they are women who never had children. When a mother looks back on her history, she sees a single chain of branches from the uber-mother root of the tree to her. All of those branches are mothers who had at least one child.

Now here is the odd part: imagine that some mothers see their personal historical chain as describing a singular soul being passed down through the generations. They believe that souls can be transferred but not created, and so that when a mother has more than one child, at most one of those children gets a soul.

Yes, this is an odd perspective to have regarding souls, but bear with me. Such an odd mother might wonder which one of her children will inherit her soul. Her beliefs about the answer to this question, and about other facts about this child, might be expressed in a subjective probability distribution. I will call such a distribution a “progeny prob”.

<strong>EMS </strong> Second, let’s consider ems, the subject of my book <a href="http://ageofem.com"><em>The Age of Em: Work, Love, and Life when Robots Rule the Earth</em></a>. Ems don’t yet exist, but they might in the future. Each em is an emulation of a particular human brain, and it acts just like that human would in the same subjective situation, even though it actually runs on an artificial computer. Each em is part of an ancestry tree that starts with a root that resulted from scanning a particular human brain.

This em tree branches when copies are made of individual ems, and the leaves of this tree are copies that are erased. Ems vary in many ways, such as in how much wealth they own, how fast their minds run relative to humans, and how long they live before they end or next split into copies. Split events also differ, such as re how many copies are made, what social role each copy is planned to fill, and which copies get what part of the original’s wealth or friends.

An em who looks toward its next future split, and foresees a resulting set of copies, may ask themselves “Which one of those copies will I be?” Of course they will actually become all of those copies. But as human minds never evolved to anticipate splitting, ems may find it hard to think that way. The fact that ems remember only one chain of branches in the past can lead them to think in terms of continuing on in only one future branch. Em “progeny prob” beliefs about who they will become can also include predictions about life details of that copy, such as wealth or speed. These beliefs can also be conditional on particular plans made for this split, such as which copies plan to take which jobs.

<strong>QUANTUM </strong> Third, let’s consider quantum states, as seen from the many worlds perspective. We start with a large system of interest, a system that can include observers like humans and ems. This system begins in some “root” quantum state, and afterward experiences many “decoherence events”, with each such event aligned to a particular key parameter, like the spatial location of a particular atom. Soon after each such decoherence event, the total system state typically becomes closely approximated by a weighted sum of component states. Each component state is associated with a different value of the key parameter. Each subsystem of such a component state, including subsystems that describe the mental states of observers, have states that match this key parameter value. For example, if these observers “measured” the location of an atom, then each observer would have a mental state corresponding to their having observed the same particular location.<span id="more-32133"></span>

These different components of a quantum state sum can thus be seen as different “worlds”, wherein observers have different and diverging mental states. Decoherence events can thus be seen as events at which each quantum world “splits” into many child worlds. The total history starting from a root quantum state can be seen as a tree of states, with each state containing observers. And so a quantum history is in part a tree of observers. Each observer in this tree can look backward and see a chain of branches back to the root, with each branch holding a version of themselves. More versions of themselves live in other branches of this tree.

After a split, different quantum worlds have almost no interaction with each other. Which is why we never notice this quantum splitting process in the world around us. So observers typically never see any concrete evidence of that there exist other versions of themselves, other than their past versions in the chain from them now back in time to the root state. That is, we never see other quantum worlds. As observers see only a sequence of past versions of themselves, they can naturally expect to see that sequence continue into the future.

That is, observers typically ask “In the future, what will be the state of the one world, including the one version of my mind?” Even though in fact there will be many worlds, holding many versions of their minds. (Quantum frameworks other than many worlds struggle to find ways, usually awkward, to make this one future version claim actually true.) Beliefs about this “who will I be?” question are thus “progeny probs”, analogous to the beliefs that an em might have about which future copy they will become, or that an odd human mother might have on which future child inherits her soul.

The standard Born rule in quantum mechanics is usually expressed as such a progeny prob. It says that if the current state splits into a weighted sum of future states, one should expect to find oneself in each component of that sum with a chance proportional to the square of that state’s weight in the sum. This is a remarkably simple and context-independent rule. Technically, quantum states are vectors, and the Born rule uses the L2 norm for relative vector size. And a key question about many worlds quantum theory, perhaps <em>the</em> key question, is: from where comes this rule?

<strong>IN GENERAL </strong> These three cases, of human souls, em copies, and quantum worlds, all have a similar structure. While the real situation is a branching tree of agents, an agent who looks back to see a sequence of ancestors can be tempted to project that sequence forward, predict that they will become only one next descendant, and wonder what that descendant will be like. This temptation is especially strong in the quantum case, where agents never see any other part of the tree than their ancestor sequence, and so can fail to realize that a larger tree even exists.

An agent’s beliefs about which next descendant will “really” be them can be described by a probability distribution, which I’ve called a “progeny prob”. This gives the chance this agent will “really” become a particular descendant, conditional on the details of a situation. For ems, this chance may be conditional on each copy’s wealth, or speed, or job role. For quantum systems, this chance is often conditional on the value of the key parameter associated with a decoherence event.

In the rest of this (long) post, I make three points about progeny probs.

<b>IS FICTION  </b>The first big thing to notice is that, for an agent who is a branch in some tree of agents, there is actually no truth of the matter regarding which future branch that agent will “really” be! They will become <em>all</em> descendant branches in that tree. So one of the most fundamental elements of quantum theory, the Born probability rule, is typically expressed in terms of an incoherent concept. Also incoherent is a big question that ems will often ask, “Who will I be next?”

However, even though progeny probs are in this sense fictional, we usually connect them to some very real data: the past sequence of ancestors we see up until today. Agents who believe that their past history was generated by the same sort of progeny prob that applies to their future should expect this history to be typical of sequences generated by such a progeny prob. This test has in fact been applied to the quantum progeny prob, which passes with high accuracy.

If one has has a detailed enough model of how a certain kind of ancestry tree of observers is generated, then one can use this tree model to predict a probability distribution over possible trees. Each such generated tree comes with a set of ancestor sequences, one for each branch in the tree. So given a distribution over trees, one can generate a distribution over ancestor sequences in these trees.

<strong>IS RELATIVE</strong>  However, in order to take a tree model and generate a distribution over ancestor sequences, one needs to pick some relative branch weights, weights which say how much each branch counts relative to others in that tree. And the progeny prob that best fits this total distribution of ancestry sequences will depend on these relative branch weights.

For example, consider all the ems that descend from some particular human, and consider a late time when there are many such descendants. There are several different ways that one could sample from these late ems to create a distribution of ems. For example, one could repeatedly sample 1) a random memory unit able to store part of the mental state of an em, 2) a random processor able to run part of an em mind, or 3) a random dollar of wealth and pick the em who owns it.

The random processor approach tends to fit better with progeny probs which say that you are more likely to be a descendant who runs faster (and who has descendants who run faster). The random memory approach tends to fit better with progeny probs that count descendants more equally, regardless of speed. And the random dollar of wealth approach tends to fit better with progeny probs that say you are more likely to become the descendants who inherit more wealth from you. Which of their descendants an em should expect to become depends on which of these methods this em thinks makes more sense for weighting future ems.

Each progeny prob predicts the existence a tiny fraction of very weird ancestors sequences, ones quite unlikely to be generated by that progeny prob. But such sequences are only actually rare if this progeny prob fits with the correct distribution. For example, few ems chosen by looking at random memories should have ancestor histories that are weird according to a memory-based progeny prob. But most of them might have ancestors histories that are weird according to processor- or wealth-based progeny probs.

For the quantum case, the standard Born rule progeny prob seems to fit well with distributions that sample from later quantum worlds in proportion to the same L2 norm that the Born rule uses. However, we lack a good widely accepted derivation of this distribution from the basic standard core of quantum mechanics. That is, we can’t explain why we should focus on later quantum worlds in proportion to this L2 norm, and mostly ignore the far larger numbers of quantum worlds that have much smaller values of this norm.

Yes, some try to derive this norm from other axioms, but none of these derivations seems a compelling explanation to me. The L2 norm is so simple that it must be implied by a great many sets of axioms. I’ve proposed a “<a href="http://mason.gmu.edu/~rhanson/mangledworlds.html">mangled worlds</a>” approach, and show that the Born rule can result from counting discrete worlds equally, <em>if</em> we ignore worlds below a size threshold that are mangled by larger worlds, and so are not hospitable to observers. But my proposal is so far [mostly](quantum-quotes) ignored.
<strong>IS COMPLEX</strong>  Finally, it is worth noting that the implicit assumptions of a progeny prob model are typically violated by the reality of even simple tree models. As a result, the best fit progeny prob for a simple tree model can be quite complex.

The progeny prob framework assumes that one and only one “me” travels along some path in the tree. Conditional on being in one branch, the chances that I become each of the child branches must sum to one. And it may seem natural to have those chances be independent of what would have happened had I instead gone to other branches at earlier times.

But even in simple tree models, there is not a fixed total quantity of branches. So when there is more growth in other branches of the tree, this part of the tree shrinks as a fraction of the whole. And so the sum of the weights for the children of a branch do not usually sum to the weight of that branch. And such weights do typically depend on what happens in distant branches that split off long ago.

It thus seems all the more remarkable that the mysterious Born rule progeny prob for quantum mechanics is so simple and context independent.

## [Em Redistribution](#table-of-contents)
_Posted on 2015-07-17_

I’m in the last few weeks of finishing [my book](oxford-to-publish-the-age-of-em) <em>The Age of Em: Work, Love, and Life When Robots Rule The Earth,</em> about social outcomes in a world dominated by brain emulations. As a teaser, let me share some hopefully non-obvious results about redistribution in the em world.
There are [many kinds](unequal_inequal) of inequality. Inequality exists between different species, between generations born at different times, and between nations of the world at a time. Within a nation at a time, there is inequality both between families and within families. There is also inequality across the moments of the life of each person. In all of these cases, there is not only financial inequality, but also inequality in status, prestige, pleasure, lifespan, happiness, and more. There is also inequality between the size of families, firms, cities, or nations, even when individuals within those groupings are equal.
Today, we have relatively little intentional redistribution between generations or between nations. Redistribution within the moments of a person’s life happens, but that is mostly left to that person to choose and to fund. Similarly, redistribution between siblings is mostly achieved via differential treatment by parents. Instead, most concern today about inequality, and most debate about redistribution to address inequality, focuses on one very particular “standard” kind of inequality.

This standard inequality looks at differences in average individual financial incomes between the families of a nation, all at a given time. This type of inequality is actually one of the smallest. For example, in the U.S. today financial inequality between families is only <a href="http://www.nytimes.com/2004/02/14/books/what-runs-in-the-family-isn-t-success.html">one third</a> the size of that inequality between siblings within families, and even that is much less than the inequality between individuals from different nations. We may focus our redistribution feelings on this standard inequality because it seems to us the most analogous to the inequality that [forager](foragers) sharing norms addressed. Alternatively, perhaps it is the most profitable type of redistribution for [opportunistic](inequality-is-about-grabbing) rent-seekers.
This history suggests that the em world will have little redistribution between em generations or city states, and also that each clan is mostly in charge of deciding how to address inequality within that clan. After all, em clan members are more similar and closer to each other than are human siblings, even if they may sometimes be more distant from each other than are typical human life moments. Also, clan members have rather complex relations with each other, making it hard pick a natural sub-clan unit to be the standard basis for counting inequality. So that leaves ems with comparing inequality between clans.

A set of em clans can be unequal in two different ways. One way focuses on individual incomes, or perhaps individual happiness or respect, and says that a clan is better off if its individuals are on average better off. The other way focuses on the overall size and success of a clan. Here a clan is better off if it has more members, resources, or respect. Historically, most redistribution efforts have focused on average individual outcomes. For example, we have seen very little efforts to redistribute between human family clans based on family size. That is, we almost never take from families with many descendants in order to give to families that have few descendants. Nor do we take much from big nations, cities, or firms to give to smaller ones.

Because most em wages are near subsistence levels, unregulated wages have less inequality than do wages today. So em clans naturally have less inequality of the standard sort that is the focus of today’s redistribution. In contrast, em clans have enormous inequality in clan size, resources, and respect. However, history gives little reason to expect much redistribution to address this inequality. It is not very analogous to forager sharing, nor does it lend itself to profitable rent-seeking.

Thus the main kind of redistribution that we have reason to expect in the em era is between the clans of a city, based on differences of average within-clan individual income. But we expect less inequality of this sort in the em world, and so expect less redistribution on this basis.

Income taxes are today one of our main mechanisms for reducing the standard inequality that compares individual incomes between families within a nation. Over the last two centuries, big increases in the top marginal tax rates have <a href="http://behl.berkeley.edu/files/2014/10/WP2014-03_londono_10-3-14.pdf">mostly followed</a> wars where over two percent of the population served in the military. For example, in the U.S. the top marginal tax rate jumped from 15% to 67% in 1917, during World War I. Controlling for this effect, top tax increases have not been correlated with wealth, democracy, or the political ideology of the party running the government. This weakly suggests that the local degree of individual income redistribution between the clans of an em city may depend on the local frequency of large expensive em wars.

If ordinary humans are included straightforwardly in the redistribution systems of the em world, then the simple result to expect is transfers, not only away from richer humans, but also from humans to ems overall. After all, in purely financial terms typical ems are poorer than the poorest humans. Redistribution systems may perhaps correct for the fact that em subsistence levels are much lower than are human subsistence levels. But if so such systems may also encourage or even require recipients of aid to switch from being a human to being an em, in order to lower costs.

During the em era, humans typically have industrial era incomes, which are much higher than subsistence level incomes. While many and perhaps most humans may pay to create a few ems, they tend to endow them with much higher than subsistence incomes. In contrast, a small number of successful humans manage to give rise to large em clans, and within these clans most members have near subsistence incomes. Thus transfers based on individual income inequality take from the descendants of less successful humans and give to descendants of more successful humans.

## [A.I. Old-Timers](#table-of-contents)
_Posted on 2008-05-31_

Artificial Intelligence pioneer Roger Schank at <a href="http://www.edge.org/q2008/q08_7.html#schank">the </a><em><a href="http://www.edge.org/q2008/q08_7.html#schank">Edge</a>:</em>


> When reporters interviewed me in the 70’s and 80’s about the possibilities for Artificial Intelligence I would always say that we would have machines that are as smart as we are within my lifetime. It seemed a safe answer since no one could ever tell me I was wrong. But I no longer believe that will happen. One reason is that I am a lot older and we are barely closer to creating smart machines.  
>  I have not soured on AI. I still believe that we can create very intelligent machines. But I no longer believe that those machines will be like us…. 
>  What AI can and should build are intelligent special purpose entities. (We can call them Specialized Intelligences or SI’s.)  Smart computers will indeed be created. But they will arrive in the form of SI’s, ones that make lousy companions but know every shipping accident that ever happened and why (the shipping industry’s SI) or as an expert on sales (a business world SI.) … So AI in the traditional sense, will not happen in my lifetime nor in my grandson’s lifetime. Perhaps a new kind of machine intelligence will one day evolve and be smarter than us, but we are a really long way from that. 


This was close to my view after nine years of A.I. research, at least regarding the non-upload A.I. path Schank has in mind.  I recently met Rodney Brooks and Peter Norvig at Google Foo Camp, and Rodney told me the two of them tried without much success to politely explain this standard "old-timers" view at a recent <a href="http://sss.stanford.edu/">Singularity summit</a>.  Greg Egan recently expressed himself more <a href="http://metamagician3000.blogspot.com/2008/04/transhumanism-still-at-crossroads.html">harshly</a>:

 <span id="more-17279"></span> 


> The overwhelming majority [of Transhumanists] might as well belong to a religious cargo cult based on the notion that self-modifying AI will have magical powers. 


The <a href="http://spectrum.ieee.org/jun08/">June IEEE Spectrum</a> is a special issue on singularity, largely skeptical.

My co-blogger Eliezer and I agree on many things, but here [we seem to disagree](biting-evolutio). Eliezer focuses on AIs possibly changing their architecture more finely and easily than humans.  We humans can change our group organizations, can train new broad thought patterns, and could in principle take a knife to our brain cells.  But yes an AI with a well-chosen modular structure might do better.  
Nevertheless, the idea that someone will soon write software allowing a single computer to use architecture-changing ease to improve itself so fast that within a few months the fate of humanity depends on it feeling friendly enough … well that seems on its face rather unlikely.  So many other huge barriers to such growth loom.  Yes it is possible and yes someone should think some about it, and sure why not Eliezer.  But I fear way too many consider this the default future scenario.

<strong>Added:</strong>  To clarify, the standard A.I. old-timer view is roughly that A.I. mostly requires lots and lots of little innovations, and that we have a rough sense of how fast we can accumulate those innovations and of how many we need to get near human level general performance.  People who look for big innovations mostly just find all the same old ideas, which don’t add that much compared to lots of little innovations.

<strong>More added:</strong>  I seem to be a lot more interested in the meta issues here than most (as usual).  Eliezer [seems to think](an-ai-new-timer) that when the when young disagree with the old, the young tend to be right, [because](an-ai-new-timer) "most of the Elders here are formidable old warriors with hopelessly obsolete arms and armor."  I’ll bet he doesn’t apply this to people younger than him; adding in other consideration he sees his current age as near best.  And I’ll bet in twenty years his estimate of the optimal age will be twenty years higher.

## [How Lumpy AI Services?](#table-of-contents)
_Posted on 2019-02-14_

Long ago people like Marx and Engels predicted that the familiar capitalist economy would naturally lead to the immiseration of workers, huge wealth inequality, and a strong concentration of firms. Each industry would be dominated by a main monopolist, and these monsters would merge into a few big firms that basically run, and ruin, everything. (This is somewhat analogous to common expectations that military conflicts naturally result in one empire ruling the world.)

Many intellectuals and ordinary people found such views quite plausible then, and still do; these are the concerns most often voiced to justify redistribution and regulation. Wealth inequality is said to be bad for social and political health, and big firms are said to be bad for the economy, workers, and consumers, especially if they are not loyal to our nation, or if they coordinate behind the scenes.

Note that many people seem much less concerned about an economy full of small firms populated by people of nearly equal wealth. Actions seem more visible in such a world, and better constrained by competition. With a few big privately-coordinating firms, in contrast, who knows that they could get up to, and they seem to have so many possible ways to screw us. Many people either want these big firms broken up, or heavily constrained by presumed-friendly regulators.

In the area of AI risk, many express great concern that the world may be taken over by a few big powerful AGI (artificial general intelligence) agents with opaque beliefs and values, who might arise suddenly via a fast local “foom” self-improvement process centered on one initially small system. I’ve [argued](how-deviant-recent-ai-progress-lumpiness) in the past that such sudden local foom seems unlikely because innovation is rarely that lumpy.

In a new book-length <a href="https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf">technical report,</a> <em>Reframing Superintelligence: Comprehensive AI Services as General Intelligence,</em> Eric Drexler makes a somewhat similar anti-lumpiness argument. But he talks about task lumpiness, not innovation lumpiness. Powerful AI is safer if it is broken into many specific services, often supplied by separate firms. The task that each service achieves has a narrow enough scope that there’s little risk of it taking over the world and killing everyone in order to achieve that task. In particular, the service of being competent at a task is separate from the service of learning how to become competent at that task. In Drexler’s words:<span id="more-32034"></span>

Comprehensive AI services (CAIS) provides a model of flexible, general intelligence in which agents are a class of service-providing products, rather than a natural or necessary engine of progress in themselves. … Strongly self-modifying agents lose their instrumental value even as their implementation becomes more accessible, while the likely context for the emergence of such agents becomes a world already in possession of general superintelligent-level capabilities. …

AI deployment today is dominated by AI services such as language translation, image recognition, speech recognition, internet search, and a host of services buried within other services. … Even applications of AI within autonomous systems (e.g., self-driving vehicles) can be regarded as providing services (planning, perception, guidance) to other system components. … [Service] tasks for advanced AI include:<br/>
• Modeling human concerns • Interpreting human requests • Suggesting implementations • Requesting clarifications • Developing and testing systems • Monitoring deployed systems • Assessing feedback from users • Upgrading and testing systems<br/>
CAIS functionality, which includes the service of developing stable, task- oriented AI agents, subsumes the instrumental functionality of proposed self-transforming AGI agents, and can present that functionality in a form that better fits the established conceptual frameworks of business innovation and software engineering.

Describing AI systems in terms of functional behaviors (“services”) aligns with concepts that have proved critical in software systems development. These include separation of concerns, functional abstraction, data abstraction, encapsulation, and modularity, including the use of client/server architectures—a set of mechanisms and design patterns that support effective program design, analysis, composition, reuse, and overall robustness.

This vision seems built on the 1988 “<a href="https://e-drexler.com/d/09/00/AgoricsPapers/agoricpapers.html">Agoric computing</a>” vision of Drexler and Mark Miller, which Miller has also built on in his computer security work. That vision is of computing systems with a fine-grain breakdown into service-providing modules with separate resources and property rights. As Peter McCluskey <a href="http://www.bayesianinvestor.com/blog/index.php/2019/01/30/drexler-on-ai-risk/">notes</a>, this vision is also related to Drexler’s later nanotech visions:

Drexler’s CAIS proposal removes the “self-” from recursive self-improvement, in much the same way that nanofactories removed the “self-” from nanobot self-replication, replacing it with a more decentralized process that involves preserving more features of existing factories / AI implementations.

McCluskey is only mildly persuaded:

[By] analogies to people … I’m tempted … to conclude that an unified agent AI will be more visionary and eager to improve. … The novelty of the situation hints we should distrust Drexler’s extrapolation from standard software practices (without placing much confidence in any alternative). … He wants humans to decompose [curing Cancer] into narrower goals (with substantial AI assistance), such that humans could verify that the goals are compatible with human welfare (or reject those that are too hard too evaluate). This seems likely to delay cancer cures compared to what an agent AGI would do, maybe by hours, maybe by months, as the humans check the subtasks. … I haven’t thought of a realistic example where I expect the delay would generate a strong incentive for using an agent AGI, but the cancer example is close enough to be unsettling. … Modularity normally makes software development easier … [but] modularity seems less important for ML.

I’ve found two other critics of this new report. <a href="https://www.alignmentforum.org/posts/x3fNwSe5aWZb5yXEG/reframing-superintelligence-comprehensive-ai-services-as">Rohin Shah</a>:

> Typically, we’re worried about the setting where the RL [reinforcement learning] agent is learning or planning at test time, which can happen in learn-to-learn and online learning settings, or even with vanilla RL if the learned policy has access to external memory and can implement a planning process separately from the training procedure. … The lesson of deep learning is that if you can do something end-to-end, that will work better than a structured approach. This has happened with computer vision, natural language processing, and seems to be in the process of happening with robotics. So I don’t buy [Drexler’s vision] — while it seems true that we will get CAIS before AGI since structured approaches tend to be available sooner and to work with less compute, I expect that a monolithic AGI agent would outperform CAIS at most tasks once we can make one.

<a href="http://thinkingcomplete.blogspot.com/2019/01/comments-on-cais.html">Richard Ngo</a>:

> The more powerful each service is, the harder it is to ensure it’s individually safe; the less powerful each service is, the harder it is to combine them in a way that’s competitive with unified agents … Our only example of general intelligence so far is ourselves – a species composed of agent-like individuals who pursue open-ended goals. … Many complex tasks don’t easily decompose into separable subtasks. … Requiring the roles of each module and the ways they interface with each other to be … human-comprehensible will be very uncompetitive … If many AI services end up requiring similar faculties to each other, it would likely be more efficient to unify them into a single entity. … Task of combining [modules] to perform well in new tasks seems like a difficult one which will require a broad understanding of the world. … Agents will eventually overtake CAIS superintelligences because they’ll have more efficient internal structure and will be optimising harder for self-improvement. … The … fuzzy notion of “service” which makes sense in our current context, but may not in the context of much more powerful AI technology.

All these critics seem to agree with Drexler that it is harder to see and control the insides of services, relative to interfaces between them. Where they disagree is in seeing productive efficiency considerations as perhaps creating large natural service “lumps.” A big lumpy service does a large set of tasks with a wide enough scope, where it would be much less efficient to break that up into many services, and where we should be scared of what this lump might do if driven by the wrong values.

Note the strong parallels with the usual concern about large firms in capitalism. The popular prediction that unregulated capitalism would make a few huge firms is based on more than productive efficiencies; people also fear market power, collusion, and corruption of governance. But big size induced by productive efficiencies of scale is definitely one of the standard concerns.

Economics and business have large literatures not only on the many factors that induce large versus small firms, but also on the particular driver of production efficiencies. This often goes under the label “make versus buy”; making something within a firm rather than buying it from other firms tends to make a firm larger. It tends to be better to make things that need to be tightly coordinated with core firm choices, and where it is harder to make useful arm-length contracts. Without such reasons to be big, smaller tends to be more efficient. Because of these effects, most scholars today don’t think unregulated firms would become huge, contrary to Marx, Engels, and popular opinion.

If the worry is that it is dangerous to allow the firms that provide AI services to get very large, then it should be a priority to reduce the many other factors that today encourage large firms in the tech area. These include incentives to create patent pools, fixed costs of complying with regulation, taxing and regulating exchanges between but not within firms, and a lack of common carrier approaches in new network industries.

Alas, as seen in the above criticisms, it seems far too common in the AI risk world to presume that past patterns of software and business are largely irrelevant, as AI will be a glorious new shiny unified thing without much internal structure or relation to previous things. (As predicted by [far views](near-far-summary).) The history of vastly overestimating the ease of making huge firms in capitalism, and the similar typical nubbie error of overestimating the ease of making large unstructured software systems, are seen as largely irrelevant.
McCluskey does briefly considers the possibility of this sort of bias:

Maybe there’s a useful analogy to markets – maybe people underestimate CAIS because very decentralized systems are harder for people to model. People often imagine that decentralized markets are less efficient that centralized command and control, and only seem to tolerate markets after seeing lots of evidence (e.g. the collapse of communism). On the other hand, Eliezer and Bostrom don’t seem especially prone to underestimate markets, so I have low confidence that this guess explains much.

It seems crazy cultish to me to, when guessing if this bias might be a problem, to put much weight on estimating the personal bias-resisting abilities of two particular people. It’s a big world, and they too are human.

Oh, many people are very impressed that current machine learning (ML) systems seem to have less visible or understandable structure than the prior systems that the’ve replaced. But it seems to me a vast exaggeration to conclude from this that future systems of vastly larger ability and scope will have little internal structure. Even today’s best ML systems have a lot of structure, and systems of much larger scope will need a lot more. Across history we’ve seen many changes in the degree of integration of particular kinds of systems, without such changes saying much about any huge global future integration trend.

## [A History Of Foom](#table-of-contents)
_Posted on 2013-01-20_

I had occasion recently to review again the causes of the few known historical cases of sudden permanent increases in capacity growth rates in broadly capable systems: humans, farmers, and industry. For each of these transitions, a large number of changes appeared at roughly the same time. The problem is to distinguish the key change that enabled all the other changes.

For <em>humans</em>, it seems that the most proximate cause of faster human than non-human growth was culture – a strong ability to reliably copy the behavior of others allowed useful behaviors to accumulate via a non-genetic path. A strong ritual ability was clearly key. It also helped to have language, to live in large bands friendly with neighboring bands, to cook and travel widely, etc., but these may not have been essential. Chimps are pretty good at culture compared to most animals, just not good enough to support sustained cultural growth.

For <em>farming</em>, it seems to me that the key was the creation of long range trade routes along which domesticated seeds and animals could move. It was the accumulation of domestication innovations that most fundamentally caused the growth in farmers, and it was these long range trade routes that allowed innovations to accumulate so much faster than they had for foragers.

How did farming enable long range trade? Since farmers stay in one place, they are easier to find, and can make more use of heavy physical capital. Higher density living requires less travel distance for trade. But perhaps most important, transferable domesticated seeds and animals embodied innovations directly, without requiring detailed copying of behavior. They were also useful in a rather wide range of environments.

On <em>industry</em>, the first burst of productivity at the start of the industrial revolution was actually in the farming sector, and had little to do with machines. It appears to have come from “amateur scientist” farmers doing lots of little local trials about what worked best, and then communicating them to farmers elsewhere who grew similar crops in similar environments, via “scientific society” like journals and meetings. These specialist networks could spread innovations much faster than could trade in seeds and animals.

Applied to machines, specialist networks could spread innovation even faster, because machine functioning depended even less on local context, and because innovations could be embodied directly in machines without the people who used those machines needing to learn them.

So far, it seems that the main causes of growth rate increases were better ways to share innovations. This suggests that when looking for what might cause future increases in growth rates, we also seek better ways to share innovations.

Whole brain emulations might be seen as allowing mental innovations to be moved more easily, by copying entire minds instead of having one mind train or teach another. Prediction and decision markets might also be seen as better ways to share info about which innovations are likely to be useful where. In what other ways might we dramatically increase our ability to share innovations?

## [I Still Don’t Get Foom](#table-of-contents)
_Posted on 2014-07-24_

Back in 2008 my ex-co-blogger Eliezer Yudkowsky and I discussed his “AI foom” concept, a discussion that we recently spun off into a [book](debate-is-now-book). I’ve heard for a while that Nick Bostrom was working on a book elaborating related ideas, and this week his <a href="http://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111"><em>Superintelligence</em></a> was finally available to me to read, via Kindle. I’ve read it now, along with a few dozen reviews I’ve found online. Alas, only the two reviews on <a href="http://www.goodreads.com/book/show/20527133-superintelligence#other_reviews"><em>GoodReads</em></a> even mention the big problem I have with one of his main premises, the same problem I’ve had with Yudkowsky’s views. Bostrom hardly mentions the issue in his 300 pages (he’s focused on control issues).
All of which makes it look like I’m the one with the problem; everyone else gets it. Even so, I’m gonna try to explain my problem again, in the hope that someone can explain where I’m going wrong. Here goes.

“Intelligence” just means an ability to do mental/calculation tasks, averaged over many tasks. I’ve always found it plausible that machines will continue to do more kinds of mental tasks better, and eventually be better at pretty much all of them. But what I’ve found it hard to accept is a “local explosion.” This is where a single machine, built by a single project using only a tiny fraction of world resources, goes in a short time (e.g., weeks) from being so weak that it is usually beat by a single human with the usual tools, to so powerful that it easily takes over the entire world. Yes, smarter machines may greatly increase overall economic growth rates, and yes such growth may be uneven. But this degree of unevenness seems implausibly extreme. Let me explain.<span id="more-30855"></span>

If we count by economic value, humans now do most of the mental tasks worth doing. Evolution has given us a brain chock-full of useful well-honed modules. And the fact that most mental tasks require the use of many modules is enough to explain why some of us are smarter than others. (There’d be a common “g” factor in task performance even with independent module variation.) Our modules aren’t that different from those of other primates, but because ours are different enough to allow lots of cultural transmission of innovation, we’ve out-competed other primates handily.

We’ve had computers for over seventy years, and have slowly build up libraries of software modules for them. Like brains, computers do mental tasks by combining modules. An important mental task is software innovation: improving these modules, adding new ones, and finding new ways to combine them. Ideas for new modules are sometimes inspired by the modules we see in our brains. When an innovation team finds an improvement, they usually sell access to it, which gives them resources for new projects, and lets others take advantage of their innovation.

Since software is often fragile and context dependent, much innovation consists of making new modules that are rather similar to old ones, except that they work in somewhat different contexts. We try to avoid this fragility via abstraction, but this is usually hard. Today humans also produce most of the value in innovations tasks, though software sometimes helps. We even try to innovate new ways to innovate, but that is also very hard.

Overall we so far just aren’t very good at writing software to compete with the rich well-honed modules in human brains. And we are bad at making software to make more software. But computer hardware gets cheaper, software libraries grow, and we learn more tricks for making better software. Over time, software will get better. And in [centuries](ai-progress-estimate), it may rival human abilities.
In this context, Bostrom imagines that a single “machine intelligence project” builds a “system” or “machine” that follows the following trajectory:

[](BostromFoom)
“Human baseline” represents the effective intellectual capabilities of a representative human adult with access to the information sources and technological aids currently available in developed countries. … “The crossover”, a point beyond which the system’s further improvement is mainly driven by the system’s own actions rather than by work performed upon it by others. … Parity with the combined intellectual capability of all of humanity (again anchored to the present) … [is] “civilization baseline”.

These usual “technological aids” include all of the other software available for sale in the world. So apparently the reason the “baseline” and “civilization” marks are flat is that this project is not sharing its innovations with the rest of the world, and available tools aren’t improving much during the period shown. Bostrom distinguishes takeoff durations that are fast (minutes, hours, or days), moderate (months or years), or slow (decades or centuries) and says “a fast or medium takeoff looks more likely.” As it now takes the world economy fifteen years to double, Bostrom sees one project becoming a “singleton” that rules all:

The nature of the intelligence explosion does encourage a winner-take-all dynamic. In this case, if there is no extensive collaboration before the takeoff, a singleton is likely to emerge – a single project would undergo the transition alone, at some point obtaining a decisive strategic advantage.

Bostrom accepts there there might be more than one such project, but suggests that likely only the first one would matter, because the time delays between projects would be like the years and decades we’ve seen between when different nations could build various kinds of nuclear weapons or rockets. Presumably these examples set the rough expectations we should have in mind for the complexity, budget, and secrecy of the machine intelligence projects Bostrom has in mind.

In Bostrom’s graph above the line for an initially small project and system has a much higher slope, which means that it becomes in a short time vastly better at software innovation. Better than the <em>entire rest of the world put together</em>. And my key question is: how could it plausibly do that? Since the rest of the world is already trying the best it can to usefully innovate, and to abstract to promote such innovation, what exactly gives one small project such a huge advantage to let it innovate so much faster?

After all, if a project can’t innovate faster than the world, it can’t grow faster to take over the world. Yes there may be feedback effects, where better software makes it easier to make more software, speeds up hardware gains to encourage better software, etc. But if these feedback effects apply nearly as strongly to software inside and outside the project, it won’t give much advantage to the project relative to the world. Yes by isolating itself the project may prevent others from building on its gains. But this also keeps the project from gaining revenue to help it to grow.

A system that can perform well across a wide range of tasks probably needs thousands of good modules. Same for a system that innovates well across that scope. And so a system that is a much better innovator across such a wide scope needs much better versions of that many modules. But this seems like far more innovation than is possible to produce within projects of the size that made nukes or rockets.

In fact, most software innovation [seems](why-does-hardware-grow-like-algorithms) to be driven by hardware advances, instead of innovator creativity. Apparently, good ideas are available but must usually wait until hardware is cheap enough to support them.
Yes, sometimes architectural choices have wider impacts. But I was an artificial intelligence researcher for nine years, ending twenty years ago, and I never saw an architecture choice make a huge difference, relative to other reasonable architecture choices. For most big systems, overall architecture matters a lot less than getting lots of detail right. Researchers have long wandered the space of architectures, mostly rediscovering variations on what others found before.

Some hope that a small project could be much better at innovation because it specializes in that topic, and much better understands new theoretical insights into the basic nature of innovation or intelligence. But I don’t think those are actually topics where one can usefully specialize much, or where we’ll find much useful new theory. To be much better at learning, the project would instead have to be much better at hundreds of specific kinds of learning. Which is very hard to do in a small project.

What does Bostrom say? Alas, not much. He distinguishes several advantages of digital over human minds, but all software shares those advantages. Bostrom also distinguishes five paths: better software, brain emulation (i.e., ems), biological enhancement of humans, brain-computer interfaces, and better human organizations. He doesn’t think interfaces would work, and sees organizations and better biology as only playing supporting roles.

That leaves software and ems. Between the two Bostrom thinks it “fairly likely” software will be first, and he thinks that even if an em transition doesn’t create a singleton, a later software-based explosion will. I can at least see a plausible sudden gain story for ems, as almost-working ems aren’t very useful. But in this post I’ll focus on software explosions.

Imagine in the year 1000 you didn’t understand “industry,” but knew it was coming, would be powerful, and involved iron and coal. You might then have pictured a blacksmith inventing and then forging himself an industry, and standing in a city square waiving it about, commanding all to bow down before his terrible weapon. Today you can see this is silly — industry sits in thousands of places, must be wielded by thousands of people, and needed thousands of inventions to make it work.

Similarly, while you might imagine someday standing in awe in front of a super intelligence that embodies all the power of a new age, superintelligence just isn’t the sort of thing that one project could invent. As “intelligence” is just the name we give to being better at many mental tasks by using many good mental modules, there’s no one place to improve it. So I can’t see a plausible way one project could increase its intelligence vastly faster than could the rest of the world.

(One might perhaps <em>move</em> a lot of intelligence at once from humans to machines, instead of creating it. But that is the em scenario, which I’ve set aside here.)

So, bottom line, much of Nick Bostrom’s book <em>Superintelligence</em> is based on the premise that a single software project, which starts out with a tiny fraction of world resources, could within a few weeks grow so strong to take over the world. But this seems to require that this project be vastly better than the rest of the world at improving software.  I don’t see how it could plausibly do that. What I am I missing?

<strong>Added 2Sep</strong>: See also related posts after: [Irreducible Detail](limits-on-generality), [Regulating Infinity](regulating-infinity).
<strong>Added 5Nov</strong>: Let me be clear: Bostrom’s book has much thoughtful analysis of AI foom consequences and policy responses. But aside from mentioning a few factors that might increase or decrease foom chances, Bostrom simply doesn’t given an argument that we should expect foom. Instead, Bostrom just assumes that the reader thinks foom likely enough to be worth his detailed analysis.

## [Foom Justifies AI Risk Efforts Now](#table-of-contents)
_Posted on 2017-08-03_

Years ago I was honored to share this blog with Eliezer Yudkowsky. One of his main topics then was AI Risk; he was one of the few people talking about it back then. We <a href="https://intelligence.org/ai-foom-debate/">debated</a> this topic here, and while we disagreed I felt we made progress in understanding each other and exploring the issues. I assigned a much lower probability than he to his key “foom” scenario.

Recently AI risk has become something of an industry, with far more going on than I can keep track of. Many call working on it one of the most effectively altruistic things one can possibly do. But I’ve searched a bit and as far as I can tell that foom scenario is still the main reason for society to be concerned about AI risk now. Yet there is almost no recent discussion evaluating its likelihood, and certainly nothing that goes into as much depth as did Eliezer and I. Even Bostrom’s [book length](30855) treatment basically just assumes the scenario. Many seem to think it obvious that if one group lets one AI get out of control, the whole world is at risk. It’s not (obvious).
As I just revisited the topic while revising <a href="http://ageofem.com"><em>Age of Em</em></a> for paperback, let me try to summarize part of my position again here.<span id="more-31584"></span>

For at least a century, every decade or two we’ve seen a burst of activity and concern about automation. The last few years have seen another such burst, with increasing activity in AI research and commerce, and also increasing concern expressed that future smart machines might get out of control and destroy humanity. Some argue that these concerns justify great efforts today to figure out how to keep future AI under control, and to more closely watch and constrain AI research efforts. Approaches considered include kill switches, requiring prior approval for AI actions, and designing AI motivational system to make AIs want to help, and not destroy, humanity.

Consider, however, an analogy with organizations. Today, the individuals and groups who create organizations and their complex technical systems are often well-advised to pay close attention to how to maintain control of such organizations and systems. A loss of control can lead not only to a loss of the resources invested in creating and maintaining such systems, but also to liability and retaliation from the rest of the world.

But exactly because individuals usually have incentives to manage their organizations and systems reasonably well, the rest of us needn’t pay much attention to the internal management of others’ organizations. In our world, most firms, cities, nations, and other organizations are much more powerful and yes smarter than are most individuals, and yet they remain largely under control in most important ways. For example, none have so far destroyed the world. Smaller than average organizations can typically exist and even thrive without being forcefully absorbed into larger ones. And outsiders can often influence and gain from organization activities via control institutions like elections, board of directors, and voting stock.

Mostly, this is all achieved neither via outside action approval nor via detailed knowledge and control of motivations. We instead rely on law, competition, social norms, and politics. If a rogue organization seems to harm others, it can be accused of legal violations, as can its official owners and managers. Those who feel hurt can choose to interact with it less. Others who are not hurt may choose to punish the rogue informally for violating informal norms, and get rewarded by associates for such efforts. And rogues may be excluded from political coalitions, who hurt it via the policies of governments and other large organizations.

AI and other advanced technologies may eventually give future organizations new options for internal structures, and those introducing such innovations should indeed consider their risks for increased chances of losing control. But it isn’t at all clear why the rest of us should be much concerned about this, especially many decades or centuries before such innovations may appear. Why can’t our usual mechanisms for keeping organizations under control, outlined above, keep working? Yes, innovations might perhaps create new external consequences, ones with which those outside of the innovating organization would need to deal. But given how little we now understand about the issues, architectures, and motivations of future AI systems, why not mostly wait and deal with any such problems later?

Yes, our usual methods do fail at times; we’ve had wars, revolutions, theft, and lies. In particular, each generation has had to accept slowly losing control of the world to succeeding generations. While prior generations can typically accumulate and then spend savings to ensure a comfortable retirement, they no longer rule the world. Wills, contracts, and other organizational commitments have not been enough to prevent this. [Some](chalmers-reply-2) find this unacceptable, and seek ways to enable a current generation, e.g., humans today, to maintain strong control over all future generations, be they biological, robotic or something else, even after such future generations have become far more capable than the current generation. To me this problem seems both very hard, and not obviously worth solving.
Returning to the basic problem of rogue systems, some forsee a rapid local “intelligence explosion”, sometimes called “foom”, wherein one initially small system quickly becomes vastly more powerful than the entire rest of the world put together. And, yes, if such a local explosion might happen soon, then it could make more sense for the rest of us today, not just those most directly involved, to worry about how to keep control of future rogue AI.

In a prototypical “foom,” or local intelligence explosion, a single AI system starts with a small supporting team. Both the team and its AI have resources and abilities that are tiny on a global scale. This team finds and then applies a big innovation in system architecture to its AI system, which as a result greatly improves in performance. (An “architectural” change is just a discrete change with big consequences.) Performance becomes so much better that this team plus AI combination can now quickly find several more related innovations, which further improve system performance. (Alternatively, instead of finding architectural innovations the system might enter a capability regime which contains a large natural threshold effect or scale economy, allowing a larger system to have capabilities well out of proportion to its relative size.)

During this short period of improvement, other parts of the world, including other AI teams and systems, improve much less. Once all of this team’s innovations are integrated into its AI system, that system is now more effective than the entire rest of the world put together, at least at one key task. That key task might be theft, i.e., stealing resources from the rest of the world. Or that key task might be innovation, i.e., improving its own abilities across a wide range of useful tasks.

That is, even though an entire world economy outside of this team, including other AIs, works to innovate, steal, and protect itself from theft, this one small AI team becomes vastly better at some combination of (1) stealing resources from others while preventing others from stealing from it, and (2) innovating to make this AI “smarter,” in the sense of being better able to do a wide range of mental tasks given fixed resources. As a result of being better at these things, this AI quickly grows the resources under its control and becomes in effect more powerful than the entire rest of the world economy put together. So, in effect it takes over the world. All of this happens within a space of hours to months.

(The hypothesized power advantage here is perhaps analogous that of the first team to make an atomic bomb, if that team had had enough other supporting resources to enable it to use the bomb to take over the world.)

Note that to believe in such a local explosion scenario, it is not enough to believe that eventually machines will be very smart, even much smarter than are humans today. Or that this will happen soon. It is also not enough to believe that a world of smart machines can overall grow and innovate much faster than we do today. One must in addition believe that an AI team that is initially small on a global scale could quickly become vastly better than the rest of the world put together, including other similar teams, at improving its internal abilities.

If a foom-like explosion can quickly make a once-small system more powerful than the rest of the world put together, the rest of the world might not be able to use law, competition, social norms, or politics to keep it in check. Safety can then depend more on making sure that such exploding systems start from safe initial designs.

In another post I may review arguments for and against the likelihood of foom. But in this one I’m content to just point out that the main reason for society, as opposed to particular projects, to be concerned about AI risk is either foom, or an ambition to place all future generations under the tight control of a current generation. So a low estimate of the probability of foom can imply a much lower social value from working on AI risk now.

<strong>Added Aug 4</strong>: I made a twitter poll on motives for AI risk concern:


If AI Risk is priority now, why?  Foom: 1 AI takes over world, Value Drift: default future has bad values, or Collapse: property rights fail

> — robin hanson (@robinhanson) <a href="https://twitter.com/robinhanson/status/893459642210295808">August 4, 2017</a>

<script async="" charset="utf-8" src="//platform.twitter.com/widgets.js"></script>

## [Is The City-ularity Near?](#table-of-contents)
_Posted on 2010-02-09_

The land around New York City is worth <em>a lot</em>.  A 2008 <a href="http://www.newyorkfed.org/research/current_issues/ci14-3/ci14-3.html">analysis</a> estimated prices for land, not counting buildings etc., for most (~80%?) of the nearby area (2750 square miles, = a 52 mile square).  The total New York area land value (total land times ave price) was 5.5T$ (trillion) in 2002 and 28T$ in 2006.

The <em>Economist</em> <a href="http://en.wikipedia.org/wiki/Real_estate">said</a> that in 2002 all developed nation real estate was worth 62T$.  Since raw land value is on average <a href="www.jstor.org/stable/3486442">about a third</a> of total real estate value, that puts New York area real estate at over 30% of all developed nation real estate in 2002!  Whatever the exact number, clearly this agglomeration contains vast value.

New York land is valuable mainly because of how it is organized.  People want to be there because they want to interact with other people they expect to be there, and they expect those interactions to be quite mutually beneficial.  If you could take any other 50 mile square (of which Earth has 72,000), and create that same expectation of mutual value from interactions, you could get people to come there, make buildings, etc., and sell that land for many trillions of dollars of profit.

Yet the organization of New York was mostly set long ago based on old tech (e.g., horses, cars, typewriters).  Worse, no one really understands at a deep level how it is organized or why that works so well.  Different people understand different parts, in mostly crude empirical ways.

So what will happen when super-duper smarties wrinkle their brows so hard that out pops a deep math theory of cities, explaining clearly how city value is produced?  What if they apply their theory to designing a city structure that takes best advantage of our most advanced techs, of 7gen phones, twitter-pedias, flying Segways, solar panels, gene-mod pigeons, and super-fluffy cupcakes?  Making each city aspect more efficient makes the city more attractive, increasing the gains from making other aspects more efficient, in a grand spiral of bigger gains.

Once they convince the world of the vast value in their super-stupendous city design, won’t everyone flock there and pay mucho trillions for the privilege? Couldn’t they leverage this lead into better theories enabling better designs giving far more trillions, and then spend all that on a super-designed war machine based on those same super insights, and turn us all into down dour super-slaves?  So isn’t the very mostest importantest cause ever to make sure that we, the friendly freedom fighters, find this super deep city theory first?

Well, no, it isn’t.  We don’t believe in a city-ularity because we don’t believe in a super-city theory found in a big brain flash of insight.  What makes cities work well is mostly getting lots of details right.  Sure new-tech-based cities designs can work better, but gradual tech gains mean no city is suddenly vastly better than others.  Each change has costs to be weighed against hoped-for gains.  Sure costs of change might be lower when making a whole new city from scratch, but for that to work you have to be damn sure you know which changes are actually good ideas.

For similar reasons, I’m skeptical of a blank-slate AI mind-design singularity.  Sure if there were a super mind theory that allowed vast mental efficiency gains all at once, but there isn’t.  Minds are vast complex structures full of parts that depend intricately on each other, much like the citizens of a city.  Minds, like cities, best improve gradually, because you just never know enough to manage a vast redesign of something with such complex inter-dependent adaptations.

## [The Betterness Explosion](#table-of-contents)
_Posted on 2011-06-21_

We all want the things around us to be better. Yet today billions struggle year after year to make just a few things a bit better. But what if our meagre success was because we just didn’t have the right grand unified theory of betterness? What if someone someday discovered the basics of such a theory? Well then this person might use his basic betterness theory to make himself better in health, wealth, sexiness, organization, work ethic, etc. More important, that might help him make his betterness theory even better.

After several iterations this better person might have a much better betterness theory. Then he might quickly make everything around him much better. Not just better looking hair, better jokes, or better sleep. He might start a better business, and get better at getting investors to invest, customers to buy, and employees to work. Or he might focus on making better investments. Or he might run for office and get better at getting elected, and then make his city or nation run better. Or he might create a better weapon, revolution, or army, to conquer any who oppose him.

Via such a “betterness explosion,” one way or another this better person might, if so inclined, soon own, rule, or conquer the world. Which seems to make it very important that the first person who discovers the first good theory of betterness be a very nice generous person who will treat the rest of us well. Right?

OK, this might sound silly. After all, we seem to have little reason to expect there is a useful grand unified theory of betterness to discover, beyond what we already know. “Betterness” seems mostly a concept about us and what we want – why should it correspond to something out there about which we can make powerful discoveries?

But a bunch of smart well-meaning folks actually do worry about a scenario that seems pretty close to this one. Except they talk about “intelligence” instead of “betterness.” They imagine an “intelligence explosion,” by which they don’t just mean that eventually the future world and many of its creatures will be more mentally capable than us in many ways, or even that the rate at which the world makes itself more mentally capable will speed up, similar to how growth rates have sped up over the long sweep of history. No, these smart well-meaning folks instead imagine that once someone has a powerful theory of “intelligence,” that person could create a particular “intelligent” creature which is good at making itself more “intelligent,” which then lets that creature get more “intelligent” about making itself “intelligent.” Within a few days or weeks, the story goes, this one creature could get so “intelligent” that it could do pretty much anything, including taking over the world.

I put the word “intelligence” in quotes to emphasize that the way these folks use this concept, it pretty much just means “betterness.” (Well, mental betterness, but most of the betterness we care about is mental.) And this fits well with common usage of the term “intelligence.” When we talk about machines or people or companies or even nations being “intelligent,” we mainly mean that such things are broadly mentally or computationally capable, in ways that are important for their tasks and goals. That is, an “intelligent” thing has a great many useful capabilities, not some particular specific capability called “intelligence.” To make something broadly smarter, you have to improve a wide range of its capabilities. And there is generally no easy or fast way to do that.

Now if you artificially hobble something so as to simultaneously reduce many of its capacities, then when you take away that limitation you may simultaneously improve a great many of its capabilities. For example, if you drug a person so that they can hardly think, then getting rid of that drug can suddenly improve a great many of their mental abilities. But beyond removing artificial restrictions, it is very hard to simultaneously improve many diverse capacities. Theories that help you improve capabilities are usually focused on a relatively narrow range of abilities – very general and useful theories are quite rare.

All of which is to say that fearing that a new grand unified theory of intelligence will let one machine suddenly take over the world isn’t that different from fearing that a grand unified theory of betterness will let one better person suddenly take over the world. This isn’t to say that such an thing is impossible, but rather that we’d sure want some clearer indications that such a theory even exists before taking such a fear especially seriously.

## [An Outside View of AI Control](#table-of-contents)
_Posted on 2017-10-01_

I’ve written much on my skepticism of local AI foom (= intelligence explosion). Recently I [said](foom-justifies-ai-risk-efforts-now) that foom offers the main justification I understand for AI risk efforts now, as well as being the main choice of my Twitter followers in a survey. It was the main argument offered by [Eliezer Yudkowsky](debate-is-now-book) in our debates here at this blog, by [Nick Bostrom](30855) in his book <em>Superintelligence</em>, and by [Max Tegmark](tegmarks-book-of-foom) in his recent book <em>Life 3.0</em> (though he denied so in his [reply](tegmarks-book-of-foom) here).

However, some privately complained to me that I haven’t addressed those with non-foom-based AI concerns. So in this post I’ll consider AI control in the context of a prototypical <em>non-em non-foom mostly-peaceful outside-view AI scenario</em>. In a future post, I’ll try to connect this to specific posts by others on AI risk.

An <em>AI scenario</em> is where software does most all jobs; humans may work for fun, but they add little value. In a <em>non-em</em> scenario, ems are never feasible. As foom scenarios are driven by AI innovations that are very lumpy in time and organization, in <em>non-foom</em> scenarios innovation lumpiness is distributed more like it is in our world. In a <em>mostly-peaceful</em> scenario, peaceful technologies of production matter much more than do technologies of war and theft. And as an <em>outside view</em> guesses that future events are like similar past events, I’ll relate future AI control problems to similar past problems.<span id="more-31639"></span>

<a href="https://en.wikipedia.org/wiki/Conway%27s_law">Conway’s law</a> of software says that the structure of software tends to reflect the structure of the social organizations that make it. This suggests that the needs of software to have particular modularity structures for particular problems is usually weaker than the needs of organizations to maintain familiar structures of communication and authority. So a world where software does most job tasks could retain a recognizable clumping of tasks into jobs, divisions, firms, professions, industries, cities, and nations.

Today, most lumpiness in firms, industries, cities, etc. is not due to lumpiness in innovation, but instead due to various scale and scope economies, and network effects. Innovation may be modestly lumpy at the scale of particular industries, but not at the level of entire economic sectors. Most innovation comes from many small contributions; big lumps contain only a small fraction of total value.

While innovation is often faster in some areas than in others, most social rates of change tend to be near the doubling time of the economy. An AI world allows for faster growth, as it isn’t held back by slow growing humans; I’ve estimated that it might double monthly. But our first guess should be that social rates of change speed up together; we’ll need specific reasons to expect specific changes, relative to today, in relative rates of change.

Today humans tend to get job specific training in the firms where they work, and general training elsewhere. Similarly, in an AI world specific software may be written in organizations near where it is used, while more general tools are written in more distant organizations.

Today most tasks are done by human brains, which come in a standard-sized unit capable of both doing specific tasks and also related meta-tasks, such as figuring out how to do tasks better, deciding which specific tasks to do when and how, and regrouping how tasks are clumped within organizations. So we tend to first automate tasks that can be done by much smaller units. And while managers and others often specialize in meta-tasks, line workers also do many meta tasks themselves. In contrast, in an AI world meta-tasks tend to be separated more from other tasks, and so done by software that is more different.

In such a division of tasks, most tasks have a relatively narrow scope. For narrow tasks, the main risks are of doing tasks badly, and of hostile agents taking control of key resources. So most control efforts focus on such problems. For narrow tasks there is little extra risk from such tasks being done very well, even if one doesn’t understand how that happens. (War tech is of course an exception; victims can suffer more when war is done well.) The control risks on which AI risk folks focus, of very effective efforts misdirected due to unclear goals, are mainly concentrated in tasks with very wide scopes, such as in investment, management, law, and governance. These are mostly very-meta-tasks.

The future problem of keeping control of advanced software is similar to the past problems of keeping control both of physical devices, and of social organizations. As the tasks we assign to physical devices tend to be narrow, we mostly focus there on specific control failure scenarios. The main risks there are losing control to hostile agents, and doing tasks badly, rather than doing them very well. The main people to get hurt when control is lost are those who rely on such devices, or who are closely connected to such people.

Humans started out long ago organized into small informal bands, and later had to learn to deal with the new organizations and institutions of rulers, command heirarchies, markets, family clans, large scale cultures, networks of talking experts, legal systems, firms, guilds, religions, clubs, and government agencies. Such organizations are often given relatively broad tasks. And even if not taken over by hostile agents, they can drift out of control. For example, organizations may on the surface seem responsive and useful, while increasingly functioning mainly to entrench and perpetuate themselves.

When social organizations get out of control in this way, the people who initiated and then participated in them are the main folks to get hurt. So such initiators and participants thus have incentives to figure out how to avoid such control losses, and this has long been a big focus of organization innovation efforts.

Innovation in control mechanisms has long been an important part of innovation in devices and organizations. People sometimes try to develop better control mechanisms in the abstract, before they’ve seen real systems. They also sometimes experiment in the context of small test versions. But most control innovations come in response to seeing real behaviors associated with typical real versions. The main reason that it becomes harder to implement innovations later is that design features often become entrenched. But if control is important enough, it can be worth paying large costs of change to implement better control features.

Humans one hundred thousand years ago might have tried to think carefully about how to control rulers with simple command hierarchies, and people one thousand years ago might have tried to think carefully about how to control complex firms and government agencies. But the value of such early efforts would have been quite limited, and it wasn’t at all too late to work on such problems after such systems appeared. In peacetime, control failures mainly threatened those who initiated and participated in such organizations, not the world as a whole.

In the AI scenario of this post, the vast majority of advanced future software does tasks of such narrow scopes that their control risks are more like those for physical devices, relative to new social organizations. So people deploying new advanced software will know to focus extra control efforts on software doing wider scope meta-tasks. Furthermore, the main people harmed by failures to control AI assigned to meta-tasks will be those associated with the organizations that do such meta-tasks.

For example, customers who let an AI tell them whom to date may suffer from bad dates. Investors in firms that let AI manage key firm decisions might lose their investments. And citizens who let AI tell their police who to put in jail may suffer in jail, or from undiscouraged crime. But such risks are mostly local, not global risks.

Of course for a long time now, coordination scales have been slowly increasingly worldwide. So over time “local” effects become increasingly larger scale effects. This is a modest reason for everyone to slowly get more concerned about “local” problems elsewhere.

Today is a very early date to be working on AI risk; I’ve [estimated](ai-progress-estimate) that without ems it is several centuries away. We are now pretty ignorant about most aspects of how advanced software will be used and arranged. So it is hard to learn much useful today about how to control future advanced software. We can learn to better control the software we have now, and later on we should expect innovations in software control to speed up roughly as fast as do innovations in making software more effective. Even if control innovations by humans don’t speed up as fast, advanced software will itself be made of many parts, and some parts will want to keep control over other parts.

The mechanisms by which humans today maintain control over organizations include law, property rights, constitutions, command hierarchies, and even democracy. Such broad mechanisms are effective, entrenched and robust enough that future advanced software systems and organizations will almost surely continue to use variations within these broad categories keep control over each other. So humans can reasonably hope to be at least modestly protected in the short run if they can share the use of such systems with advanced software. For example, if law protects software from stealing from software, it may also protect humans from such theft.

Of course humans have long suffered from events like wars and revolutions, events that create risks of harm and loss of control. And the rate of such events can scale with the main rates of change in the economy, which go inversely as the economic doubling time. So a much faster changing future AI economy can have faster rates of such risky events. It seems a robust phenomenon that when the world speeds up, those who do not speed up with it face larger subjective risks if they do not ally with sped-up protectors.

Having humans create and become <a href="http://ageofem.com">ems</a> is one reasonable approach to creating sped-up allies for humans. Humans will no doubt also try to place advanced software in such an ally role. Once software is powerful, then attempts by humans to control such software are probably mostly based on copying the general lessons and approaches that advanced software discovers for how to maintain control over advanced software. Humans may also learn extra lessons that are specific to the human control problem, and some of those lessons may come from our era, long before any of this plays out.

But in the sort of AI scenario I’ve described in this post, I find it very hard to see such early efforts as the do-or-die priority that some seem to suggest. Outside of a foom scenario, control failures threaten to cause local, not global losses (though on increasingly larger scales).

From this view, those tempted to spend resources now on studying AI control should consider two reasonable alternatives. The first alternative is to just save more now to grow resources to be used later, when we understand more. The second alternative is to work to innovate with our general control institutions, to make them more robust, and thus better able to handle larger coordination scales, and whatever other problems the future may hold. (E.g., <a href="http://hanson.gmu.edu/futarchy.html">futarchy</a>.)

Okay, this is how I see the AI control problem in a <em>non-em non-foom mostly-peaceful outside-view AI scenario</em>. But clearly others disagree with me; what am I missing?

<strong>Added 4 Oct: </strong>

In the context of foom, the usual AI concern is a total loss of control of the one super AI, whose goals quickly drift to a random point in the space of possible goals. Humans are then robustly exterminated. As the AI is so smart and inscrutable, any small loss of control is said to open the door to such extreme failure. I have presumed that those who tell me to look at non-foom AI risk are focused on similar failure scenarios.

Today most social systems suffer from agency costs, and larger costs (in % terms) for larger systems. But these mostly take the form of modestly increasing costs. It isn’t that you can’t reliably use these systems to do the things that you want. You just have to pay more. That extra cost mostly isn’t a transfer accumulating in someone else’s account. Instead there is just waste that goes to no one, and there are more cushy jobs and roles where people can comfortably sit as parasites. Over time, even though agency costs take a bigger cut, total costs get lower and humans get more of what they want.

When I say that in my prototypical non-foom AI scenario, AI will still pay agency costs but the AI control problem seems mostly manageable, I mean that very competent future social and software systems will suffer from waste and parasites as do current systems, but that humans can still reliably use such systems to get what they want. Not only are humans not exterminated, they get more than before of what they want.

## [AI Risk, Again](#table-of-contents)
_Posted on 2023-03-03_

Large language models like ChatGPT have recently spooked a great many, and my Twitter feed is full of worriers saying how irresponsible orgs have been to make and release such models. Because, they say, such a system might have killed us all. And, as some researchers say that they are working on how to better control such things, worriers say we must regulate to slow/stop AI progress until such researchers achieve their goals. While I’ve written [on](https://www.lesswrong.com/posts/D3NspiH2nhKA6B2PE/what-evidence-is-alphago-zero-re-agi-complexity) [this](https://www.overcomingbias.com/p/why-not-wait-on-ai-riskhtml) [many](https://www.overcomingbias.com/p/foom-updatehtml) [times](https://www.overcomingbias.com/p/how-lumpy-ai-serviceshtml) [before](https://www.overcomingbias.com/p/agency-failure-ai-apocalypsehtml), it seems time to restate my position.

First, if past trends continue, then sometimes in the next few centuries the world economy is likely to enter a transition that lasts roughly a decade, after which it may double every few months or faster, in contrast to our current fifteen year doubling time. (Doubling times have been relatively steady as innovations are typically tiny compared to the world economy.)

The most likely cause for such a transition seems to be a transition to an economy dominated by artificial intelligence (AI). (Perhaps in the form of brain emulations, but perhaps also in more alien forms.) Especially as the doubling time of a fully-automated factory today is a few months, and computer algorithm gains have been [close](https://www.overcomingbias.com/p/why-does-hardware-grow-like-algorithmshtml) to hardware gains. And within a year or two from then, another transition to an even faster mode might plausibly occur. 

Second, coordination and control are hard. Today, org leaders often gain rents from their positions, rents which come at the expense of org owners, suppliers, and customers. This happens more-so at non-profits and publicly-held for-profits, compared to privately held for-profits. Political and military leaders also gain rents, and sometimes take over control of nations via coups. While leader rents are not the only control problem, the level of such rents is a rough indication of the magnitude of our control problems. Those who are culturally more distant from leaders, such as the poor and third world residents, typically pay higher rents.

Today such rents are non-trivial, but even so competition between orgs keeps them tolerable. That is, we mostly keep our orgs under control. Even though, compared to individual humans, large orgs are in effect “super-intelligences”.

Third, there may be extra obstacles to slow bio humans controlling future org ventures. Bio humans would be more culturally distant, slower, and less competent than em AIs. (Though the principle-agent lit doesn’t yet show smarts differences to be an issue.) And non-em AIs could be even more culturally distant. However, even an increase of a factor of two or four in control rents for AIs seems tolerable, offering such bio humans a rich and growing future. Yes, periodically some ventures would suffer the equivalent of a coup. But if, like today, each venture were only a small part of this future world, bio humans as a whole would do fine. Ems, if they exist, could do even better.

Of course the owners of such future ventures, be they bio humans, ems, or other, are well advised to consider how best to control such ventures, to cut leader rents and other related costs of imperfect control. But such efforts seem most effective when based on actual experience with concrete fielded systems. For example, there was little folks could do in the year 1500 to figure out how to control 20th century orgs, weapons, or other tech. Thus as we now know very little about the details of future AI-based ventures, leaders, or systems, we should today mostly either save resources to devote to future efforts, or focus our innovation efforts on improving control of existing ventures. Such as via decision markets.

Most of the worriers mentioned above, however, reject the above analysis, based as it is on expecting a continuation of historical patterns, wherein ventures and innovations have been consistently small compared to the world economy. They instead say that it is possible that a single small AI venture might stumble across a single extremely potent innovation, which enables it to suddenly “foom”, i.e., explode in power from tiny compared to the world economy, to more powerful than the entire rest of the world put together. (Including all other AIs.)

This scenario requires that this venture prevent other ventures from using its key innovation during this explosive period. It also requires that this new more powerful system not only be far smarter in most all important areas, but also be extremely capable at managing its now-enormous internal coordination problems. And it requires that this system be not a mere tool, but a full “agent” with its own plans, goals, and actions. 

Furthermore it is possible that even though this system was, before this explosion, and like most all computer systems today, very well tested to assure that its behavior was aligned well with its owners’ goals across its domains of usage, its behavior after the explosion would be nearly maximally non-aligned. (That is, orthogonal in a high dim space.) Perhaps resulting in human extinction. The usual testing and monitoring processes would be prevented from either noticing this problem or calling a halt when it so noticed, either due to this explosion happening too fast, or due to this system creating and hiding divergent intentions from its owners prior to the explosion.

While I agree that this is a logically possible scenario, not excluded by what we know, I am disappointed to see so many giving it such a high credence, given how crazy far it seems from our prior experience. Yes, there is a sense in which the human, farming, and industry revolutions were each likely the result of a single underlying innovation. But those were the three biggest innovations in all of human history. And large parts of the relevant prior world exploded together in those cases, not one tiny part suddenly exterminating all the rest.

In addition, the roughly decade duration predicted from prior trends for the length of the next transition period seems _plenty_ of time for today’s standard big computer system testing practices to notice alignment issues. And note that the impressive recent AI chatbots are especially unlike the systems of concern here: self-improving very-broadly-able full-agents with hidden intentions. Making this an especially odd time to complain that new AI systems might have killed us all.

You might think that folks would take a lesson from our history of prior bursts of anxiety and concern about automation, bursts which have appeared roughly every three decades since at least the 1930s. Each time, new impressive demos revealed unprecedented capabilities, inducing a burst of activity and discussion, with many then expressing fears that a rapid explosion might soon commence, automating all human labor. They were, of course, very wrong. 

Worriers often invoke a Pascal’s wager sort of calculus, wherein any tiny risk of this nightmare scenario could justify large cuts in AI progress. But that seems to assume that it is relatively easy to assure the same total future progress, just spread out over a longer time period. I instead fear that overall economic growth and technical progress is more fragile that this assumes. Consider how regulations inspired by nuclear power nightmare scenarios have for seventy years prevented most of its potential from being realized. I have also seen progress on many other promising techs mostly stopped, not merely slowed, via regulation inspired by vague fears. In fact, progress seems to me to be slowing down worldwide due to excess fear-induced regulation. 

Over the last few centuries the world did relatively little to envision problems with future techs, and to prepare for those problems far in advance of seeing concrete versions. And I just do not believe that the world would have been better off if we had instead greatly slowed tech progress in order to attempt such preparations. Especially considering the degree of centralized controls that might have been required to implement such a slowdown policy.

As I discussed above, it just looks way too early to learn much about how to control future AI systems, about which we know so few details. Thus when facing the risk of our fear essentially halting progress here, I’d rather continue down our current path, and work harder on controls when we better see concrete serious control problems to manage. 

**Added March 7**: Some say that, given enough data and hardware, predict-the-next-token models like ChatGPT will have human or better performance. Using action tokens, that would include many kinds of behavior. But this isn’t sufficient for such a system to rapidly “foom”. To even try, it needs high competence in design and testing alternative system architectures, and there’s no guarantee even with that.

## [Foom Update](#table-of-contents)
_Posted on 2022-05-06_

To extend our reach, we humans have built tools, machines, firms, and nations. And as these are powerful, we try to maintain control of them. But as efforts to control them usually depend on their details, we have usually waited to think about how to control them until we had concrete examples in front of us. In the year 1000, for example, there wasn’t much we could do to usefully think about how to control most things that have only appeared in the last two centuries, such as cars or international courts.

Someday we will have far more powerful computer tools, including “advanced artificial general intelligence” (AAGI), i.e., with capabilities even higher and broader than those of individual human brains today. And some people today spend substantial efforts today worrying about how we will control these future tools. Their most common argument for this unusual strategy is “foom”.

That is, they postulate a single future computer system, initially quite weak and fully controlled by its human sponsors, but capable of action in the world and with general values to drive such action. Then over a short time (days to weeks) this system dramatically improves (i.e., “fooms”) to become an AAGI far more capable even than the sum total of all then-current humans and computer systems. This happens via a process of self-reflection and self-modification, and this self-modification also produces large and unpredictable changes to its effective values. They seek to delay this event until they can find a way to prevent such dangerous “value drift”, and to persuade those who might initiate such an event to use that method.

I’ve argued at length ([1](https://www.overcomingbias.com/2017/08/foom-justifies-ai-risk-efforts-now.html) [2](https://www.overcomingbias.com/2014/07/30855.html) [3](https://www.overcomingbias.com/2013/02/foom-debate-again.html) [4](https://www.overcomingbias.com/2011/06/the-betterness-explosion.html) [5](https://www.overcomingbias.com/2010/02/is-the-city-ularity-near.html) [6](https://www.overcomingbias.com/2008/11/fund-ubertool.html) [7](https://www.overcomingbias.com/2008/11/setting-the-sta.html)) against the plausibility of this scenario. Its not that its impossible, or that no one should work on it, but that far too many take it as a default future scenario. But I haven’t written on it for many years now, so perhaps it is time for an update. Recently we have seen noteworthy [progress](https://www.overcomingbias.com/2022/04/ailanguageprogess.html) in AI system demos (if not yet commercial application), and some have urged me to update my views as a result.

The recent systems have used relative simple architectures and basic algorithms to produce models with enormous numbers of parameters from very large datasets. Compared to prior systems, these systems have produced impressive performance on an impressively wide range of tasks. Even though they are still quite far from displacing humans in any substantial fraction of their current tasks.

For the purpose of reconsidering foom, however, the key things to notice are: (1) these systems have kept their values quite simple and very separate from the rest of the system, and (2) they have done basically zero self-reflection or self-improvement. As I see AAGI as still a long way off, the features of these recent systems can only offer weak evidence regarding the features of AAGI.

Even so, recent developments offer little support for the hypothesis that AAGI will be created soon via the process of self-reflection and self-improvement, or for the hypothesis that such a process risks large “value drifts”. These current ways that we are now moving toward AAGI just don’t look much like the foom scenario. And I don’t see them as saying much about whether ems or AAGI will appear first.

Again, I’m not saying foom is impossible, just that it looks unlikely, and that recent events haven’t made it seem moreso.

These new systems do suggest a substantial influence of architecture on system performance, though not obviously at a level out of line with that in most prior AI systems. And note that the abilities of the very best systems here are not that much better than that of the 2nd and 3rd best systems, arguing weakly against AAGI scenarios where the best system is vastly better.
